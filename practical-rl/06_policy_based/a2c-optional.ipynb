{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "a2c-optional.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ymp_uxMWvbEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    USE_COLAB = True\n",
        "except:\n",
        "    USE_COLAB = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PtvD0sJipzt",
        "colab_type": "code",
        "outputId": "fee737cb-6df8-4e8a-d4f5-a8746f4a0a85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# in google colab uncomment this\n",
        "if USE_COLAB:\n",
        "  import os\n",
        "\n",
        "  os.system('apt-get install -y xvfb')\n",
        "  os.system('wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb -O ../xvfb')\n",
        "\n",
        "  file_format = 'https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/week06_policy_based/{file} -O {file}'\n",
        "\n",
        "  os.system('wget ' + file_format.format(file='atari_wrappers.py'))\n",
        "  os.system('wget ' + file_format.format(file='env_batch.py'))\n",
        "  os.system('wget ' + file_format.format(file='runners.py'))\n",
        "\n",
        "  os.system('apt-get install -y python-opengl ffmpeg')\n",
        "  os.system('pip install pyglet==1.2.4')\n",
        "\n",
        "  os.system('python -m pip install -U pygame --user')\n",
        "\n",
        "  print('setup complete')\n",
        "\n",
        "  # # XVFB will be launched if you run on a server\n",
        "  # if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "  #     !bash ../xvfb start\n",
        "  #     os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "setup complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbntG0hRipzy",
        "colab_type": "text"
      },
      "source": [
        "# Implementing Advantage-Actor Critic (A2C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVcCkfNVipzz",
        "colab_type": "text"
      },
      "source": [
        "In this notebook you will implement Advantage Actor Critic algorithm that trains on a batch of Atari 2600 environments running in parallel. \n",
        "\n",
        "Firstly, we will use environment wrappers implemented in file `atari_wrappers.py`. These wrappers preprocess observations (resize, grayscal, take max between frames, skip frames and stack them together) and rewards. Some of the wrappers help to reset the environment and pass `done` flag equal to `True` when agent dies.\n",
        "File `env_batch.py` includes implementation of `ParallelEnvBatch` class that allows to run multiple environments in parallel. To create an environment we can use `nature_dqn_env` function. Note that if you are using \n",
        "PyTorch and not using `tensorboardX` you will need to implement a wrapper that will log **raw** total rewards that the *unwrapped* environment returns and redefine the implemention of `nature_dqn_env` function here. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2UWAJZEipzz",
        "colab_type": "code",
        "outputId": "2bf377ee-94ea-43db-90a3-b52b63e19839",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "if USE_COLAB:\n",
        "    %tensorflow_version 1.x\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from atari_wrappers import nature_dqn_env\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from tqdm import trange\n",
        "from pandas import DataFrame\n",
        "\n",
        "test_env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=8)\n",
        "test_obs = test_env.reset()\n",
        "assert test_obs.shape == (8, 84, 84, 4)\n",
        "assert test_obs.dtype == np.uint8"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:From /content/atari_wrappers.py:283: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DQvP7HIipz2",
        "colab_type": "text"
      },
      "source": [
        "Next, we will need to implement a model that predicts logits and values. It is suggested that you use the same model as in [Nature DQN paper](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) with a modification that instead of having a single output layer, it will have two output layers taking as input the output of the last hidden layer. **Note** that this model is different from the model you used in homework where you implemented DQN. You can use your favorite deep learning framework here. We suggest that you use orthogonal initialization with parameter $\\sqrt{2}$ for kernels and initialize biases with zeros. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EvTnPMBipz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_weights(m):\n",
        "    if type(m) in (nn.Linear, nn.Conv2d):\n",
        "        nn.init.orthogonal_(m.weight.data, math.sqrt(2.0))\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0)\n",
        "            \n",
        "def conv2d_size_out(size, kernel_size, stride):\n",
        "    \"\"\"\n",
        "    common use case:\n",
        "    cur_layer_img_w = conv2d_size_out(cur_layer_img_w, kernel_size, stride)\n",
        "    cur_layer_img_h = conv2d_size_out(cur_layer_img_h, kernel_size, stride)\n",
        "    to understand the shape for dense layer's input\n",
        "    \"\"\"\n",
        "    size = np.array(size)\n",
        "    return (size - (kernel_size - 1) - 1) // stride  + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc8hcu73ipz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent(nn.Module):\n",
        "    def __init__(self, state_shape, n_actions):\n",
        "        \"\"\"\n",
        "        An actor-critic agent\n",
        "        param: state_shape shape of a single test with shape [h, w, n_channels]\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        self.n_actions = n_actions\n",
        "        n_features = 128\n",
        "        \n",
        "        self.body_network = self.get_body_network(state_shape, n_features).apply(init_weights)\n",
        "        self.v_head = nn.Linear(n_features, 1).apply(init_weights)\n",
        "        self.logits_head = nn.Linear(n_features, n_actions).apply(init_weights)\n",
        "\n",
        "    def get_body_network(self, in_shape, out_features):\n",
        "        h, w, in_channels = in_shape\n",
        "        network = nn.Sequential()\n",
        "        \n",
        "        h, w = conv2d_size_out([h, w], 3, 2)\n",
        "        network.add_module('Conv1', nn.Conv2d(in_channels=in_channels, out_channels=16, kernel_size=3, stride=2))\n",
        "        network.add_module('Act1', nn.ReLU())\n",
        "\n",
        "        h, w = conv2d_size_out([h, w], 3, 2)\n",
        "        network.add_module('Conv2', nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2))\n",
        "        network.add_module('Act2', nn.ReLU())\n",
        "        \n",
        "        h, w = conv2d_size_out([h, w], 3, 2)\n",
        "        network.add_module('Conv3', nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2))\n",
        "        network.add_module('Act3', nn.ReLU())\n",
        "        network.add_module('flatten', nn.Flatten())\n",
        "        \n",
        "        network.add_module('FC1', nn.Linear(in_features=h * w * 64, out_features=out_features))\n",
        "        network.add_module('Act4', nn.ReLU())\n",
        "        return network\n",
        "    \n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        takes agent's observation, returns logits and state value\n",
        "        :param state: a batch of 4-frame buffers, shape = [batch_size, h, w, n_channels]\n",
        "        \"\"\"\n",
        "\n",
        "        state_t = state.permute(0, 3, 1, 2)\n",
        "        state_t = self.body_network(state_t)\n",
        "        logits = self.logits_head(state_t)\n",
        "        state_value = self.v_head(state_t)\n",
        "        \n",
        "        return logits, state_value\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        logits, _ = self.forward(state)\n",
        "        probas = nn.functional.softmax(logits).cpu().detach()\n",
        "        \n",
        "        return torch.multinomial(probas, num_samples=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsriF3anipz8",
        "colab_type": "text"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll9rT8E7ipz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_shape = (64, 64, 4)\n",
        "test_n_actions = 4\n",
        "test_agent = Agent(test_shape, test_n_actions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8vVNzjqipz_",
        "colab_type": "code",
        "outputId": "999ea52b-8bfd-4a50-9821-36919a9d06c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "test_state = torch.ones((2, 64, 64, 4))\n",
        "test_agent(test_state)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-8.5566, -1.7124,  1.1689, -1.9729],\n",
              "         [-8.5566, -1.7124,  1.1689, -1.9729]], grad_fn=<AddmmBackward>),\n",
              " tensor([[2.5737],\n",
              "         [2.5737]], grad_fn=<AddmmBackward>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR6EqopXip0C",
        "colab_type": "code",
        "outputId": "2884b8e4-512e-4d36-9522-c2fdd0e49134",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "test_agent.get_action(test_state)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2],\n",
              "        [2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48aX8g7vip0F",
        "colab_type": "text"
      },
      "source": [
        "You will also need to define and use a policy that wraps the model. While the model computes logits for all actions, the policy will sample actions and also compute their log probabilities.  `policy.act` should return a dictionary of all the arrays that are needed to interact with an environment and train the model.\n",
        " Note that actions must be an `np.ndarray` while the other\n",
        "tensors need to have the type determined by your deep learning framework. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cFEW9-Vip0F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Policy:\n",
        "    def __init__(self, model, device=torch.device('cpu')):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "    \n",
        "    def act(self, inputs):\n",
        "        \"\"\"\n",
        "        Implementation of policy which calls model, samples actions and computes their log probs\n",
        "        :return dict containing keys ['actions', 'logits', 'log_probs', 'values'].\n",
        "        \"\"\"\n",
        "        \n",
        "        inputs = torch.FloatTensor(inputs).to(self.device)\n",
        "        logits, state_values = self.model.forward(inputs)\n",
        "        probs = nn.functional.softmax(logits, -1)\n",
        "        log_probs = nn.functional.log_softmax(logits, -1)\n",
        "        actions = sample_actions(probs.cpu().detach().numpy())\n",
        "        n_actions = len(actions)\n",
        "        return {\n",
        "            'actions': actions,\n",
        "            'logits': logits,\n",
        "            'log_probs': log_probs[np.arange(n_actions), actions],\n",
        "            'values': state_values.view(-1)\n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4717OOnip0H",
        "colab_type": "text"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnMFrZC5ip0I",
        "colab_type": "code",
        "outputId": "eb5a4140-bdd8-4db6-805c-7d490fbcb645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_state = np.ones((2, 64, 64, 4))\n",
        "test_n_actions = 4\n",
        "test_agent = Agent(test_state.shape[1:], test_n_actions)\n",
        "test_policy = Policy(test_agent)\n",
        "\n",
        "test_res_dict = test_policy.act(test_state)\n",
        "\n",
        "assert all(nn.functional.log_softmax(test_res_dict['logits'], -1)[np.arange(2), test_res_dict['actions']] == test_res_dict['log_probs'])\n",
        "print(\"Test passed\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nR5_R6_ip0K",
        "colab_type": "text"
      },
      "source": [
        "Next will pass the environment and policy to a runner that collects partial trajectories from the environment. \n",
        "The class that does is is already implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqluG0a7ip0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from runners import EnvRunner"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nieN9dVUip0O",
        "colab_type": "text"
      },
      "source": [
        "This runner interacts with the environment for a given number of steps and returns a dictionary containing\n",
        "keys \n",
        "\n",
        "* 'observations' \n",
        "* 'rewards' \n",
        "* 'resets'\n",
        "* 'actions'\n",
        "* all other keys that you defined in `Policy`\n",
        "\n",
        "under each of these keys there is a python `list` of interactions with the environment of specified length $T$ &mdash; the size of partial trajectory. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlSXcGBlip0O",
        "colab_type": "text"
      },
      "source": [
        "**Test** Runner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_O6zdkuip0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_agent = Agent(test_obs.shape[1:], test_env.action_space.n)\n",
        "test_trajectory = EnvRunner(test_env, Policy(test_agent), 10).get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G1R1l6jip0R",
        "colab_type": "code",
        "outputId": "8682402d-a692-4ec4-cbf7-1f06b4b85024",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_trajectory.keys()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['actions', 'logits', 'log_probs', 'values', 'observations', 'rewards', 'resets', 'state'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2nqHk4iip0T",
        "colab_type": "code",
        "outputId": "bf8b3aef-329c-4983-b4c5-724f3bd8cf07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "test_trajectory['actions']"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([2, 2, 2, 2, 2, 2, 2, 2]),\n",
              " array([2, 2, 2, 2, 2, 2, 2, 2]),\n",
              " array([2, 2, 2, 2, 2, 2, 2, 2]),\n",
              " array([2, 2, 2, 2, 2, 2, 2, 2]),\n",
              " array([2, 2, 2, 2, 2, 2, 2, 2]),\n",
              " array([2, 2, 2, 2, 2, 2, 2, 2]),\n",
              " array([2, 2, 2, 2, 2, 2, 2, 2]),\n",
              " array([2, 2, 2, 2, 2, 2, 2, 2]),\n",
              " array([2, 2, 2, 2, 2, 2, 2, 2]),\n",
              " array([2, 2, 2, 2, 2, 2, 2, 2])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWDEuEbNip0Y",
        "colab_type": "code",
        "outputId": "bce54c7b-533f-4575-984e-20d01b5a8783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "test_trajectory['values']"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([-19.4506, -19.4506, -19.4506, -19.4506, -19.4506, -19.4506, -19.4506,\n",
              "         -19.4506], grad_fn=<ViewBackward>),\n",
              " tensor([-19.4506, -19.4506, -19.4506, -19.4506, -19.4506, -19.4506, -19.4506,\n",
              "         -19.4506], grad_fn=<ViewBackward>),\n",
              " tensor([-18.1989, -18.1989, -18.1989, -18.1989, -18.1989, -18.1989, -18.1989,\n",
              "         -18.1989], grad_fn=<ViewBackward>),\n",
              " tensor([-19.4088, -19.4088, -19.4088, -19.4088, -19.4088, -19.4088, -19.4088,\n",
              "         -19.4088], grad_fn=<ViewBackward>),\n",
              " tensor([-17.1944, -17.1944, -17.1944, -17.1944, -17.1944, -17.1944, -17.1944,\n",
              "         -17.1944], grad_fn=<ViewBackward>),\n",
              " tensor([-18.5031, -18.5031, -18.5031, -18.5031, -18.5031, -18.5031, -18.5031,\n",
              "         -18.5031], grad_fn=<ViewBackward>),\n",
              " tensor([-16.3744, -16.3744, -16.3744, -16.3744, -16.3744, -16.3744, -16.3744,\n",
              "         -16.3744], grad_fn=<ViewBackward>),\n",
              " tensor([-19.4088, -19.4088, -19.4088, -19.4088, -19.4088, -19.4088, -19.4088,\n",
              "         -19.4088], grad_fn=<ViewBackward>),\n",
              " tensor([-17.1944, -17.1944, -17.1944, -17.1944, -17.1944, -17.1944, -17.1944,\n",
              "         -17.1944], grad_fn=<ViewBackward>),\n",
              " tensor([-18.5031, -18.5031, -18.5031, -18.5031, -18.5031, -18.5031, -18.5031,\n",
              "         -18.5031], grad_fn=<ViewBackward>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMyl06cqip0a",
        "colab_type": "code",
        "outputId": "fb518ff3-827f-4a23-b35d-fd8df0f5db21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "test_trajectory['rewards']"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0.])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB-OnTVgip0d",
        "colab_type": "code",
        "outputId": "32d53a25-978a-46f9-ae8c-54b845365340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "test_trajectory['resets']"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([False, False, False, False, False, False, False, False]),\n",
              " array([False, False, False, False, False, False, False, False]),\n",
              " array([False, False, False, False, False, False, False, False]),\n",
              " array([False, False, False, False, False, False, False, False]),\n",
              " array([False, False, False, False, False, False, False, False]),\n",
              " array([False, False, False, False, False, False, False, False]),\n",
              " array([False, False, False, False, False, False, False, False]),\n",
              " array([False, False, False, False, False, False, False, False]),\n",
              " array([False, False, False, False, False, False, False, False]),\n",
              " array([False, False, False, False, False, False, False, False])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjYVqIqCip0g",
        "colab_type": "text"
      },
      "source": [
        "To train the part of the model that predicts state values you will need to compute the value targets. \n",
        "Any callable could be passed to `EnvRunner` to be applied to each partial trajectory after it is collected. \n",
        "Thus, we can implement and use `ComputeValueTargets` callable. \n",
        "The formula for the value targets is simple:\n",
        "\n",
        "$$\n",
        "\\hat v(s_t) = \\sum_{t'=t}^{T - 1}\\gamma^{t'-t}r_{t'}\\prod_{t''=t}^{t'} resets_{t''}{} + resets_{T-1}\\gamma^{T-1-t} \\hat{v}(s_{T-1}),\n",
        "$$\n",
        "\n",
        "In implementation, however, do not forget to use \n",
        "`trajectory['resets']` flags to check if you need to add the value targets at the next step when \n",
        "computing value targets for the current step. You can access `trajectory['state']['latest_observation']`\n",
        "to get last observations in partial trajectory &mdash; $s_{t+T}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AqREPsFip0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_target_values(rewards, resets, v_T, gamma):\n",
        "    assert len(rewards) == len(resets)\n",
        "    n = len(rewards)\n",
        "\n",
        "    target_values = []\n",
        "    curr_value = v_T\n",
        "    for r, done in reversed(list(zip(rewards, resets))):\n",
        "        if done:\n",
        "            curr_value = r\n",
        "        else:\n",
        "            curr_value = curr_value*gamma + r\n",
        "        target_values.append(curr_value)\n",
        "    return target_values[::-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BGvoBeaip0j",
        "colab_type": "text"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sb0y7wqip0j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0996d5dd-868d-450e-a94c-73a68bad4817"
      },
      "source": [
        "assert compute_target_values([1, 1, 1, 1], [False, True, False, False], 10, 1) == [2, 1, 12, 11]\n",
        "print(\"Test passed\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBoFz8F2ip0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ComputeValueTargets:\n",
        "    def __init__(self, policy, gamma=0.99):\n",
        "        self.policy = policy\n",
        "        self.gamma = gamma\n",
        "    \n",
        "    def __call__(self, trajectory):\n",
        "        # This method should modify trajectory inplace by adding \n",
        "        # an item with key 'value_targets' to it. \n",
        "        n_envs = len(trajectory['actions'][0])\n",
        "        T = len(trajectory['actions'])\n",
        "\n",
        "        trajectory['value_targets'] = [[] for i in range(T)]\n",
        "\n",
        "        for i in range(n_envs):\n",
        "            rewards = [trajectory['rewards'][t][i] for t in range(T)]\n",
        "            resets = [trajectory['resets'][t][i] for t in range(T)]\n",
        "            v_T = self.policy.act(trajectory['state']['latest_observation'])['values'][i]\n",
        "            target_values = compute_target_values(rewards, resets, v_T, self.gamma)\n",
        "            for t, target_value in enumerate(target_values):\n",
        "                trajectory['value_targets'][t].append(target_value)\n",
        "\n",
        "        return trajectory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GAKRZw7SWaq",
        "colab_type": "text"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM1cRAHgMeRO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9770a52b-5462-4c38-867d-46067713504d"
      },
      "source": [
        "test_obs = test_env.reset()\n",
        "test_n_actions = test_env.action_space.n\n",
        "test_agent = Agent(test_obs.shape[1:], test_n_actions)\n",
        "test_policy = Policy(test_agent)\n",
        "test_compute_value_targets = ComputeValueTargets(test_policy)\n",
        "\n",
        "test_trajectory = EnvRunner(test_env, test_policy, 2).get_next()\n",
        "assert len(test_compute_value_targets(test_trajectory)['value_targets']) == 2\n",
        "assert len(test_compute_value_targets(test_trajectory)['value_targets'][0]) == 8 \n",
        "print(\"Test passed\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3AU5R3Pip0q",
        "colab_type": "text"
      },
      "source": [
        "After computing value targets we will transform lists of interactions into tensors\n",
        "with the first dimension `batch_size` which is equal to `T * nenvs`, i.e. you essentially need\n",
        "to flatten the first two dimensions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-VyvqxMip0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_of_lists_to_tensor(l):\n",
        "    if type(l) == torch.Tensor:\n",
        "        return l \n",
        "\n",
        "    assert type(l) == list\n",
        "    for i in range(len(l)):\n",
        "        if type(l[i]) == torch.Tensor:\n",
        "            continue\n",
        "        if type(l[i]) == np.ndarray:\n",
        "            l[i] = torch.tensor(l[i])\n",
        "            continue\n",
        "\n",
        "        assert type(l[i]) == list\n",
        "        for j in range(len(l[i])):\n",
        "            if type(l[i][j]) == torch.Tensor:\n",
        "                continue\n",
        "            \n",
        "            l[i][j] = torch.Tensor([l[i][j]])\n",
        "        \n",
        "        if l[i][0].shape == torch.Size([]):\n",
        "            l[i] = torch.stack(l[i])\n",
        "        else:\n",
        "            l[i] = torch.cat(l[i])\n",
        "    return torch.stack(l)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Exxem4w5Tft3",
        "colab_type": "text"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aezLOjiNThNu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "00e1c978-8843-463e-f48b-faf2018840ce"
      },
      "source": [
        "assert torch.min(list_of_lists_to_tensor([[1, 2, 3], [3, 4, 5], [5, 6, 7]]) == torch.tensor([[1, 2, 3], [3, 4, 5], [5, 6, 7]])) == True\n",
        "assert torch.min(list_of_lists_to_tensor([torch.tensor([1, 2, 3]), torch.tensor([3, 4, 5]), torch.tensor([5, 6, 7])]) == torch.tensor([[1, 2, 3], [3, 4, 5], [5, 6, 7]])) == True\n",
        "print(\"Test passed\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqeIlc2oip0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MergeTimeBatch:\n",
        "    \"\"\" Merges first two axes typically representing time and env batch. \"\"\"\n",
        "    def __call__(self, trajectory):\n",
        "        for key, val in trajectory.items():\n",
        "            if key not in ['actions', 'observations', 'rewards', 'resets', 'logits', 'log_probs', 'values', 'value_targets']:\n",
        "                continue\n",
        "\n",
        "            val = list_of_lists_to_tensor(val)\n",
        "            val = val.reshape(-1, *val.shape[2:])\n",
        "            trajectory[key] = val\n",
        "        return trajectory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtdCpA0bY04F",
        "colab_type": "text"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwptwDusVvLW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f882ad3d-a3c8-4bdf-80a1-1f360c12ac9f"
      },
      "source": [
        "test_obs = test_env.reset()\n",
        "test_n_actions = test_env.action_space.n\n",
        "test_agent = Agent(test_obs.shape[1:], test_n_actions)\n",
        "test_policy = Policy(test_agent)\n",
        "test_merge_time_batch = MergeTimeBatch()\n",
        "test_trajectory = ComputeValueTargets(test_policy)(EnvRunner(test_env, test_policy, 2).get_next())\n",
        "\n",
        "test_res = test_merge_time_batch(test_trajectory)\n",
        "for key in ['actions', 'observations', 'rewards', 'resets', 'logits', 'log_probs', 'values', 'value_targets']:\n",
        "    assert test_res[key].shape[0] == 16\n",
        "\n",
        "print(\"Test passed\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGWGze8KiqhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MoveToDevice:\n",
        "    def __init__(self, device):\n",
        "        self.device = device\n",
        "    \n",
        "    def __call__(self, trajectory):\n",
        "        # This method should move all values to device\n",
        "        for key, val in trajectory.items():\n",
        "            if type(val) != torch.tensor:\n",
        "                continue\n",
        "            \n",
        "            trajectory[key] = val.to(self.device)\n",
        "        return trajectory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llVDB0mzip0z",
        "colab_type": "text"
      },
      "source": [
        "Now is the time to implement the advantage actor critic algorithm itself. You can look into your lecture,\n",
        "[Mnih et al. 2016](https://arxiv.org/abs/1602.01783) paper, and [lecture](https://www.youtube.com/watch?v=Tol_jw5hWnI&list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37&index=20) by Sergey Levine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpibdsDwip0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class A2C:\n",
        "    def __init__(\n",
        "        self,\n",
        "        policy,\n",
        "        optimizer,\n",
        "        value_loss_coef=0.005,\n",
        "        entropy_coef=0.002,\n",
        "        max_grad_norm=0.5\n",
        "    ):\n",
        "        self.policy = policy\n",
        "        self.optimizer = optimizer\n",
        "        self.value_loss_coef = value_loss_coef\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "    \n",
        "    def policy_loss(self, trajectory):\n",
        "        # You will need to compute advantages here. \n",
        "        advantages = trajectory['value_targets'] - trajectory['values']\n",
        "        return -torch.mean(trajectory['log_probs'] * advantages.detach())\n",
        "    \n",
        "    def value_loss(self, trajectory):\n",
        "        advantages = trajectory['value_targets'].detach() - trajectory['values']\n",
        "        value_loss = advantages.pow(2).mean()\n",
        "        return value_loss\n",
        "    \n",
        "    def entropy_loss(self, trajectory):\n",
        "        probs = nn.functional.softmax(trajectory['logits'], -1)\n",
        "        log_probs = nn.functional.log_softmax(trajectory['logits'], -1)\n",
        "        entropy = torch.sum(probs * log_probs, -1).mean()\n",
        "        return entropy\n",
        "    \n",
        "    def loss(self, trajectory):\n",
        "        value_loss = self.value_loss(trajectory)\n",
        "        policy_loss = self.policy_loss(trajectory)\n",
        "        entropy_loss = self.entropy_loss(trajectory)\n",
        "        \n",
        "        return self.value_loss_coef*value_loss + policy_loss + self.entropy_coef*entropy_loss, value_loss, policy_loss, entropy_loss\n",
        "    \n",
        "    def step(self, trajectory):\n",
        "        loss, value_loss, policy_loss, entropy_loss = self.loss(trajectory)\n",
        "        \n",
        "        loss.backward()\n",
        "        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), self.max_grad_norm)\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "        return loss, value_loss, policy_loss, entropy_loss, grad_norm\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOB9j6onip01",
        "colab_type": "text"
      },
      "source": [
        "Now you can train your model. With reasonable hyperparameters training on a single GTX1080 for 10 million steps across all batched environments (which translates to about 5 hours of wall clock time)\n",
        "it should be possible to achieve *average raw reward over last 100 episodes* (the average is taken over 100 last \n",
        "episodes in each environment in the batch) of about 600. You should plot this quantity with respect to \n",
        "`runner.step_var` &mdash; the number of interactions with all environments. It is highly \n",
        "encouraged to also provide plots of the following quantities (these are useful for debugging as well):\n",
        "\n",
        "* [Coefficient of Determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) between \n",
        "value targets and value predictions\n",
        "* Entropy of the policy $\\pi$\n",
        "* Value loss\n",
        "* Policy loss\n",
        "* Value targets\n",
        "* Value predictions\n",
        "* Gradient norm\n",
        "* Advantages\n",
        "* A2C loss\n",
        "\n",
        "For optimization we suggest you use RMSProp with learning rate starting from 7e-4 and linearly decayed to 0, smoothing constant (alpha in PyTorch and decay in TensorFlow) equal to 0.99 and epsilon equal to 1e-5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXnIafYCip02",
        "colab_type": "text"
      },
      "source": [
        "**Define Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibIjVVIGip02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "USE_CUDA = True\n",
        "TOTAL_EPOCH = 100000\n",
        "LAST_EPOCH = 0\n",
        "\n",
        "DROW_STEP = 200\n",
        "FIG_SIZE = 17, 8\n",
        "SAVE_STEP = 2000\n",
        "\n",
        "MOUNT_DIR = '/content/drive/'\n",
        "MODELS_FOLDER = MOUNT_DIR + 'My Drive/models/rl/a2c/' if USE_COLAB else 'models/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp2ltx3Yip04",
        "colab_type": "text"
      },
      "source": [
        "**Mount GDrive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1vP2iFjip05",
        "colab_type": "code",
        "outputId": "329972a7-0a4e-437f-8395-5edb24f33b48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "if USE_COLAB:\n",
        "    from google.colab import files, drive\n",
        "    drive.mount(MOUNT_DIR)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEw5XB0Qip07",
        "colab_type": "text"
      },
      "source": [
        "**Select Device**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeG-UGypip08",
        "colab_type": "code",
        "outputId": "44d2b9c2-d452-4b58-f2e7-90c964de001a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "if USE_CUDA and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(\"device: \", device)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device:  cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYACpzGpip0-",
        "colab_type": "text"
      },
      "source": [
        "**Set up env**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm_QRC2fip0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=8)\n",
        "evaluate_env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=1)\n",
        "obs = env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEXFUnemip1B",
        "colab_type": "text"
      },
      "source": [
        "**define model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR3nRovBip1B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "2ff0ed40-b383-4431-f908-a2c64318e963"
      },
      "source": [
        "if LAST_EPOCH:\n",
        "    model = torch.load(f'{MODELS_FOLDER}a2c_{LAST_EPOCH}_epoch', map_location=torch.device('cpu'))\n",
        "else:\n",
        "    model = Agent(obs.shape[1:], env.action_space.n)\n",
        "model.to(device)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Agent(\n",
              "  (body_network): Sequential(\n",
              "    (Conv1): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2))\n",
              "    (Act1): ReLU()\n",
              "    (Conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\n",
              "    (Act2): ReLU()\n",
              "    (Conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
              "    (Act3): ReLU()\n",
              "    (flatten): Flatten()\n",
              "    (FC1): Linear(in_features=5184, out_features=128, bias=True)\n",
              "    (Act4): ReLU()\n",
              "  )\n",
              "  (v_head): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (logits_head): Linear(in_features=128, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5pkicW8ip1E",
        "colab_type": "text"
      },
      "source": [
        "**define stuff**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KTJpCHUip1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy = Policy(model)\n",
        "runner = EnvRunner(\n",
        "    env,\n",
        "    policy,\n",
        "    nsteps=50,\n",
        "    transforms=[\n",
        "                ComputeValueTargets(policy),\n",
        "                MergeTimeBatch(),\n",
        "                MoveToDevice(device)\n",
        "                ]\n",
        ")\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=7e-2, alpha=0.99, eps=1e-5)\n",
        "a2c = A2C(policy, optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFyf948Fip1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def moving_average(x, span=100): return DataFrame(\n",
        "    {'x': np.asarray(x)}).x.ewm(span=span).mean().values\n",
        "\n",
        "def play_and_evaluate(env, agent, tmax=300):\n",
        "    \"\"\"\n",
        "    This function should \n",
        "    - run a full game, actions given by agent.getAction(s)\n",
        "    - train agent using agent.update(...) whenever possible\n",
        "    - return total reward\n",
        "    :param replay: ReplayBuffer where agent can store and sample (s,a,r,s',done) tuples.\n",
        "        If None, do not use experience replay\n",
        "    \"\"\"\n",
        "    total_reward = 0.0\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(tmax):\n",
        "        # get agent to pick action given state s\n",
        "        a = agent.get_action(s)\n",
        "\n",
        "        next_s, r, done, _ = env.step(a)\n",
        "\n",
        "        s = next_s\n",
        "        total_reward += r[0]\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plGpMi-cip1I",
        "colab_type": "text"
      },
      "source": [
        "**learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVO5pU25ip1J",
        "colab_type": "code",
        "outputId": "0b098cd3-1df9-48b0-8307-97a05c41ebe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "curves_names = ['A2c loss', 'value_loss', 'policy_loss', 'enropy_loss', 'grad_norm', 'Reward']\n",
        "curves = [[], [], [], [], [], []]\n",
        "\n",
        "for i in trange(LAST_EPOCH, TOTAL_EPOCH):\n",
        "    trajectory = runner.get_next()\n",
        "    losses = a2c.step(trajectory)\n",
        "    \n",
        "    for j, loss in enumerate(losses):\n",
        "        curves[j].append(loss)\n",
        "    curves[-1].append(play_and_evaluate(evaluate_env, model))\n",
        "    \n",
        "    if i and i % 20 == 0:\n",
        "        clear_output(True)\n",
        "        fig1, axs = plt.subplots(3, 2, figsize=FIG_SIZE, constrained_layout=True)\n",
        "        for j, curve in enumerate(curves):\n",
        "            axs[j // 2][j % 2].set_ylim((-10, 10))\n",
        "            axs[j // 2][j % 2].set_title(curves_names[j])\n",
        "            try:\n",
        "                smoth_curve = moving_average(curve, span=100)\n",
        "            except:\n",
        "                print(f\"Error with {curves_names[j]}\")\n",
        "                print(curve)\n",
        "                continue\n",
        "            axs[j // 2][j % 2].plot(smoth_curve, label=f'{curves_names[j]}= {smoth_curve[-1]}')\n",
        "        fig1.legend()\n",
        "        plt.show()\n",
        "        "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100000 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-ce71d883c6bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'curves_names = [\\'A2c loss\\', \\'value_loss\\', \\'policy_loss\\', \\'enropy_loss\\', \\'grad_norm\\', \\'Reward\\']\\ncurves = [[], [], [], [], [], []]\\n\\nfor i in trange(LAST_EPOCH, TOTAL_EPOCH):\\n    trajectory = runner.get_next()\\n    losses = a2c.step(trajectory)\\n    \\n    for j, loss in enumerate(losses):\\n        curves[j].append(loss)\\n    curves[-1].append(play_and_evaluate(evaluate_env, model))\\n    \\n    if i and i % 20 == 0:\\n        clear_output(True)\\n        fig1, axs = plt.subplots(3, 2, figsize=FIG_SIZE, constrained_layout=True)\\n        for j, curve in enumerate(curves):\\n            axs[j // 2][j % 2].set_ylim((-10, 10))\\n            axs[j // 2][j % 2].set_title(curves_names[j])\\n            try:\\n                smoth_curve = moving_average(curve, span=100)\\n            except:\\n                print(f\"Error with {curves_names[j]}\")\\n                print(curve)\\n                continue\\n            axs[j // 2][j % 2].plot(smoth_curve, label=f\\'{curves_names[j]}= {smoth_curve[-1]}\\')\\n        fig1.legend()\\n        plt.show()\\n        '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/content/runners.py\u001b[0m in \u001b[0;36mget_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnsteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mobservations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"latest_observation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"latest_observation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"actions\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 raise ValueError(\"result of policy.act must contain 'actions' \"\n",
            "\u001b[0;32m<ipython-input-9-25b9d53f489e>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \"\"\"\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-a45fd6e77954>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mstate_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mstate_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mstate_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mstate_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-peARz6bip1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}