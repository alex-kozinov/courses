{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    USE_COLAB = True\n",
    "except:\n",
    "    USE_COLAB = False\n",
    "\n",
    "RUN_TESTS = True\n",
    "\n",
    "if USE_COLAB:\n",
    "    print(\"Don't forget to avoid disconnections:\")\n",
    "    print(\"\"\"\n",
    "    function ClickConnect(){\n",
    "        console.log(\"Clicking\"); \n",
    "        document.querySelector(\"colab-connect-button\").click() \n",
    "    }\n",
    "    setInterval(ClickConnect,60000)\n",
    "    \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "import scipy.signal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "import gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from itertools import count\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use TRPO to train evil robots! (pick any of two)\n",
    "* [MuJoCo robots](https://gym.openai.com/envs#mujoco)\n",
    "* [Box2d robot](https://gym.openai.com/envs/BipedalWalker-v2)\n",
    "\n",
    "The catch here is that those environments have continuous action spaces. \n",
    "\n",
    "Luckily, TRPO is a policy gradient method, so it's gonna work for any parametric $\\pi_\\theta(a|s)$. We recommend starting with gaussian policy:\n",
    "\n",
    "$$\\pi_\\theta(a|s) = N(\\mu_\\theta(s),\\sigma^2_\\theta(s)) = {1 \\over \\sqrt { 2 \\pi {\\sigma^2}_\\theta(s) } } e^{ (a - \n",
    "\\mu_\\theta(s))^2 \\over 2 {\\sigma^2}_\\theta(s) } $$\n",
    "\n",
    "In the $\\sqrt { 2 \\pi {\\sigma^2}_\\theta(s) }$ clause, $\\pi$ means ~3.1415926, not agent's policy.\n",
    "\n",
    "This essentially means that you will need two output layers:\n",
    "* $\\mu_\\theta(s)$, a dense layer with linear activation\n",
    "* ${\\sigma^2}_\\theta(s)$, a dense layer with activation tf.exp (to make it positive; like rho from bandits)\n",
    "\n",
    "For multidimensional actions, you can use fully factorized gaussian (basically a vector of gaussians).\n",
    "\n",
    "__bonus task__: compare performance of continuous action space method to action space discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "Explore given environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex-kozinov/opt/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(\"BipedalWalker-v2\")\n",
    "test_obs = test_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24,)\n",
      "Box(4,)\n"
     ]
    }
   ],
   "source": [
    "print(test_obs.shape)\n",
    "print(test_env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 possible actions: for all 4 Joints. Each parameter in range [-1, 1]. \n",
    "\n",
    "Let model learn it by **iteslf**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defenition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) in (nn.Linear, nn.Conv2d):\n",
    "        nn.init.orthogonal_(m.weight.data, np.sqrt(2.0))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "def get_body_network(layers_size):\n",
    "    layers = []\n",
    "    for in_size, out_size in zip(layers_size[:-1], layers_size[1:]):\n",
    "        layers.append(nn.Linear(in_size, out_size))\n",
    "        layers.append(nn.Tanh())\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defenition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRPOAgents(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, hidden_size=[128, 64, 64]):\n",
    "        '''\n",
    "        Here you should define your model\n",
    "        '''\n",
    "        super(TRPOAgent, self).__init__()\n",
    "        \n",
    "        layers_size = [state_shape[0]] + hidden_size\n",
    "\n",
    "        self.body = get_body_network(layers_size).apply(init_weights)\n",
    "        self.mu_head = nn.Linear(hidden_size[1], n_actions).apply(init_weights)\n",
    "        self.log_std_head = nn.Parameter(torch.zeros(n_actions))\n",
    "    \n",
    "    \n",
    "    def forward(self, states):\n",
    "        \"\"\"\n",
    "        takes agent's observation (Variable), returns gaussian distribution parameters\n",
    "        :param states: a batch of states, shape = [batch_size, state_shape]\n",
    "        \"\"\"\n",
    "        hidden_state = self.body(states)\n",
    "        \n",
    "        mu = self.mu_head(hidden_state)\n",
    "        log_std = self.log_std_head\n",
    "        std = torch.exp(log_std)\n",
    "        return mu, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "test_model = TRPOAgent(test_obs.shape, test_env.action_space.shape[0])\n",
    "test_input1d = torch.ones(24)\n",
    "test_input2d = torch.ones((2, 24))\n",
    "\n",
    "mu, std = test_model(test_input1d)\n",
    "assert mu.shape == torch.Size([4])\n",
    "assert (std == torch.tensor([1., 1., 1., 1.])).all()\n",
    "\n",
    "mu, std = test_model(test_input2d)\n",
    "assert mu.shape == torch.Size([2, 4])\n",
    "assert (std == torch.tensor([1., 1., 1., 1.])).all()\n",
    "\n",
    "print('Test passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def get_prob_dist(self, states):\n",
    "        \"\"\"\n",
    "        takes agent's observation (Variable), returns gaussian distribution\n",
    "        :param states: a batch of states, shape = [batch_size, state_shape]\n",
    "        \"\"\"\n",
    "\n",
    "        mu, std = self.model(states)\n",
    "        dist = MultivariateNormal(mu, torch.diag(std))\n",
    "        return dist\n",
    "\n",
    "    def act(self, inputs, training=False):\n",
    "        '''\n",
    "        Samples action from policy distribution\n",
    "        :param: inputs - observations vector\n",
    "        :returns: action (single integer) and probabilities class for all actions\n",
    "        '''\n",
    "        states = Variable(torch.FloatTensor(inputs))\n",
    "        \n",
    "        if training:\n",
    "            with torch.no_grad():\n",
    "                distribution = self.get_prob_dist(states)\n",
    "        else:\n",
    "            distribution = self.get_prob_dist(states)\n",
    "        \n",
    "        actions = distribution.sample()\n",
    "        return actions, distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "test_model = TRPOAgent(test_obs.shape, test_env.action_space.shape[0])\n",
    "test_policy = Policy(test_model)\n",
    "test_input1d = np.ones(24)\n",
    "test_input2d = np.ones((2, 24))\n",
    "\n",
    "test_actions, test_distribution = test_policy.act(test_input1d)\n",
    "assert test_actions.shape == torch.Size([4])\n",
    "assert (test_distribution.stddev == torch.ones(4)).all()\n",
    "\n",
    "test_actions, test_distribution = test_policy.act(test_input2d)\n",
    "assert test_actions.shape == torch.Size([2, 4])\n",
    "assert (test_distribution.stddev == torch.ones(4)).all()\n",
    "print('Test passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. flat parameters operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flat_params_from(model):\n",
    "    params = []\n",
    "    for param in model.parameters():\n",
    "        params.append(param.data.view(-1))\n",
    "\n",
    "    flat_params = torch.cat(params)\n",
    "    return flat_params\n",
    "\n",
    "\n",
    "def set_flat_params_to(model, flat_params):\n",
    "    prev_ind = 0\n",
    "    for param in model.parameters():\n",
    "        flat_size = int(np.prod(list(param.size())))\n",
    "        param.data.copy_(\n",
    "            flat_params[prev_ind:prev_ind + flat_size].view(param.size()))\n",
    "        prev_ind += flat_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. count cummulative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cummulative_returns(r, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Computes cummulative discounted rewards given immediate rewards\n",
    "    G_i = r_i + gamma*r_{i+1} + gamma^2*r_{i+2} + ...\n",
    "    Also known as R(s,a).\n",
    "    \"\"\"\n",
    "    r = np.array(r)\n",
    "    assert r.ndim >= 1\n",
    "    return scipy.signal.lfilter([1], [1, -gamma], r[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed\n"
     ]
    }
   ],
   "source": [
    "# simple demo on rewards [0,0,1,0,0,1]\n",
    "assert (get_cummulative_returns([0, 0, 1, 0, 0, 1], gamma=1.) == np.array([2., 2., 2., 1., 1., 1.])).all()\n",
    "print('Tests passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. transformation list of distributions to one multidimensial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_normal(normals):\n",
    "    \"\"\"\n",
    "    cat list of Normal distributions to single distribution\n",
    "    :param: normals - list of Normal distributions\n",
    "    \"\"\"\n",
    "    mu_s = [dist.mean for dist in normals]\n",
    "    std_s = [dist.stddev for dist in normals]\n",
    "    if len(mu_s[0].shape) == 1:\n",
    "        return Normal(torch.stack(mu_s), torch.stack(std_s))\n",
    "    else:\n",
    "        return Normal(torch.cat(mu_s), torch.cat(std_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "A = Normal(torch.tensor([0., 0.]), torch.tensor([1., 1.]))\n",
    "B = Normal(torch.tensor([2., 2.]), torch.tensor([3., 3.]))\n",
    "AB = cat_normal([A, B])\n",
    "ABAB = cat_normal([AB, AB])\n",
    "\n",
    "assert (AB.mean == torch.tensor([[0., 0.], [2., 2.]])).all()\n",
    "assert (AB.stddev == torch.tensor([[1., 1.], [3., 3.]])).all()\n",
    "assert (ABAB.mean.shape == torch.Size([4, 2]))\n",
    "print('Test passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(env, agent, max_pathlength=2500, n_timesteps=50000):\n",
    "    \"\"\"\n",
    "    Generate rollouts for training.\n",
    "    :param: env - environment in which we will make actions to generate rollouts.\n",
    "    :param: act - the function that can return policy and action given observation.\n",
    "    :param: max_pathlength - maximum size of one path that we generate.\n",
    "    :param: n_timesteps - total sum of sizes of all pathes we generate.\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "\n",
    "    total_timesteps = 0\n",
    "    while total_timesteps < n_timesteps:\n",
    "        obervations, actions, rewards, probs_dist = [], [], [], []\n",
    "        obervation = env.reset()\n",
    "        for _ in range(max_pathlength):\n",
    "            action, prob_dist = agent.act(obervation)\n",
    "            obervations.append(obervation)\n",
    "            actions.append(action)\n",
    "            probs_dist.append(prob_dist)\n",
    "            obervation, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            total_timesteps += 1\n",
    "            if done or total_timesteps == n_timesteps:\n",
    "                path = {\"observations\": np.array(obervations),\n",
    "                        \"policy\": cat_normal(probs_dist),\n",
    "                        \"actions\": np.array(actions),\n",
    "                        \"rewards\": np.array(rewards),\n",
    "                        \"cumulative_returns\": get_cummulative_returns(rewards, gamma=0.99),\n",
    "                        }\n",
    "                paths.append(path)\n",
    "                break\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'observations': array([[ 2.74737785e-03, -1.57472654e-05,  1.22499783e-03,\n",
      "        -1.59998930e-02,  9.19331312e-02, -1.61658891e-03,\n",
      "         8.60285625e-01,  2.59747620e-03,  1.00000000e+00,\n",
      "         3.23389731e-02, -1.61647156e-03,  8.53834331e-01,\n",
      "         1.14326132e-03,  1.00000000e+00,  4.40814018e-01,\n",
      "         4.45820123e-01,  4.61422771e-01,  4.89550203e-01,\n",
      "         5.34102798e-01,  6.02461040e-01,  7.09148884e-01,\n",
      "         8.85931849e-01,  1.00000000e+00,  1.00000000e+00],\n",
      "       [ 7.57573347e-04, -1.22843349e-02, -8.50787848e-03,\n",
      "         3.14872551e-02,  4.03214425e-01,  9.99907255e-01,\n",
      "         1.64986551e-01, -9.99957800e-01,  1.00000000e+00,\n",
      "        -3.80436748e-01, -6.59479856e-01,  1.76042593e+00,\n",
      "         9.97951349e-01,  1.00000000e+00,  4.53531981e-01,\n",
      "         4.58682507e-01,  4.74735320e-01,  5.03674269e-01,\n",
      "         5.49512267e-01,  6.19842708e-01,  7.29608595e-01,\n",
      "         9.11491930e-01,  1.00000000e+00,  1.00000000e+00],\n",
      "       [ 1.70010999e-02,  4.19181108e-02,  9.41214949e-03,\n",
      "         4.74022150e-03,  4.12851244e-01,  2.62406975e-01,\n",
      "         1.14369154e-01, -1.00000008e+00,  0.00000000e+00,\n",
      "         7.73042291e-02, -3.07192028e-01,  1.28587523e+00,\n",
      "         2.20268965e-04,  1.00000000e+00,  4.60916102e-01,\n",
      "         4.66150492e-01,  4.82464671e-01,  5.11874735e-01,\n",
      "         5.58459044e-01,  6.29934549e-01,  7.41487622e-01,\n",
      "         9.26332235e-01,  1.00000000e+00,  1.00000000e+00],\n",
      "       [ 4.59475629e-02,  5.62412691e-02,  1.23234433e-02,\n",
      "         7.97149599e-03,  3.84689897e-01, -2.42113918e-01,\n",
      "         2.14809656e-01,  6.62210027e-01,  0.00000000e+00,\n",
      "        -3.04499865e-02, -4.13721651e-01,  1.30583793e+00,\n",
      "        -7.45058060e-09,  1.00000000e+00,  4.63151187e-01,\n",
      "         4.68410969e-01,  4.84804243e-01,  5.14356971e-01,\n",
      "         5.61167121e-01,  6.32989287e-01,  7.45083272e-01,\n",
      "         9.30824280e-01,  1.00000000e+00,  1.00000000e+00],\n",
      "       [ 6.55402616e-02,  3.25324535e-02, -5.81564873e-03,\n",
      "         2.18070149e-04,  3.77668917e-01,  3.19778025e-01,\n",
      "         4.06787932e-01,  9.99992688e-01,  0.00000000e+00,\n",
      "        -8.23414177e-02, -2.21593052e-01,  1.27781585e+00,\n",
      "         6.82771206e-05,  1.00000000e+00,  4.69311506e-01,\n",
      "         4.74641234e-01,  4.91252571e-01,  5.21198332e-01,\n",
      "         5.68631172e-01,  6.41408563e-01,  7.54993558e-01,\n",
      "         9.43205059e-01,  1.00000000e+00,  1.00000000e+00]]), 'policy': Normal(loc: torch.Size([5, 4]), scale: torch.Size([5, 4])), 'actions': array([[ 2.659769  , -1.2845905 , -0.5838814 ,  1.056735  ],\n",
      "       [-0.5704378 , -0.55420697, -1.0983752 ,  0.25245738],\n",
      "       [ 0.55088365,  0.43483755, -0.9569828 ,  0.6879758 ],\n",
      "       [ 1.0148094 ,  0.35230345, -0.38124084,  0.45162487],\n",
      "       [-2.6214306 , -1.1770422 ,  0.05991492,  0.2800969 ]],\n",
      "      dtype=float32), 'rewards': array([-0.09030461, -0.1860112 , -0.21019643, -0.16328908, -0.23069938]), 'cumulative_returns': array([-0.86051736, -0.77799268, -0.59796108, -0.39168147, -0.23069938])}\n",
      "Test Passed\n"
     ]
    }
   ],
   "source": [
    "test_agent = TRPOAgent(test_obs.shape, test_env.action_space.shape[0])\n",
    "paths = rollout(test_env, test_agent, max_pathlength=5, n_timesteps=100)\n",
    "print(paths[-1])\n",
    "assert (type(paths[0]['policy']) == torch.distributions.normal.Normal)\n",
    "assert (paths[0]['policy'].mean.shape == (5,test_env.action_space.shape[0]))\n",
    "assert (paths[0]['cumulative_returns'].shape == (5,))\n",
    "assert (paths[0]['rewards'].shape == (5,))\n",
    "assert (paths[0]['observations'].shape == (5,)+test_obs.shape)\n",
    "assert (paths[0]['actions'].shape == (5,test_env.action_space.shape[0]))\n",
    "print('Test Passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Surrogate loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the loss functions and something else for actual TRPO training.\n",
    "\n",
    "The surrogate reward should be\n",
    "$$J_{surr}= {1 \\over N} \\sum\\limits_{i=0}^N \\frac{\\pi_{\\theta}(s_i, a_i)}{\\pi_{\\theta_{old}}(s_i, a_i)}A_{\\theta_{old}(s_i, a_i)}$$\n",
    "\n",
    "For simplicity, let's use cummulative returns instead of advantage for now:\n",
    "$$J'_{surr}= {1 \\over N} \\sum\\limits_{i=0}^N \\frac{\\pi_{\\theta}(s_i, a_i)}{\\pi_{\\theta_{old}}(s_i, a_i)}G_{\\theta_{old}(s_i, a_i)}$$\n",
    "\n",
    "Or alternatively, minimize the surrogate loss:\n",
    "$$ L_{surr} = - J'_{surr} $$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(agent, observations, actions, cummulative_returns, old_prob_dist):\n",
    "    \"\"\"\n",
    "    Computes TRPO objective\n",
    "    :param: observations - batch of observations\n",
    "    :param: actions - batch of actions\n",
    "    :param: cummulative_returns - batch of cummulative returns\n",
    "    :param: old_prob_dist - torch gaussian distribution with shape of actions batch shape.\n",
    "    :returns: scalar value of the objective function\n",
    "    \"\"\"\n",
    "    batch_size, n_actions = actions.shape\n",
    "    \n",
    "    assert old_prob_dist.mean.shape == torch.Size([batch_size, n_actions])\n",
    "    \n",
    "    prob_dist = agent.get_prob_dist(observations)\n",
    "    \n",
    "    probs_for_actions = torch.exp(prob_dist.log_prob(torch.tensor(actions)))\n",
    "    old_probs_for_actions = torch.exp(old_prob_dist.log_prob(torch.tensor(actions)))\n",
    "    \n",
    "    probs_for_actions = torch.prod(probs_for_actions, dim=1)\n",
    "    old_probs_for_actions = torch.prod(old_probs_for_actions, dim=1)\n",
    "\n",
    "    Loss = -torch.mean(probs_for_actions / old_probs_for_actions * cummulative_returns)\n",
    "\n",
    "    assert Loss.shape == torch.Size([])\n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "test_agent = TRPOAgent(test_obs.shape, test_env.action_space.shape[0])\n",
    "paths = rollout(test_env, test_agent, max_pathlength=5, n_timesteps=100)\n",
    "\n",
    "with torch.no_grad():\n",
    "    assert get_loss(\n",
    "        test_agent,\n",
    "        paths[0]['observations'],\n",
    "        paths[0]['actions'],\n",
    "        paths[0]['cumulative_returns'],\n",
    "        paths[0]['policy']\n",
    "    ) > 0\n",
    "print('Test passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. KL metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ascend these gradients as long as our $pi_\\theta(a|s)$ satisfies the constraint\n",
    "$$E_{s,\\pi_{\\Theta_{t}}}\\Big[KL(\\pi(\\Theta_{t}, s) \\:||\\:\\pi(\\Theta_{t+1}, s))\\Big]< \\alpha$$\n",
    "\n",
    "\n",
    "The Kullback-Leibler divergence between a Gaussian distribution $p$ with mean $\\mu_1$ and variance $\\sigma_1^2$ and a Gaussian distribution $q$ with mean $\\mu_2$ and variance $\\sigma_2^2$ is following: \n",
    "$$\\displaystyle \\text{KL}(p, q) = \\log \\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kl(agent, observations, actions, cummulative_returns, old_prob_dist):\n",
    "    \"\"\"\n",
    "    Computes KL-divergence between network policy and old policy\n",
    "    :param: observations - batch of observations\n",
    "    :param: actions - batch of actions\n",
    "    :param: cummulative_returns - batch of cummulative returns (we don't need it actually)\n",
    "    :param: old_prob_dist - torch gaussian distribution with shape of actions batch shape.\n",
    "    :returns: scalar value of the KL-divergence\n",
    "    \"\"\"\n",
    "    batch_size, n_actions = actions.shape\n",
    "    \n",
    "    assert old_prob_dist.mean.shape == torch.Size([batch_size, n_actions])\n",
    "    \n",
    "    prob_dist = agent.get_prob_dist(observations)\n",
    "    \n",
    "    # Compute Kullback-Leibler divergence (see formula above)\n",
    "    # shape [batch_size, n_actions]\n",
    "    kl_s = torch.log(prob_dist.stddev / old_prob_dist.stddev) +\\\n",
    "        (old_prob_dist.stddev ** 2 + (old_prob_dist.mean - prob_dist.mean) ** 2) / (2 * prob_dist.stddev ** 2) - 0.5\n",
    "    kl = torch.mean(torch.sum(kl_s, dim=1))\n",
    "\n",
    "    assert kl_s.shape == torch.Size([batch_size, n_actions])\n",
    "    assert kl.shape == torch.Size([])\n",
    "    assert (kl > -0.0001).all() and (kl < 10000).all()\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "test_agent = TRPOAgent(test_obs.shape, test_env.action_space.shape[0])\n",
    "paths = rollout(test_env, test_agent, max_pathlength=5, n_timesteps=100)\n",
    "\n",
    "with torch.no_grad():\n",
    "    assert get_kl(\n",
    "        test_agent,\n",
    "        paths[0]['observations'],\n",
    "        paths[0]['actions'],\n",
    "        paths[0]['cumulative_returns'],\n",
    "        paths[0]['policy']\n",
    "    ) < 0.001\n",
    "\n",
    "print('Test passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(agent, observations):\n",
    "    \"\"\"\n",
    "    Computes entropy of the network policy \n",
    "    :param: observations - batch of observations\n",
    "    :returns: scalar value of the entropy\n",
    "    \"\"\"\n",
    "\n",
    "    probs_dist = agent.get_prob_dist(observations)\n",
    "    entropy = torch.mean(probs_dist.entropy())\n",
    "\n",
    "    assert entropy.shape == torch.Size([])\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4189, grad_fn=<MeanBackward0>)\n",
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "test_agent = TRPOAgent(test_obs.shape, test_env.action_space.shape[0])\n",
    "print(get_entropy(test_agent, test_obs))\n",
    "print('Test passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Linear search\n",
    "TRPO in its core involves ascending surrogate policy gradient constrained by KL divergence. \n",
    "\n",
    "In order to enforce this constraint, we're gonna use linesearch. You can find out more about it [here](https://en.wikipedia.org/wiki/Linear_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linesearch(f, x, fullstep, max_kl):\n",
    "    \"\"\"\n",
    "    Linesearch finds the best parameters of neural networks in the direction of fullstep contrainted by KL divergence.\n",
    "    :param: f - function that returns loss, kl and arbitrary third component.\n",
    "    :param: x - old parameters of neural network.\n",
    "    :param: fullstep - direction in which we make search.\n",
    "    :param: max_kl - constraint of KL divergence.\n",
    "    :returns:\n",
    "    \"\"\"\n",
    "    max_backtracks = 10\n",
    "    loss, _, = f(x)\n",
    "    for stepfrac in .5**np.arange(max_backtracks):\n",
    "        xnew = x + stepfrac * fullstep\n",
    "        new_loss, kl = f(xnew)\n",
    "        actual_improve = new_loss - loss\n",
    "        if kl.data.numpy() <= max_kl and actual_improve.data.numpy() < 0:\n",
    "            x = xnew\n",
    "            loss = new_loss\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Conjugate gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since TRPO includes contrainted optimization, we will need to solve Ax=b using conjugate gradients.\n",
    "\n",
    "In general, CG is an algorithm that solves Ax=b where A is positive-defined. A is Hessian matrix so A is positive-defined. You can find out more about them [here](https://en.wikipedia.org/wiki/Conjugate_gradient_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(f_Ax, b, cg_iters=10, residual_tol=1e-10):\n",
    "    \"\"\"\n",
    "    This method solves system of equation Ax=b using iterative method called conjugate gradients\n",
    "    :f_Ax: function that returns Ax\n",
    "    :b: targets for Ax\n",
    "    :cg_iters: how many iterations this method should do\n",
    "    :residual_tol: epsilon for stability\n",
    "    \"\"\"\n",
    "    p = b.clone()\n",
    "    r = b.clone()\n",
    "    x = torch.zeros(b.size())\n",
    "    rdotr = torch.sum(r*r)\n",
    "    for i in range(cg_iters):\n",
    "        z = f_Ax(p)\n",
    "        v = rdotr / (torch.sum(p*z) + 1e-8)\n",
    "        x += v * p\n",
    "        r -= v * z\n",
    "        newrdotr = torch.sum(r*r)\n",
    "        mu = newrdotr / (rdotr + 1e-8)\n",
    "        p = r + mu * p\n",
    "        rdotr = newrdotr\n",
    "        if rdotr < residual_tol:\n",
    "            break\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.70671541 -1.51468377 -0.36074676 -2.5384675   2.74491156  2.4741922\n",
      " -1.37270964  1.09881173]\n",
      "[-0.70670116 -1.5146954  -0.36071748 -2.5384498   2.7449124   2.4741642\n",
      " -1.3727542   1.098823  ]\n",
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "# This code validates conjugate gradients\n",
    "A = np.random.rand(8, 8)\n",
    "A = np.matmul(np.transpose(A), A)\n",
    "\n",
    "\n",
    "def f_Ax(x):\n",
    "    return torch.matmul(torch.FloatTensor(A), x.view((-1, 1))).view(-1)\n",
    "\n",
    "\n",
    "b = np.random.rand(8)\n",
    "\n",
    "w = np.matmul(np.matmul(inv(np.matmul(np.transpose(A), A)),\n",
    "                        np.transpose(A)), b.reshape((-1, 1))).reshape(-1)\n",
    "print(w)\n",
    "print(conjugate_gradient(f_Ax, torch.FloatTensor(b)).numpy())\n",
    "print('Test passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. update_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we construct the whole update step function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_step(agent, observations, actions, cummulative_returns, old_prob_dist, max_kl):\n",
    "    \"\"\"\n",
    "    This function does the TRPO update step\n",
    "    :param: observations - batch of observations\n",
    "    :param: actions - batch of actions\n",
    "    :param: cummulative_returns - batch of cummulative returns\n",
    "    :param: old_prob_dist - torch gaussian distribution with shape of actions batch shape.\n",
    "    :param: max_kl - controls how big KL divergence may be between old and new policy every step.\n",
    "    :returns: KL between new and old policies and the value of the loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    # Here we prepare the information\n",
    "    actions = torch.LongTensor(actions)\n",
    "    cummulative_returns = Variable(torch.FloatTensor(cummulative_returns))\n",
    "#     old_probs = Variable(torch.FloatTensor(old_probs))\n",
    "\n",
    "    # Here we compute gradient of the loss function\n",
    "    loss = get_loss(agent, observations, actions,\n",
    "                    cummulative_returns, old_prob_dist)\n",
    "    grads = torch.autograd.grad(loss, agent.parameters())\n",
    "    loss_grad = torch.cat([grad.view(-1) for grad in grads]).data\n",
    "\n",
    "    def Fvp(v):\n",
    "        # Here we compute Fx to do solve Fx = g using conjugate gradients\n",
    "        # We actually do here a couple of tricks to compute it efficiently\n",
    "\n",
    "        kl = get_kl(agent, observations, actions,\n",
    "                    cummulative_returns, old_prob_dist)\n",
    "\n",
    "        grads = torch.autograd.grad(kl, agent.parameters(), create_graph=True)\n",
    "        flat_grad_kl = torch.cat([grad.view(-1) for grad in grads])\n",
    "\n",
    "        kl_v = (flat_grad_kl * Variable(v)).sum()\n",
    "        grads = torch.autograd.grad(kl_v, agent.parameters())\n",
    "        flat_grad_grad_kl = torch.cat(\n",
    "            [grad.contiguous().view(-1) for grad in grads]).data\n",
    "\n",
    "        return flat_grad_grad_kl + v * 0.1\n",
    "\n",
    "    # Here we solveolve Fx = g system using conjugate gradients\n",
    "    stepdir = conjugate_gradient(Fvp, -loss_grad, 10)\n",
    "\n",
    "    # Here we compute the initial vector to do linear search\n",
    "    shs = 0.5 * (stepdir * Fvp(stepdir)).sum(0, keepdim=True)\n",
    "\n",
    "    lm = torch.sqrt(shs / max_kl)\n",
    "    fullstep = stepdir / lm[0]\n",
    "\n",
    "    neggdotstepdir = (-loss_grad * stepdir).sum(0, keepdim=True)\n",
    "\n",
    "    # Here we get the start point\n",
    "    prev_params = get_flat_params_from(agent)\n",
    "\n",
    "    def get_loss_kl(params):\n",
    "        # Helper for linear search\n",
    "        set_flat_params_to(agent, params)\n",
    "        return [get_loss(agent, observations, actions, cummulative_returns, old_prob_dist),\n",
    "                get_kl(agent, observations, actions, cummulative_returns, old_prob_dist)]\n",
    "\n",
    "    # Here we find our new parameters\n",
    "    new_params = linesearch(get_loss_kl, prev_params, fullstep, max_kl)\n",
    "\n",
    "    # And we set it to our network\n",
    "    set_flat_params_to(agent, new_params)\n",
    "\n",
    "    return get_loss_kl(new_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is hyperparameter of TRPO. It controls how big KL divergence may be between old and new policy every step.\n",
    "max_kl = 0.01\n",
    "numeptotal = 0  # this is number of episodes that we played.\n",
    "\n",
    "rollout_max_pathlength = 3000\n",
    "rollout_n_timesteps = 500000\n",
    "# rollout_max_pathlength = 30\n",
    "# rollout_n_timesteps = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Objects definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BipedalWalker-v2\")\n",
    "observation_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TRPOAgent(\n",
       "  (body): Sequential(\n",
       "    (0): Linear(in_features=24, out_features=128, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (5): Tanh()\n",
       "  )\n",
       "  (mu_head): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = TRPOAgent(observation_shape, n_actions)\n",
    "agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** Iteration 1 ************\n",
      "Rollout\n",
      "Made rollout\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex-kozinov/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n",
      "/Users/alex-kozinov/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of episodes:                 803\n",
      "Average sum of rewards per episode:       -122.05639855127235\n",
      "Std of rewards per episode:               23.78502772787846\n",
      "Time elapsed:                             12.54 mins\n",
      "KL between old and new distribution:      0.009968565\n",
      "Entropy:                                  1.459826\n",
      "Surrogate loss:                           11.7816\n",
      "Total paths:                              803\n",
      "Mean path length:                         622.66500622665\n",
      "\n",
      "********** Iteration 2 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 1717\n",
      "Average sum of rewards per episode:       -122.48672018999443\n",
      "Std of rewards per episode:               25.014194899715044\n",
      "Time elapsed:                             20.83 mins\n",
      "KL between old and new distribution:      0.009995617\n",
      "Entropy:                                  1.4974455\n",
      "Surrogate loss:                           12.885286\n",
      "Total paths:                              914\n",
      "Mean path length:                         547.0459518599563\n",
      "\n",
      "********** Iteration 3 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 2497\n",
      "Average sum of rewards per episode:       -129.2267936030391\n",
      "Std of rewards per episode:               31.07467532611398\n",
      "Time elapsed:                             28.73 mins\n",
      "KL between old and new distribution:      0.009965871\n",
      "Entropy:                                  1.5353422\n",
      "Surrogate loss:                           12.408076\n",
      "Total paths:                              780\n",
      "Mean path length:                         641.025641025641\n",
      "\n",
      "********** Iteration 4 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 3336\n",
      "Average sum of rewards per episode:       -133.41595203299386\n",
      "Std of rewards per episode:               35.19067230817167\n",
      "Time elapsed:                             37.34 mins\n",
      "KL between old and new distribution:      0.009999323\n",
      "Entropy:                                  1.5715551\n",
      "Surrogate loss:                           13.699095\n",
      "Total paths:                              839\n",
      "Mean path length:                         595.9475566150179\n",
      "\n",
      "********** Iteration 5 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 4007\n",
      "Average sum of rewards per episode:       -147.0224865599259\n",
      "Std of rewards per episode:               42.133503361627604\n",
      "Time elapsed:                             46.12 mins\n",
      "KL between old and new distribution:      0.009968855\n",
      "Entropy:                                  1.6079388\n",
      "Surrogate loss:                           13.454019\n",
      "Total paths:                              671\n",
      "Mean path length:                         745.156482861401\n",
      "\n",
      "********** Iteration 6 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 4844\n",
      "Average sum of rewards per episode:       -143.87353793097074\n",
      "Std of rewards per episode:               40.65246444282347\n",
      "Time elapsed:                             54.88 mins\n",
      "KL between old and new distribution:      0.0099992445\n",
      "Entropy:                                  1.6415813\n",
      "Surrogate loss:                           15.469047\n",
      "Total paths:                              837\n",
      "Mean path length:                         597.3715651135007\n",
      "\n",
      "********** Iteration 7 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 5498\n",
      "Average sum of rewards per episode:       -159.68050762461172\n",
      "Std of rewards per episode:               45.05490862688805\n",
      "Time elapsed:                             63.65 mins\n",
      "KL between old and new distribution:      0.0099703865\n",
      "Entropy:                                  1.6794035\n",
      "Surrogate loss:                           15.034503\n",
      "Total paths:                              654\n",
      "Mean path length:                         764.525993883792\n",
      "\n",
      "********** Iteration 8 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 6264\n",
      "Average sum of rewards per episode:       -156.04356744642635\n",
      "Std of rewards per episode:               45.76135446601588\n",
      "Time elapsed:                             72.45 mins\n",
      "KL between old and new distribution:      0.009988074\n",
      "Entropy:                                  1.7128627\n",
      "Surrogate loss:                           16.412588\n",
      "Total paths:                              766\n",
      "Mean path length:                         652.7415143603133\n",
      "\n",
      "********** Iteration 9 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 6973\n",
      "Average sum of rewards per episode:       -164.84450723605815\n",
      "Std of rewards per episode:               47.88690554252416\n",
      "Time elapsed:                             81.24 mins\n",
      "KL between old and new distribution:      0.009983489\n",
      "Entropy:                                  1.7482014\n",
      "Surrogate loss:                           16.868742\n",
      "Total paths:                              709\n",
      "Mean path length:                         705.2186177715091\n",
      "\n",
      "********** Iteration 10 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 7724\n",
      "Average sum of rewards per episode:       -162.96190718383494\n",
      "Std of rewards per episode:               44.89733227633983\n",
      "Time elapsed:                             90.03 mins\n",
      "KL between old and new distribution:      0.009993075\n",
      "Entropy:                                  1.7821555\n",
      "Surrogate loss:                           17.729843\n",
      "Total paths:                              751\n",
      "Mean path length:                         665.7789613848203\n",
      "\n",
      "********** Iteration 11 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 8542\n",
      "Average sum of rewards per episode:       -163.83181396112107\n",
      "Std of rewards per episode:               46.06483147546288\n",
      "Time elapsed:                             98.89 mins\n",
      "KL between old and new distribution:      0.009983583\n",
      "Entropy:                                  1.8151343\n",
      "Surrogate loss:                           19.261639\n",
      "Total paths:                              818\n",
      "Mean path length:                         611.2469437652812\n",
      "\n",
      "********** Iteration 12 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 9380\n",
      "Average sum of rewards per episode:       -163.43870474229203\n",
      "Std of rewards per episode:               44.21911881173176\n",
      "Time elapsed:                             107.71 mins\n",
      "KL between old and new distribution:      0.009979559\n",
      "Entropy:                                  1.8482676\n",
      "Surrogate loss:                           19.81507\n",
      "Total paths:                              838\n",
      "Mean path length:                         596.6587112171837\n",
      "\n",
      "********** Iteration 13 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 10195\n",
      "Average sum of rewards per episode:       -167.8595858696202\n",
      "Std of rewards per episode:               46.588931465151\n",
      "Time elapsed:                             116.48 mins\n",
      "KL between old and new distribution:      0.009978637\n",
      "Entropy:                                  1.8822907\n",
      "Surrogate loss:                           20.11398\n",
      "Total paths:                              815\n",
      "Mean path length:                         613.4969325153374\n",
      "\n",
      "********** Iteration 14 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 11046\n",
      "Average sum of rewards per episode:       -164.58321243634563\n",
      "Std of rewards per episode:               44.07529777423826\n",
      "Time elapsed:                             125.24 mins\n",
      "KL between old and new distribution:      0.009987718\n",
      "Entropy:                                  1.9135629\n",
      "Surrogate loss:                           20.513409\n",
      "Total paths:                              851\n",
      "Mean path length:                         587.5440658049354\n",
      "\n",
      "********** Iteration 15 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 11871\n",
      "Average sum of rewards per episode:       -169.35928784232848\n",
      "Std of rewards per episode:               45.28181252710261\n",
      "Time elapsed:                             134.03 mins\n",
      "KL between old and new distribution:      0.009971095\n",
      "Entropy:                                  1.9481014\n",
      "Surrogate loss:                           20.91512\n",
      "Total paths:                              825\n",
      "Mean path length:                         606.060606060606\n",
      "\n",
      "********** Iteration 16 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 12753\n",
      "Average sum of rewards per episode:       -166.16872521114394\n",
      "Std of rewards per episode:               44.14187963233667\n",
      "Time elapsed:                             142.90 mins\n",
      "KL between old and new distribution:      0.0099838935\n",
      "Entropy:                                  1.9785076\n",
      "Surrogate loss:                           21.713642\n",
      "Total paths:                              882\n",
      "Mean path length:                         566.8934240362812\n",
      "\n",
      "********** Iteration 17 ************\n",
      "Rollout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made rollout\n",
      "Total number of episodes:                 13615\n",
      "Average sum of rewards per episode:       -168.77037496788924\n",
      "Std of rewards per episode:               44.01150171889259\n",
      "Time elapsed:                             151.69 mins\n",
      "KL between old and new distribution:      0.009963588\n",
      "Entropy:                                  2.0101202\n",
      "Surrogate loss:                           22.001259\n",
      "Total paths:                              862\n",
      "Mean path length:                         580.046403712297\n",
      "\n",
      "********** Iteration 18 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 14563\n",
      "Average sum of rewards per episode:       -163.4063818204725\n",
      "Std of rewards per episode:               43.69199867763981\n",
      "Time elapsed:                             160.52 mins\n",
      "KL between old and new distribution:      0.009964312\n",
      "Entropy:                                  2.0387876\n",
      "Surrogate loss:                           22.857285\n",
      "Total paths:                              948\n",
      "Mean path length:                         527.4261603375527\n",
      "\n",
      "********** Iteration 19 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 15461\n",
      "Average sum of rewards per episode:       -168.11187687556418\n",
      "Std of rewards per episode:               43.04560072322891\n",
      "Time elapsed:                             168.85 mins\n",
      "KL between old and new distribution:      0.009983265\n",
      "Entropy:                                  2.070183\n",
      "Surrogate loss:                           22.931368\n",
      "Total paths:                              898\n",
      "Mean path length:                         556.7928730512249\n",
      "\n",
      "********** Iteration 20 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 16450\n",
      "Average sum of rewards per episode:       -163.27496959342693\n",
      "Std of rewards per episode:               43.02381040810074\n",
      "Time elapsed:                             177.24 mins\n",
      "KL between old and new distribution:      0.009975429\n",
      "Entropy:                                  2.0980077\n",
      "Surrogate loss:                           23.997025\n",
      "Total paths:                              989\n",
      "Mean path length:                         505.5611729019211\n",
      "\n",
      "********** Iteration 21 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 17320\n",
      "Average sum of rewards per episode:       -171.20713891957467\n",
      "Std of rewards per episode:               43.275250473232845\n",
      "Time elapsed:                             185.19 mins\n",
      "KL between old and new distribution:      0.009971664\n",
      "Entropy:                                  2.1289377\n",
      "Surrogate loss:                           23.184086\n",
      "Total paths:                              870\n",
      "Mean path length:                         574.7126436781609\n",
      "\n",
      "********** Iteration 22 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 18322\n",
      "Average sum of rewards per episode:       -162.6380458784318\n",
      "Std of rewards per episode:               42.43494632502893\n",
      "Time elapsed:                             193.84 mins\n",
      "KL between old and new distribution:      0.0099771125\n",
      "Entropy:                                  2.156605\n",
      "Surrogate loss:                           24.469816\n",
      "Total paths:                              1002\n",
      "Mean path length:                         499.001996007984\n",
      "\n",
      "********** Iteration 23 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 19303\n",
      "Average sum of rewards per episode:       -164.75125398202152\n",
      "Std of rewards per episode:               43.09931661972659\n",
      "Time elapsed:                             202.64 mins\n",
      "KL between old and new distribution:      0.009979207\n",
      "Entropy:                                  2.1852217\n",
      "Surrogate loss:                           24.3984\n",
      "Total paths:                              981\n",
      "Mean path length:                         509.683995922528\n",
      "\n",
      "********** Iteration 24 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 20347\n",
      "Average sum of rewards per episode:       -161.87361552516384\n",
      "Std of rewards per episode:               41.26076936198772\n",
      "Time elapsed:                             210.56 mins\n",
      "KL between old and new distribution:      0.009972078\n",
      "Entropy:                                  2.2118058\n",
      "Surrogate loss:                           25.55912\n",
      "Total paths:                              1044\n",
      "Mean path length:                         478.9272030651341\n",
      "\n",
      "********** Iteration 25 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 21427\n",
      "Average sum of rewards per episode:       -160.2316652740395\n",
      "Std of rewards per episode:               41.760535088061225\n",
      "Time elapsed:                             218.57 mins\n",
      "KL between old and new distribution:      0.009978608\n",
      "Entropy:                                  2.2368696\n",
      "Surrogate loss:                           25.85138\n",
      "Total paths:                              1080\n",
      "Mean path length:                         462.962962962963\n",
      "\n",
      "********** Iteration 26 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 22447\n",
      "Average sum of rewards per episode:       -164.04087289899553\n",
      "Std of rewards per episode:               40.31842851875141\n",
      "Time elapsed:                             227.41 mins\n",
      "KL between old and new distribution:      0.009980188\n",
      "Entropy:                                  2.2653804\n",
      "Surrogate loss:                           25.750269\n",
      "Total paths:                              1020\n",
      "Mean path length:                         490.19607843137254\n",
      "\n",
      "********** Iteration 27 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 23537\n",
      "Average sum of rewards per episode:       -160.37540130209416\n",
      "Std of rewards per episode:               40.9438160483483\n",
      "Time elapsed:                             278.95 mins\n",
      "KL between old and new distribution:      0.009964475\n",
      "Entropy:                                  2.2921882\n",
      "Surrogate loss:                           26.333612\n",
      "Total paths:                              1090\n",
      "Mean path length:                         458.7155963302752\n",
      "\n",
      "********** Iteration 28 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 24702\n",
      "Average sum of rewards per episode:       -157.42634362985817\n",
      "Std of rewards per episode:               39.12623246678791\n",
      "Time elapsed:                             286.91 mins\n",
      "KL between old and new distribution:      0.0099776415\n",
      "Entropy:                                  2.315867\n",
      "Surrogate loss:                           27.64296\n",
      "Total paths:                              1165\n",
      "Mean path length:                         429.18454935622316\n",
      "\n",
      "********** Iteration 29 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 25784\n",
      "Average sum of rewards per episode:       -161.55828154682462\n",
      "Std of rewards per episode:               40.53218846207451\n",
      "Time elapsed:                             295.21 mins\n",
      "KL between old and new distribution:      0.009978128\n",
      "Entropy:                                  2.3430326\n",
      "Surrogate loss:                           26.775927\n",
      "Total paths:                              1082\n",
      "Mean path length:                         462.1072088724584\n",
      "\n",
      "********** Iteration 30 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 26917\n",
      "Average sum of rewards per episode:       -160.02107339454753\n",
      "Std of rewards per episode:               39.21268592915558\n",
      "Time elapsed:                             303.98 mins\n",
      "KL between old and new distribution:      0.009971067\n",
      "Entropy:                                  2.369194\n",
      "Surrogate loss:                           27.630713\n",
      "Total paths:                              1133\n",
      "Mean path length:                         441.306266548985\n",
      "\n",
      "********** Iteration 31 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 28149\n",
      "Average sum of rewards per episode:       -155.82516138762594\n",
      "Std of rewards per episode:               38.00166545785993\n",
      "Time elapsed:                             312.80 mins\n",
      "KL between old and new distribution:      0.009997621\n",
      "Entropy:                                  2.392472\n",
      "Surrogate loss:                           28.641998\n",
      "Total paths:                              1232\n",
      "Mean path length:                         405.84415584415586\n",
      "\n",
      "********** Iteration 32 ************\n",
      "Rollout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made rollout\n",
      "Total number of episodes:                 29349\n",
      "Average sum of rewards per episode:       -157.15363162309032\n",
      "Std of rewards per episode:               38.54572979009522\n",
      "Time elapsed:                             321.63 mins\n",
      "KL between old and new distribution:      0.009973323\n",
      "Entropy:                                  2.4145079\n",
      "Surrogate loss:                           28.492682\n",
      "Total paths:                              1200\n",
      "Mean path length:                         416.6666666666667\n",
      "\n",
      "********** Iteration 33 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 30528\n",
      "Average sum of rewards per episode:       -158.7514711205224\n",
      "Std of rewards per episode:               38.81307863254617\n",
      "Time elapsed:                             329.83 mins\n",
      "KL between old and new distribution:      0.009981131\n",
      "Entropy:                                  2.4382715\n",
      "Surrogate loss:                           28.540419\n",
      "Total paths:                              1179\n",
      "Mean path length:                         424.08821034775235\n",
      "\n",
      "********** Iteration 34 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 31739\n",
      "Average sum of rewards per episode:       -157.17025050923027\n",
      "Std of rewards per episode:               39.68882250846378\n",
      "Time elapsed:                             337.74 mins\n",
      "KL between old and new distribution:      0.009968928\n",
      "Entropy:                                  2.4601386\n",
      "Surrogate loss:                           28.603182\n",
      "Total paths:                              1211\n",
      "Mean path length:                         412.8819157720892\n",
      "\n",
      "********** Iteration 35 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 33000\n",
      "Average sum of rewards per episode:       -155.32671425957423\n",
      "Std of rewards per episode:               39.26934812636514\n",
      "Time elapsed:                             346.31 mins\n",
      "KL between old and new distribution:      0.009971404\n",
      "Entropy:                                  2.4827945\n",
      "Surrogate loss:                           29.217554\n",
      "Total paths:                              1261\n",
      "Mean path length:                         396.5107057890563\n",
      "\n",
      "********** Iteration 36 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 34169\n",
      "Average sum of rewards per episode:       -159.23090525713246\n",
      "Std of rewards per episode:               38.50663360904175\n",
      "Time elapsed:                             355.09 mins\n",
      "KL between old and new distribution:      0.009976382\n",
      "Entropy:                                  2.5043\n",
      "Surrogate loss:                           28.524368\n",
      "Total paths:                              1169\n",
      "Mean path length:                         427.71599657827204\n",
      "\n",
      "********** Iteration 37 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 35344\n",
      "Average sum of rewards per episode:       -159.63194076698463\n",
      "Std of rewards per episode:               38.823698763753306\n",
      "Time elapsed:                             363.88 mins\n",
      "KL between old and new distribution:      0.009994577\n",
      "Entropy:                                  2.5261896\n",
      "Surrogate loss:                           28.682829\n",
      "Total paths:                              1175\n",
      "Mean path length:                         425.531914893617\n",
      "\n",
      "********** Iteration 38 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 36581\n",
      "Average sum of rewards per episode:       -157.06527358262684\n",
      "Std of rewards per episode:               40.29896099497846\n",
      "Time elapsed:                             372.73 mins\n",
      "KL between old and new distribution:      0.009990974\n",
      "Entropy:                                  2.546568\n",
      "Surrogate loss:                           29.193169\n",
      "Total paths:                              1237\n",
      "Mean path length:                         404.2037186742118\n",
      "\n",
      "********** Iteration 39 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 37865\n",
      "Average sum of rewards per episode:       -155.41495299910682\n",
      "Std of rewards per episode:               39.6301586345407\n",
      "Time elapsed:                             381.50 mins\n",
      "KL between old and new distribution:      0.009964615\n",
      "Entropy:                                  2.5654416\n",
      "Surrogate loss:                           29.772549\n",
      "Total paths:                              1284\n",
      "Mean path length:                         389.4080996884735\n",
      "\n",
      "********** Iteration 40 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 39127\n",
      "Average sum of rewards per episode:       -156.78412715650953\n",
      "Std of rewards per episode:               37.82030952643094\n",
      "Time elapsed:                             390.29 mins\n",
      "KL between old and new distribution:      0.009972856\n",
      "Entropy:                                  2.5836515\n",
      "Surrogate loss:                           30.168907\n",
      "Total paths:                              1262\n",
      "Mean path length:                         396.19651347068145\n",
      "\n",
      "********** Iteration 41 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 40392\n",
      "Average sum of rewards per episode:       -157.3749577647618\n",
      "Std of rewards per episode:               38.317400433891464\n",
      "Time elapsed:                             399.16 mins\n",
      "KL between old and new distribution:      0.009982031\n",
      "Entropy:                                  2.6030757\n",
      "Surrogate loss:                           30.069094\n",
      "Total paths:                              1265\n",
      "Mean path length:                         395.25691699604744\n",
      "\n",
      "********** Iteration 42 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 41663\n",
      "Average sum of rewards per episode:       -157.16443462943664\n",
      "Std of rewards per episode:               38.07110489834166\n",
      "Time elapsed:                             407.91 mins\n",
      "KL between old and new distribution:      0.009984576\n",
      "Entropy:                                  2.6208372\n",
      "Surrogate loss:                           30.242504\n",
      "Total paths:                              1271\n",
      "Mean path length:                         393.3910306845004\n",
      "\n",
      "********** Iteration 43 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 42980\n",
      "Average sum of rewards per episode:       -155.68363012922526\n",
      "Std of rewards per episode:               39.1810512649997\n",
      "Time elapsed:                             416.68 mins\n",
      "KL between old and new distribution:      0.009969506\n",
      "Entropy:                                  2.638126\n",
      "Surrogate loss:                           30.598406\n",
      "Total paths:                              1317\n",
      "Mean path length:                         379.65072133637057\n",
      "\n",
      "********** Iteration 44 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 44329\n",
      "Average sum of rewards per episode:       -153.7545678812094\n",
      "Std of rewards per episode:               38.41812302075622\n",
      "Time elapsed:                             425.46 mins\n",
      "KL between old and new distribution:      0.009964035\n",
      "Entropy:                                  2.6546893\n",
      "Surrogate loss:                           30.695229\n",
      "Total paths:                              1349\n",
      "Mean path length:                         370.64492216456637\n",
      "\n",
      "********** Iteration 45 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 45658\n",
      "Average sum of rewards per episode:       -155.91582914819898\n",
      "Std of rewards per episode:               39.38261384045415\n",
      "Time elapsed:                             434.21 mins\n",
      "KL between old and new distribution:      0.009992241\n",
      "Entropy:                                  2.669957\n",
      "Surrogate loss:                           30.79052\n",
      "Total paths:                              1329\n",
      "Mean path length:                         376.2227238525207\n",
      "\n",
      "********** Iteration 46 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 46856\n",
      "Average sum of rewards per episode:       -160.42612606701314\n",
      "Std of rewards per episode:               40.82511490154383\n",
      "Time elapsed:                             442.97 mins\n",
      "KL between old and new distribution:      0.009989015\n",
      "Entropy:                                  2.6840506\n",
      "Surrogate loss:                           29.57289\n",
      "Total paths:                              1198\n",
      "Mean path length:                         417.3622704507512\n",
      "\n",
      "********** Iteration 47 ************\n",
      "Rollout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made rollout\n",
      "Total number of episodes:                 48131\n",
      "Average sum of rewards per episode:       -156.72719794807946\n",
      "Std of rewards per episode:               40.97084333255264\n",
      "Time elapsed:                             451.70 mins\n",
      "KL between old and new distribution:      0.009979413\n",
      "Entropy:                                  2.6981375\n",
      "Surrogate loss:                           30.02851\n",
      "Total paths:                              1275\n",
      "Mean path length:                         392.15686274509807\n",
      "\n",
      "********** Iteration 48 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 49503\n",
      "Average sum of rewards per episode:       -153.98054465901936\n",
      "Std of rewards per episode:               39.61796354206393\n",
      "Time elapsed:                             460.46 mins\n",
      "KL between old and new distribution:      0.009995975\n",
      "Entropy:                                  2.710285\n",
      "Surrogate loss:                           31.266808\n",
      "Total paths:                              1372\n",
      "Mean path length:                         364.4314868804665\n",
      "\n",
      "********** Iteration 49 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 50817\n",
      "Average sum of rewards per episode:       -155.18626493758876\n",
      "Std of rewards per episode:               41.123990900599644\n",
      "Time elapsed:                             469.24 mins\n",
      "KL between old and new distribution:      0.0099676335\n",
      "Entropy:                                  2.7236695\n",
      "Surrogate loss:                           30.342007\n",
      "Total paths:                              1314\n",
      "Mean path length:                         380.517503805175\n",
      "\n",
      "********** Iteration 50 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 52032\n",
      "Average sum of rewards per episode:       -160.44005800478158\n",
      "Std of rewards per episode:               41.82741098980052\n",
      "Time elapsed:                             477.98 mins\n",
      "KL between old and new distribution:      0.009982151\n",
      "Entropy:                                  2.736829\n",
      "Surrogate loss:                           29.646374\n",
      "Total paths:                              1215\n",
      "Mean path length:                         411.52263374485597\n",
      "\n",
      "********** Iteration 51 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 53298\n",
      "Average sum of rewards per episode:       -157.7390925890052\n",
      "Std of rewards per episode:               41.45136890715015\n",
      "Time elapsed:                             486.72 mins\n",
      "KL between old and new distribution:      0.009986945\n",
      "Entropy:                                  2.7495916\n",
      "Surrogate loss:                           29.922098\n",
      "Total paths:                              1266\n",
      "Mean path length:                         394.9447077409163\n",
      "\n",
      "********** Iteration 52 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 54632\n",
      "Average sum of rewards per episode:       -155.97659756813604\n",
      "Std of rewards per episode:               40.693317227007114\n",
      "Time elapsed:                             495.49 mins\n",
      "KL between old and new distribution:      0.009975541\n",
      "Entropy:                                  2.760365\n",
      "Surrogate loss:                           30.878674\n",
      "Total paths:                              1334\n",
      "Mean path length:                         374.8125937031484\n",
      "\n",
      "********** Iteration 53 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 55967\n",
      "Average sum of rewards per episode:       -155.61660960353854\n",
      "Std of rewards per episode:               41.87569306921499\n",
      "Time elapsed:                             504.25 mins\n",
      "KL between old and new distribution:      0.009969984\n",
      "Entropy:                                  2.7716875\n",
      "Surrogate loss:                           30.860954\n",
      "Total paths:                              1335\n",
      "Mean path length:                         374.53183520599254\n",
      "\n",
      "********** Iteration 54 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 57300\n",
      "Average sum of rewards per episode:       -155.27840383196877\n",
      "Std of rewards per episode:               41.759424713168166\n",
      "Time elapsed:                             513.01 mins\n",
      "KL between old and new distribution:      0.009964243\n",
      "Entropy:                                  2.7826598\n",
      "Surrogate loss:                           30.76032\n",
      "Total paths:                              1333\n",
      "Mean path length:                         375.0937734433608\n",
      "\n",
      "********** Iteration 55 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 58529\n",
      "Average sum of rewards per episode:       -159.59882828220842\n",
      "Std of rewards per episode:               42.97437177872562\n",
      "Time elapsed:                             521.76 mins\n",
      "KL between old and new distribution:      0.009972959\n",
      "Entropy:                                  2.7932258\n",
      "Surrogate loss:                           29.778753\n",
      "Total paths:                              1229\n",
      "Mean path length:                         406.83482506102524\n",
      "\n",
      "********** Iteration 56 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 59801\n",
      "Average sum of rewards per episode:       -157.8026977273981\n",
      "Std of rewards per episode:               41.80550411613982\n",
      "Time elapsed:                             530.56 mins\n",
      "KL between old and new distribution:      0.009972939\n",
      "Entropy:                                  2.8046558\n",
      "Surrogate loss:                           29.981615\n",
      "Total paths:                              1272\n",
      "Mean path length:                         393.0817610062893\n",
      "\n",
      "********** Iteration 57 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 61121\n",
      "Average sum of rewards per episode:       -155.5461559556896\n",
      "Std of rewards per episode:               43.10851366070586\n",
      "Time elapsed:                             539.30 mins\n",
      "KL between old and new distribution:      0.009998444\n",
      "Entropy:                                  2.8140063\n",
      "Surrogate loss:                           30.31662\n",
      "Total paths:                              1320\n",
      "Mean path length:                         378.7878787878788\n",
      "\n",
      "********** Iteration 58 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 62452\n",
      "Average sum of rewards per episode:       -154.61729446086832\n",
      "Std of rewards per episode:               42.64124788989865\n",
      "Time elapsed:                             548.11 mins\n",
      "KL between old and new distribution:      0.009986169\n",
      "Entropy:                                  2.823552\n",
      "Surrogate loss:                           30.304302\n",
      "Total paths:                              1331\n",
      "Mean path length:                         375.6574004507889\n",
      "\n",
      "********** Iteration 59 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 63725\n",
      "Average sum of rewards per episode:       -157.41128017561658\n",
      "Std of rewards per episode:               43.76551886898829\n",
      "Time elapsed:                             556.90 mins\n",
      "KL between old and new distribution:      0.00999722\n",
      "Entropy:                                  2.833214\n",
      "Surrogate loss:                           29.939886\n",
      "Total paths:                              1273\n",
      "Mean path length:                         392.77297721916733\n",
      "\n",
      "********** Iteration 60 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 64952\n",
      "Average sum of rewards per episode:       -159.4675469657578\n",
      "Std of rewards per episode:               42.9316826440059\n",
      "Time elapsed:                             565.64 mins\n",
      "KL between old and new distribution:      0.009983125\n",
      "Entropy:                                  2.8427074\n",
      "Surrogate loss:                           29.465113\n",
      "Total paths:                              1227\n",
      "Mean path length:                         407.49796251018745\n",
      "\n",
      "********** Iteration 61 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 66231\n",
      "Average sum of rewards per episode:       -156.5342031170583\n",
      "Std of rewards per episode:               42.329080406303575\n",
      "Time elapsed:                             574.36 mins\n",
      "KL between old and new distribution:      0.009985002\n",
      "Entropy:                                  2.8509796\n",
      "Surrogate loss:                           29.856186\n",
      "Total paths:                              1279\n",
      "Mean path length:                         390.93041438623925\n",
      "\n",
      "********** Iteration 62 ************\n",
      "Rollout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made rollout\n",
      "Total number of episodes:                 67543\n",
      "Average sum of rewards per episode:       -155.34672488475164\n",
      "Std of rewards per episode:               44.109996531545626\n",
      "Time elapsed:                             583.10 mins\n",
      "KL between old and new distribution:      0.009999125\n",
      "Entropy:                                  2.8570082\n",
      "Surrogate loss:                           30.03298\n",
      "Total paths:                              1312\n",
      "Mean path length:                         381.0975609756098\n",
      "\n",
      "********** Iteration 63 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 68885\n",
      "Average sum of rewards per episode:       -154.382861842666\n",
      "Std of rewards per episode:               44.22567314731277\n",
      "Time elapsed:                             591.82 mins\n",
      "KL between old and new distribution:      0.00998532\n",
      "Entropy:                                  2.8642206\n",
      "Surrogate loss:                           30.371021\n",
      "Total paths:                              1342\n",
      "Mean path length:                         372.5782414307005\n",
      "\n",
      "********** Iteration 64 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 70057\n",
      "Average sum of rewards per episode:       -160.9566948699201\n",
      "Std of rewards per episode:               44.705299827656326\n",
      "Time elapsed:                             600.66 mins\n",
      "KL between old and new distribution:      0.009987738\n",
      "Entropy:                                  2.8735971\n",
      "Surrogate loss:                           28.561325\n",
      "Total paths:                              1172\n",
      "Mean path length:                         426.6211604095563\n",
      "\n",
      "********** Iteration 65 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 71283\n",
      "Average sum of rewards per episode:       -158.77482688670733\n",
      "Std of rewards per episode:               43.937777854619156\n",
      "Time elapsed:                             609.57 mins\n",
      "KL between old and new distribution:      0.009979101\n",
      "Entropy:                                  2.879921\n",
      "Surrogate loss:                           29.32667\n",
      "Total paths:                              1226\n",
      "Mean path length:                         407.8303425774878\n",
      "\n",
      "********** Iteration 66 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 72594\n",
      "Average sum of rewards per episode:       -153.47287034643642\n",
      "Std of rewards per episode:               43.188472837314535\n",
      "Time elapsed:                             618.32 mins\n",
      "KL between old and new distribution:      0.009969616\n",
      "Entropy:                                  2.88416\n",
      "Surrogate loss:                           29.333591\n",
      "Total paths:                              1311\n",
      "Mean path length:                         381.38825324180016\n",
      "\n",
      "********** Iteration 67 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 73937\n",
      "Average sum of rewards per episode:       -154.20555785895888\n",
      "Std of rewards per episode:               43.189894437631445\n",
      "Time elapsed:                             627.11 mins\n",
      "KL between old and new distribution:      0.009985042\n",
      "Entropy:                                  2.8909438\n",
      "Surrogate loss:                           30.391972\n",
      "Total paths:                              1343\n",
      "Mean path length:                         372.3008190618019\n",
      "\n",
      "********** Iteration 68 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 75220\n",
      "Average sum of rewards per episode:       -156.5414371827697\n",
      "Std of rewards per episode:               43.29041187639353\n",
      "Time elapsed:                             635.94 mins\n",
      "KL between old and new distribution:      0.0099871\n",
      "Entropy:                                  2.898227\n",
      "Surrogate loss:                           29.748863\n",
      "Total paths:                              1283\n",
      "Mean path length:                         389.7116134060795\n",
      "\n",
      "********** Iteration 69 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 76394\n",
      "Average sum of rewards per episode:       -160.756495519437\n",
      "Std of rewards per episode:               44.591443458398935\n",
      "Time elapsed:                             644.83 mins\n",
      "KL between old and new distribution:      0.009967843\n",
      "Entropy:                                  2.9052556\n",
      "Surrogate loss:                           28.786299\n",
      "Total paths:                              1174\n",
      "Mean path length:                         425.89437819420783\n",
      "\n",
      "********** Iteration 70 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 77673\n",
      "Average sum of rewards per episode:       -156.21791479731587\n",
      "Std of rewards per episode:               43.6035950585704\n",
      "Time elapsed:                             653.79 mins\n",
      "KL between old and new distribution:      0.00999535\n",
      "Entropy:                                  2.9066954\n",
      "Surrogate loss:                           29.684551\n",
      "Total paths:                              1279\n",
      "Mean path length:                         390.93041438623925\n",
      "\n",
      "********** Iteration 71 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 78971\n",
      "Average sum of rewards per episode:       -155.15211723804418\n",
      "Std of rewards per episode:               43.855023644884085\n",
      "Time elapsed:                             662.65 mins\n",
      "KL between old and new distribution:      0.009999244\n",
      "Entropy:                                  2.9132566\n",
      "Surrogate loss:                           30.003214\n",
      "Total paths:                              1298\n",
      "Mean path length:                         385.2080123266564\n",
      "\n",
      "********** Iteration 72 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 80158\n",
      "Average sum of rewards per episode:       -158.27081043934885\n",
      "Std of rewards per episode:               46.97598483239026\n",
      "Time elapsed:                             671.47 mins\n",
      "KL between old and new distribution:      0.009996466\n",
      "Entropy:                                  2.9183896\n",
      "Surrogate loss:                           28.344627\n",
      "Total paths:                              1187\n",
      "Mean path length:                         421.22999157540016\n",
      "\n",
      "********** Iteration 73 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 81246\n",
      "Average sum of rewards per episode:       -164.68518073807192\n",
      "Std of rewards per episode:               45.48954680796711\n",
      "Time elapsed:                             680.33 mins\n",
      "KL between old and new distribution:      0.009978701\n",
      "Entropy:                                  2.9229286\n",
      "Surrogate loss:                           27.653881\n",
      "Total paths:                              1088\n",
      "Mean path length:                         459.55882352941177\n",
      "\n",
      "********** Iteration 74 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 82515\n",
      "Average sum of rewards per episode:       -155.14229130407253\n",
      "Std of rewards per episode:               42.24467835594745\n",
      "Time elapsed:                             689.06 mins\n",
      "KL between old and new distribution:      0.009970089\n",
      "Entropy:                                  2.9280608\n",
      "Surrogate loss:                           29.530476\n",
      "Total paths:                              1269\n",
      "Mean path length:                         394.01103230890465\n",
      "\n",
      "********** Iteration 75 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 83839\n",
      "Average sum of rewards per episode:       -154.11686205461976\n",
      "Std of rewards per episode:               44.57824647606361\n",
      "Time elapsed:                             697.79 mins\n",
      "KL between old and new distribution:      0.009981979\n",
      "Entropy:                                  2.9306214\n",
      "Surrogate loss:                           30.13457\n",
      "Total paths:                              1324\n",
      "Mean path length:                         377.64350453172204\n",
      "\n",
      "********** Iteration 76 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 85056\n",
      "Average sum of rewards per episode:       -157.78065338641622\n",
      "Std of rewards per episode:               43.60528612900816\n",
      "Time elapsed:                             706.60 mins\n",
      "KL between old and new distribution:      0.009974852\n",
      "Entropy:                                  2.9328806\n",
      "Surrogate loss:                           29.12376\n",
      "Total paths:                              1217\n",
      "Mean path length:                         410.84634346754314\n",
      "\n",
      "********** Iteration 77 ************\n",
      "Rollout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made rollout\n",
      "Total number of episodes:                 86228\n",
      "Average sum of rewards per episode:       -160.76901376069313\n",
      "Std of rewards per episode:               43.96404143952083\n",
      "Time elapsed:                             715.35 mins\n",
      "KL between old and new distribution:      0.009971526\n",
      "Entropy:                                  2.9381094\n",
      "Surrogate loss:                           28.73919\n",
      "Total paths:                              1172\n",
      "Mean path length:                         426.6211604095563\n",
      "\n",
      "********** Iteration 78 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 87392\n",
      "Average sum of rewards per episode:       -159.3527354937026\n",
      "Std of rewards per episode:               43.38237511416692\n",
      "Time elapsed:                             724.10 mins\n",
      "KL between old and new distribution:      0.0099785505\n",
      "Entropy:                                  2.94071\n",
      "Surrogate loss:                           28.220766\n",
      "Total paths:                              1164\n",
      "Mean path length:                         429.553264604811\n",
      "\n",
      "********** Iteration 79 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 88617\n",
      "Average sum of rewards per episode:       -156.3845941788746\n",
      "Std of rewards per episode:               44.30332556928484\n",
      "Time elapsed:                             732.83 mins\n",
      "KL between old and new distribution:      0.009967077\n",
      "Entropy:                                  2.9411366\n",
      "Surrogate loss:                           28.875137\n",
      "Total paths:                              1225\n",
      "Mean path length:                         408.16326530612247\n",
      "\n",
      "********** Iteration 80 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 89748\n",
      "Average sum of rewards per episode:       -162.37382194338474\n",
      "Std of rewards per episode:               44.19672213252001\n",
      "Time elapsed:                             741.59 mins\n",
      "KL between old and new distribution:      0.009964613\n",
      "Entropy:                                  2.9444475\n",
      "Surrogate loss:                           28.499086\n",
      "Total paths:                              1131\n",
      "Mean path length:                         442.0866489832007\n",
      "\n",
      "********** Iteration 81 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 90916\n",
      "Average sum of rewards per episode:       -160.30470145298753\n",
      "Std of rewards per episode:               42.72273355794191\n",
      "Time elapsed:                             750.30 mins\n",
      "KL between old and new distribution:      0.009975168\n",
      "Entropy:                                  2.9468877\n",
      "Surrogate loss:                           28.791996\n",
      "Total paths:                              1168\n",
      "Mean path length:                         428.0821917808219\n",
      "\n",
      "********** Iteration 82 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 92061\n",
      "Average sum of rewards per episode:       -160.0837922753482\n",
      "Std of rewards per episode:               45.09210874868202\n",
      "Time elapsed:                             759.11 mins\n",
      "KL between old and new distribution:      0.009961865\n",
      "Entropy:                                  2.948151\n",
      "Surrogate loss:                           28.108253\n",
      "Total paths:                              1145\n",
      "Mean path length:                         436.68122270742356\n",
      "\n",
      "********** Iteration 83 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 93181\n",
      "Average sum of rewards per episode:       -161.12289198098318\n",
      "Std of rewards per episode:               44.92306473736688\n",
      "Time elapsed:                             767.87 mins\n",
      "KL between old and new distribution:      0.009993303\n",
      "Entropy:                                  2.950062\n",
      "Surrogate loss:                           27.634209\n",
      "Total paths:                              1120\n",
      "Mean path length:                         446.42857142857144\n",
      "\n",
      "********** Iteration 84 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 94335\n",
      "Average sum of rewards per episode:       -161.7985408664373\n",
      "Std of rewards per episode:               42.582254251388086\n",
      "Time elapsed:                             776.60 mins\n",
      "KL between old and new distribution:      0.009986568\n",
      "Entropy:                                  2.9522884\n",
      "Surrogate loss:                           28.654608\n",
      "Total paths:                              1154\n",
      "Mean path length:                         433.27556325823224\n",
      "\n",
      "********** Iteration 85 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 95535\n",
      "Average sum of rewards per episode:       -159.10153093988814\n",
      "Std of rewards per episode:               41.6306496965854\n",
      "Time elapsed:                             785.29 mins\n",
      "KL between old and new distribution:      0.009977854\n",
      "Entropy:                                  2.9515147\n",
      "Surrogate loss:                           29.281664\n",
      "Total paths:                              1200\n",
      "Mean path length:                         416.6666666666667\n",
      "\n",
      "********** Iteration 86 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 96736\n",
      "Average sum of rewards per episode:       -156.57525861628574\n",
      "Std of rewards per episode:               42.815688584570275\n",
      "Time elapsed:                             793.99 mins\n",
      "KL between old and new distribution:      0.009972232\n",
      "Entropy:                                  2.9525497\n",
      "Surrogate loss:                           28.42308\n",
      "Total paths:                              1201\n",
      "Mean path length:                         416.31973355537053\n",
      "\n",
      "********** Iteration 87 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 97753\n",
      "Average sum of rewards per episode:       -169.10462176127677\n",
      "Std of rewards per episode:               41.63791231730237\n",
      "Time elapsed:                             802.72 mins\n",
      "KL between old and new distribution:      0.00998748\n",
      "Entropy:                                  2.9537175\n",
      "Surrogate loss:                           27.391832\n",
      "Total paths:                              1017\n",
      "Mean path length:                         491.6420845624385\n",
      "\n",
      "********** Iteration 88 ************\n",
      "Rollout\n",
      "Made rollout\n",
      "Total number of episodes:                 98895\n",
      "Average sum of rewards per episode:       -160.85424009288568\n",
      "Std of rewards per episode:               43.258264764448526\n",
      "Time elapsed:                             811.41 mins\n",
      "KL between old and new distribution:      0.009980997\n",
      "Entropy:                                  2.9524205\n",
      "Surrogate loss:                           28.40181\n",
      "Total paths:                              1142\n",
      "Mean path length:                         437.82837127845886\n",
      "\n",
      "********** Iteration 89 ************\n",
      "Rollout\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-4cee878df0bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Generating paths.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rollout\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_pathlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrollout_max_pathlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrollout_n_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Made rollout\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-fcdbf081fbb5>\u001b[0m in \u001b[0;36mrollout\u001b[0;34m(env, agent, max_pathlength, n_timesteps)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mprobs_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mobervation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.6/site-packages/gym/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhull\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mvel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhull\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinearVelocity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.6/site-packages/Box2D/Box2D.py\u001b[0m in \u001b[0;36m__GetLinearVelocity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3962\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__GetLinearVelocity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3963\u001b[0m         \"\"\"\n\u001b[1;32m   3964\u001b[0m         \u001b[0m__GetLinearVelocity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb2Body\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mb2Vec2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for i in count(1):\n",
    "    print(\"\\n********** Iteration %i ************\" % i)\n",
    "\n",
    "    # Generating paths.\n",
    "    print(\"Rollout\")\n",
    "    paths = rollout(env, agent, max_pathlength=rollout_max_pathlength, n_timesteps=rollout_n_timesteps)\n",
    "    print(\"Made rollout\")\n",
    "\n",
    "    # Updating policy.\n",
    "    observations = np.concatenate([path[\"observations\"] for path in paths])\n",
    "    actions = np.concatenate([path[\"actions\"] for path in paths])\n",
    "    returns = np.concatenate([path[\"cumulative_returns\"] for path in paths])\n",
    "    old_prob_dist = cat_normal([path[\"policy\"] for path in paths])\n",
    "\n",
    "    loss, kl = update_step(agent, observations, actions,\n",
    "                           returns, old_prob_dist, max_kl)\n",
    "\n",
    "    # Report current progress\n",
    "    episode_rewards = np.array([path[\"rewards\"].sum() for path in paths])\n",
    "\n",
    "    stats = OrderedDict()\n",
    "    numeptotal += len(episode_rewards)\n",
    "    stats[\"Total number of episodes\"] = numeptotal\n",
    "    stats[\"Average sum of rewards per episode\"] = episode_rewards.mean()\n",
    "    stats[\"Std of rewards per episode\"] = episode_rewards.std()\n",
    "    stats[\"Time elapsed\"] = \"%.2f mins\" % ((time.time() - start_time)/60.)\n",
    "    stats[\"KL between old and new distribution\"] = kl.data.numpy()\n",
    "    stats[\"Entropy\"] = get_entropy(agent, observations).data.numpy()\n",
    "    stats[\"Surrogate loss\"] = loss.data.numpy()\n",
    "    stats[\"Total paths\"] = len(paths)\n",
    "    stats[\"Mean path length\"] = float(np.mean([paths[i]['observations'].shape[0] for i in range(len(paths))]))\n",
    "    for k, v in stats.items():\n",
    "        print(k + \": \" + \" \" * (40 - len(k)) + str(v))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
