{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Упражнение, для реализации \"Ванильной\" RNN\n",
    "* Попробуем обучить сеть восстанавливать слово hello по первой букве. т.е. построим charecter-level модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((3,3))*3\n",
    "b = torch.ones((3,3))*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45., 45., 45.],\n",
       "        [45., 45., 45.],\n",
       "        [45., 45., 45.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 15., 15.],\n",
       "        [15., 15., 15.],\n",
       "        [15., 15., 15.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word = 'ololoasdasddqweqw123456789'\n",
    "word = 'hello'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет. \n",
    "Позволяет:\n",
    "* Закодировать символ при помощи one-hot\n",
    "* Делать итератор по слову, которыей возвращает текущий символ и следующий как таргет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataSet:\n",
    "    \n",
    "    def __init__(self, word):\n",
    "        self.chars2idx = {}\n",
    "        self.indexs  = []\n",
    "        for c in word: \n",
    "            if c not in self.chars2idx:\n",
    "                self.chars2idx[c] = len(self.chars2idx)\n",
    "                \n",
    "            self.indexs.append(self.chars2idx[c])\n",
    "            \n",
    "        self.vec_size = len(self.chars2idx)\n",
    "        self.seq_len  = len(word)\n",
    "        \n",
    "    def get_one_hot(self, idx):\n",
    "        x = torch.zeros(self.vec_size)\n",
    "        x[idx] = 1\n",
    "        return x\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return zip(self.indexs[:-1], self.indexs[1:])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq_len\n",
    "    \n",
    "    def get_char_by_id(self, id):\n",
    "        for c, i in self.chars2idx.items():\n",
    "            if id == i: return c\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализация базовой RNN\n",
    "<br/>\n",
    "Скрытый элемент\n",
    "$$ h_t= tanh⁡ (W_{ℎℎ} h_{t−1}+W_{xh} x_t) $$\n",
    "Выход сети\n",
    "\n",
    "$$ y_t = W_{hy} h_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(VanillaRNN, self).__init__()        \n",
    "        self.x2hidden    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden      = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.activation  = nn.Tanh()\n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        hidden = self.activation(self.x2hidden(x) + self.hidden(prev_hidden))\n",
    "#         Версия без активации - может происходить gradient exploding\n",
    "#         hidden = self.x2hidden(x) + self.hidden(prev_hidden)\n",
    "        output = self.outweight(hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_output(*kwargs):\n",
    "    return (kwargs[0], kwargs[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация переменных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "rnn = VanillaRNN(in_size=ds.vec_size, hidden_size=7, out_size=ds.vec_size)\n",
    "vanilla_start_params = (torch.zeros(rnn.hidden.in_features), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning(ds, net, start_hidden_params, e_cnt=100):\n",
    "    CLIP_GRAD = True\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optim     = SGD(net.parameters(), lr = 0.1, momentum=0.9)\n",
    "    \n",
    "    for epoch in range(e_cnt):\n",
    "        hidden_params = start_hidden_params\n",
    "        loss = 0\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        for sample, next_sample in ds:\n",
    "            x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "            target =  torch.LongTensor([next_sample])\n",
    "            \n",
    "            y, hidden_params = filter_output(*net(x, *hidden_params))\n",
    "            \n",
    "            loss += criterion(y, target)\n",
    "\n",
    "\n",
    "        loss.backward( retain_graph=True)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print (loss.data.item())\n",
    "            if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=5))\n",
    "        else: \n",
    "            if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1)\n",
    "\n",
    "    #     print(\"Params : \")\n",
    "    #     num_params = 0\n",
    "    #     for item in rnn.parameters():\n",
    "    #         num_params += 1\n",
    "    #         print(item.grad)\n",
    "    #     print(\"NumParams :\", num_params)\n",
    "    #     print(\"Optimize\")\n",
    "\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.372060775756836\n",
      "Clip gradient :  2.825483806701919\n",
      "1.3506267070770264\n",
      "Clip gradient :  1.5056248964000374\n",
      "0.0091094970703125\n",
      "Clip gradient :  0.027584052052596172\n",
      "0.0014925003051757812\n",
      "Clip gradient :  0.004743617317189838\n",
      "0.0007624626159667969\n",
      "Clip gradient :  0.0022821125541011636\n",
      "0.0005841255187988281\n",
      "Clip gradient :  0.0017153544780453974\n",
      "0.0005168914794921875\n",
      "Clip gradient :  0.001496709232124883\n",
      "0.00048160552978515625\n",
      "Clip gradient :  0.0013742137434327164\n",
      "0.000457763671875\n",
      "Clip gradient :  0.001285835996576162\n",
      "0.0004405975341796875\n",
      "Clip gradient :  0.0012167375987139465\n"
     ]
    }
   ],
   "source": [
    "learning(ds, rnn, vanilla_start_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(ds, net, start_hidden_params):\n",
    "    net.eval()\n",
    "    softmax  = nn.Softmax(dim=1)\n",
    "    \n",
    "    id = 0\n",
    "    predword = ds.get_char_by_id(id)\n",
    "    hidden_params = start_hidden_params\n",
    "    for c in range(len(ds) - 1):\n",
    "        x = ds.get_one_hot(id).unsqueeze(0)\n",
    "        \n",
    "        y, hidden_params = filter_output(*net(x, *hidden_params))\n",
    "        y = softmax(y)\n",
    "        \n",
    "        m, id = torch.max(y, 1)\n",
    "        id = id.data[0]\n",
    "        predword += ds.get_char_by_id(id)\n",
    "    \n",
    "    print ('Prediction:\\t' , predword)\n",
    "    print(\"Original:\\t\", word)\n",
    "    assert(predword == word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t hello\n",
      "Original:\t hello\n"
     ]
    }
   ],
   "source": [
    "test(ds, rnn, vanilla_start_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ\n",
    "Реализовать LSTM и GRU модули, обучить их предсказывать тестовое слово\n",
    "Сохранить ноутбук с предсказанием и пройденным assert и прислать на почту a.murashev@corp.mail.ru\n",
    "c темой:\n",
    "\n",
    "\n",
    "[МФТИ\\_2019\\_1] ДЗ №8 ФИО"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#тестовое слово\n",
    "word = 'ololoasdasddqweqw123456789'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализовать LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(LSTM, self).__init__()        \n",
    "        \n",
    "        self.x2c    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.x2i    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.x2f    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.x2o    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        \n",
    "        self.h2c    = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.h2i    = nn.Linear(in_features=hidden_size, out_features=hidden_size)        \n",
    "        self.h2f    = nn.Linear(in_features=hidden_size, out_features=hidden_size)        \n",
    "        self.h2o    = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.activation  = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden, prev_cell):\n",
    "        candidate_cell = self.activation(self.x2c(x) + self.h2c(prev_hidden))\n",
    "        input_gate  = self.sigmoid(self.x2i(x) + self.h2i(prev_hidden))\n",
    "        forget_gate = self.sigmoid(self.x2f(x) + self.h2f(prev_hidden))\n",
    "        output_gate = self.sigmoid(self.x2o(x) + self.h2o(prev_hidden))\n",
    "        \n",
    "        cell = forget_gate * prev_cell + input_gate * candidate_cell\n",
    "        next_hidden = output_gate * self.activation(cell)\n",
    "        output = self.outweight(next_hidden)\n",
    "        return output, next_hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация переменных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'ololoasdasddqweqw123456789'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "lstm = LSTM(in_size=ds.vec_size, hidden_size=7, out_size=ds.vec_size)\n",
    "base_params = torch.zeros(lstm.h2c.in_features)\n",
    "lstm_start_params = (base_params, base_params.clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.75397491455078\n",
      "Clip gradient :  3.3991802167386043\n",
      "65.08412170410156\n",
      "Clip gradient :  3.959230754248246\n",
      "44.83808517456055\n",
      "Clip gradient :  7.456596479112804\n",
      "31.306686401367188\n",
      "Clip gradient :  10.027044319855715\n",
      "64.57586669921875\n",
      "Clip gradient :  50.0484574782862\n",
      "42.71078872680664\n",
      "Clip gradient :  21.355588776551382\n",
      "36.351959228515625\n",
      "Clip gradient :  7.400320848877463\n",
      "27.535188674926758\n",
      "Clip gradient :  6.011776114960499\n",
      "16.64202308654785\n",
      "Clip gradient :  9.047759056270557\n",
      "13.88967514038086\n",
      "Clip gradient :  5.758831329798922\n",
      "8.740039825439453\n",
      "Clip gradient :  3.9905241632740185\n",
      "5.665721893310547\n",
      "Clip gradient :  2.482155003409084\n",
      "4.398443698883057\n",
      "Clip gradient :  1.4455499902300755\n",
      "3.7307419776916504\n",
      "Clip gradient :  1.9734192203858738\n",
      "2.577390670776367\n",
      "Clip gradient :  3.035892327691981\n",
      "1.3400287628173828\n",
      "Clip gradient :  0.8521444207883518\n",
      "0.6303787231445312\n",
      "Clip gradient :  0.35416556645571345\n",
      "0.3552837371826172\n",
      "Clip gradient :  0.16336651252914072\n",
      "0.2448263168334961\n",
      "Clip gradient :  0.09105079072758715\n",
      "0.18945598602294922\n",
      "Clip gradient :  0.05990793982805831\n"
     ]
    }
   ],
   "source": [
    "learning(ds, lstm, lstm_start_params, e_cnt=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "test(ds, lstm, lstm_start_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(GRU, self).__init__()        \n",
    "        self.x2u    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.x2r    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.x2h    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        \n",
    "        self.h2u    = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.h2r    = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.h2h    = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        \n",
    "        self.activation  = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        update_gate = self.sigmoid(self.x2u(x) + self.h2u(prev_hidden))\n",
    "        reset_gate = self.sigmoid(self.x2r(x) + self.h2r(prev_hidden))\n",
    "        hiddent_state = self.activation(self.x2h(x) + self.h2h(reset_gate * prev_hidden))\n",
    "        \n",
    "        next_hidden = (1 - update_gate) * hiddent_state + update_gate * prev_hidden\n",
    "        output = self.outweight(next_hidden)\n",
    "        return output, next_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация переменных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ololoasdasddqweqw123456789'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'ololoasdasddqweqw123456789'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "gru = GRU(in_size=ds.vec_size, hidden_size=7, out_size=ds.vec_size)\n",
    "gru_start_params = (torch.zeros(lstm.h2c.in_features), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.4969711303711\n",
      "Clip gradient :  4.218776590248741\n",
      "58.83040237426758\n",
      "Clip gradient :  7.7946891066138635\n",
      "40.53915023803711\n",
      "Clip gradient :  7.973134748697141\n",
      "24.969440460205078\n",
      "Clip gradient :  6.404975025110897\n",
      "11.901238441467285\n",
      "Clip gradient :  3.4857794909960202\n",
      "4.732065200805664\n",
      "Clip gradient :  2.4398715475321637\n",
      "1.970916748046875\n",
      "Clip gradient :  2.479642894744235\n",
      "0.6413145065307617\n",
      "Clip gradient :  1.1579704408184015\n",
      "0.26062965393066406\n",
      "Clip gradient :  0.34073289102231424\n",
      "0.1478433609008789\n",
      "Clip gradient :  0.16045513693172309\n",
      "0.10283279418945312\n",
      "Clip gradient :  0.15107458643146382\n",
      "0.08113670349121094\n",
      "Clip gradient :  0.06945719459598802\n",
      "0.06879997253417969\n",
      "Clip gradient :  0.03879333147955721\n",
      "0.06056690216064453\n",
      "Clip gradient :  0.02646288436612143\n",
      "0.054451942443847656\n",
      "Clip gradient :  0.022159373576912937\n",
      "0.049620628356933594\n",
      "Clip gradient :  0.02013358908544194\n",
      "0.04564380645751953\n",
      "Clip gradient :  0.018460227564917817\n",
      "0.04229164123535156\n",
      "Clip gradient :  0.017042698814062538\n",
      "0.039414405822753906\n",
      "Clip gradient :  0.015841282650173684\n",
      "0.036910057067871094\n",
      "Clip gradient :  0.014824251750762142\n"
     ]
    }
   ],
   "source": [
    "learning(ds, gru, gru_start_params, e_cnt=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "test(ds, gru, gru_start_params)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
