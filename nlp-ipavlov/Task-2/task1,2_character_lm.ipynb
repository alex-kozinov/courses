{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "task1,2_character_lm.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVBpzGzF6icO",
        "colab_type": "code",
        "outputId": "1fe8605c-06a9-4513-e51f-04366f42dc33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    USE_COLAB = True\n",
        "except:\n",
        "    USE_COLAB = False\n",
        "print(\"Use colab - \", USE_COLAB)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use colab -  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBbWPRe7Bt9q",
        "colab_type": "code",
        "outputId": "171d4d3f-7755-4c96-dc70-71d79b881210",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if USE_COLAB:\n",
        "    !pip install --user deeppavlov"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deeppavlov\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/ff/8e6ad196e1bc732f6967452b2a245345aa72c1b8007d6ddf95c7f60a04e4/deeppavlov-0.8.0-py3-none-any.whl (750kB)\n",
            "\u001b[K     |████████████████████████████████| 757kB 4.7MB/s \n",
            "\u001b[?25hCollecting pyopenssl==19.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/de/f8342b68fa9e981d348039954657bdf681b2ab93de27443be51865ffa310/pyOpenSSL-19.1.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.4MB/s \n",
            "\u001b[?25hCollecting uvicorn==0.11.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/24/11f4b4bf3963ead6de570feeae49eeced02f6768cf1f68e16f4b16d3b0aa/uvicorn-0.11.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.3MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/9b/358faaff410f65a4ad159275e897b5956dcb20576c5b8e764b971c1634d7/pymorphy2_dicts_ru-2.4.404381.4453942-py2.py3-none-any.whl (8.0MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0MB 46.4MB/s \n",
            "\u001b[?25hCollecting rusenttokenize==0.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/25/4c/a2f00be5def774a3df2e5387145f1cb54e324607ec4a7e23f573645946e7/rusenttokenize-0.0.5-py3-none-any.whl\n",
            "Collecting scikit-learn==0.21.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/04/49633f490f726da6e454fddc8e938bbb5bfed2001681118d3814c219b723/scikit_learn-0.21.2-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 52.8MB/s \n",
            "\u001b[?25hCollecting aio-pika==6.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/07/196a4115cbef31fa0c3dabdea146f02dffe5e49998341d20dbe2278953bc/aio_pika-6.4.1-py3-none-any.whl (40kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.2MB/s \n",
            "\u001b[?25hCollecting fastapi==0.47.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/a7/4804d7abf8a1544d079d50650af872387154ebdac5bd07d54b2e60e2b334/fastapi-0.47.1-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.1MB/s \n",
            "\u001b[?25hCollecting tqdm==4.41.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/c9/7fc20feac72e79032a7c8138fd0d395dc6d8812b5b9edf53c3afd0b31017/tqdm-4.41.1-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.2MB/s \n",
            "\u001b[?25hCollecting overrides==2.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/98/2430afd204c48ac0a529d439d7e22df8fa603c668d03456b5947cb59ec36/overrides-2.7.0.tar.gz\n",
            "Requirement already satisfied: pandas==0.25.3 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (0.25.3)\n",
            "Collecting pydantic==1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/24/e78cf017628e7eaed20cb040999b1ecc69f872da53dfd0d9aed40c0fa5f1/pydantic-1.3-cp36-cp36m-manylinux2010_x86_64.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 47.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (1.4.1)\n",
            "Collecting fuzzywuzzy==0.17.0\n",
            "  Downloading https://files.pythonhosted.org/packages/d8/f1/5a267addb30ab7eaa1beab2b9323073815da4551076554ecc890a3595ec9/fuzzywuzzy-0.17.0-py2.py3-none-any.whl\n",
            "Collecting requests==2.22.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.6MB/s \n",
            "\u001b[?25hCollecting numpy==1.18.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e6/45f71bd24f4e37629e9db5fb75caab919507deae6a5a257f9e4685a5f931/numpy-1.18.0-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 112kB/s \n",
            "\u001b[?25hCollecting pytelegrambotapi==3.6.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/ab/99c606f69fcda57e35788b913dd34c9d9acb48dd26349141b3855dcf6351/pyTelegramBotAPI-3.6.7.tar.gz (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.4MB/s \n",
            "\u001b[?25hCollecting pymorphy2==0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (2.10.0)\n",
            "Collecting nltk==3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 42.9MB/s \n",
            "\u001b[?25hCollecting Cython==0.29.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/d1/4d3f8a7a920e805488a966cc6ab55c978a712240f584445d703c08b9f405/Cython-0.29.14-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 45.8MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/9a/7cece52c46546e214e10811b36b2da52ce1ea7fa203203a629b8dfadad53/cryptography-2.8-cp34-abi3-manylinux2010_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 51.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from pyopenssl==19.1.0->deeppavlov) (1.12.0)\n",
            "Requirement already satisfied: click==7.* in /usr/local/lib/python3.6/dist-packages (from uvicorn==0.11.1->deeppavlov) (7.1.1)\n",
            "Collecting uvloop>=0.14.0; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"pypy\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/48/586225bbb02d3bdca475b17e4be5ce5b3f09da2d6979f359916c1592a687/uvloop-0.14.0-cp36-cp36m-manylinux2010_x86_64.whl (3.9MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 46.2MB/s \n",
            "\u001b[?25hCollecting websockets==8.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/d9/856af84843912e2853b1b6e898ac8b802989fcf9ecf8e8445a1da263bf3b/websockets-8.1-cp36-cp36m-manylinux2010_x86_64.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.7MB/s \n",
            "\u001b[?25hCollecting httptools==0.0.13; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"pypy\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/03/215969db11abe8741e9c266a4cbe803a372bd86dd35fa0084c4df6d4bd00/httptools-0.0.13.tar.gz (104kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 60.3MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.2->deeppavlov) (0.14.1)\n",
            "Collecting yarl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/8f/0209fc5d975f839344c33c822ff2f7ef80f6b1e984673a5a68f960bfa583/yarl-1.4.2-cp36-cp36m-manylinux1_x86_64.whl (252kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 62.9MB/s \n",
            "\u001b[?25hCollecting aiormq<4,>=3.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/f8/77a1694064c677afaaae111e28e2b32dd70c5f9dce3836bad0df20f27201/aiormq-3.2.1-py3-none-any.whl\n",
            "Collecting starlette<=0.12.9,>=0.12.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/95/2220fe5bf287e693a6430d8ee36c681b0157035b7249ec08f8fb36319d16/starlette-0.12.9.tar.gz (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas==0.25.3->deeppavlov) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas==0.25.3->deeppavlov) (2018.9)\n",
            "Requirement already satisfied: dataclasses>=0.6; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from pydantic==1.3->deeppavlov) (0.7)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (2019.11.28)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (1.24.3)\n",
            "Collecting dawg-python>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 45.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (1.14.0)\n",
            "Collecting multidict>=4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/2e/3ab2f1fb72571f75013db323a3799d505d99f3bc203513604f1ffb9b7858/multidict-4.7.5-cp36-cp36m-manylinux1_x86_64.whl (148kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 57.4MB/s \n",
            "\u001b[?25hCollecting pamqp==2.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/56/afa06143361e640c9159d828dadc95fc9195c52c95b4a97d136617b0166d/pamqp-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (2.20)\n",
            "Building wheels for collected packages: overrides, pytelegrambotapi, nltk, httptools, starlette\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-2.7.0-cp36-none-any.whl size=5600 sha256=37b9d3c19bed5072b3eb0af9ab8205d1b8b293d050d48d92313f21a22b7f8524\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/7c/ef/80508418b67d87371c5b3de49e03eb22ee7c1d19affb5099f8\n",
            "  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytelegrambotapi: filename=pyTelegramBotAPI-3.6.7-cp36-none-any.whl size=47178 sha256=3c8492fe927418c2d375db310f74e66bd61f3d611ec56438c6a03a8d275ac815\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/40/18/8a34153f95ef0dc19e3954898e5a5079244b76a8afdd7d0ec5\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449903 sha256=7361a942a1c24ef6d5e35c5390eac2f44a43e1af6eb1d1242cc560fa41510f82\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "  Building wheel for httptools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for httptools: filename=httptools-0.0.13-cp36-cp36m-linux_x86_64.whl size=212528 sha256=f8651c1309f1999b618ece3fd71ee5a6c95112bb1ebdae5ff793af5ce523b060\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/3e/2e/013f99b42efc25cf3589730cf380738e46b1e5edaf2f78d525\n",
            "  Building wheel for starlette (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for starlette: filename=starlette-0.12.9-cp36-none-any.whl size=57245 sha256=04abc945f3b6190f61c0d3321971238b87138884ece80f696c43bbf396a70149\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/51/5b/3828d52e185cafad941c4291b6f70894d0794be28c70addae5\n",
            "Successfully built overrides pytelegrambotapi nltk httptools starlette\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.21.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: cryptography, pyopenssl, uvloop, websockets, httptools, h11, uvicorn, pymorphy2-dicts-ru, rusenttokenize, numpy, scikit-learn, multidict, yarl, pamqp, aiormq, aio-pika, pydantic, starlette, fastapi, tqdm, overrides, fuzzywuzzy, requests, pytelegrambotapi, dawg-python, pymorphy2-dicts, pymorphy2, nltk, Cython, deeppavlov\n",
            "\u001b[33m  WARNING: The script uvicorn is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.6 are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script tqdm is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The scripts cygdb, cython and cythonize are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "Successfully installed Cython-0.29.14 aio-pika-6.4.1 aiormq-3.2.1 cryptography-2.8 dawg-python-0.7.2 deeppavlov-0.8.0 fastapi-0.47.1 fuzzywuzzy-0.17.0 h11-0.9.0 httptools-0.0.13 multidict-4.7.5 nltk-3.4.5 numpy-1.18.0 overrides-2.7.0 pamqp-2.3.0 pydantic-1.3 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 pymorphy2-dicts-ru-2.4.404381.4453942 pyopenssl-19.1.0 pytelegrambotapi-3.6.7 requests-2.22.0 rusenttokenize-0.0.5 scikit-learn-0.21.2 starlette-0.12.9 tqdm-4.41.1 uvicorn-0.11.1 uvloop-0.14.0 websockets-8.1 yarl-1.4.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "requests",
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXKbKjTB6gsY",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 2. Language modeling.\n",
        "\n",
        "This task is devoted to language modeling. Its goal is to write in PyTorch an RNN-based language model. Since word-based language modeling requires long training and is memory-consuming due to large vocabulary, we start with character-based language modeling. We are going to train the model to generate words as sequence of characters. During training we teach it to predict characters of the words in the training set.\n",
        "\n",
        "\n",
        "\n",
        "## Task 1. Character-based language modeling: data preparation (15 points)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuDXcqhP6gsZ",
        "colab_type": "text"
      },
      "source": [
        "We train the language models on the materials of **Sigmorphon 2018 Shared Task**. First, download the Russian datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjvyqV-f6gsa",
        "colab_type": "code",
        "outputId": "01e673d1-8a1f-4d38-f7f5-c3c128c755af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-train-high\n",
        "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-dev\n",
        "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-test"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-31 13:33:00--  https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-train-high\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 533309 (521K) [text/plain]\n",
            "Saving to: ‘russian-train-high’\n",
            "\n",
            "\rrussian-train-high    0%[                    ]       0  --.-KB/s               \rrussian-train-high  100%[===================>] 520.81K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-03-31 13:33:01 (13.6 MB/s) - ‘russian-train-high’ saved [533309/533309]\n",
            "\n",
            "--2020-03-31 13:33:02--  https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-dev\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53671 (52K) [text/plain]\n",
            "Saving to: ‘russian-dev’\n",
            "\n",
            "russian-dev         100%[===================>]  52.41K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-03-31 13:33:03 (3.69 MB/s) - ‘russian-dev’ saved [53671/53671]\n",
            "\n",
            "--2020-03-31 13:33:06--  https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-test\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53514 (52K) [text/plain]\n",
            "Saving to: ‘russian-test’\n",
            "\n",
            "russian-test        100%[===================>]  52.26K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-03-31 13:33:06 (3.52 MB/s) - ‘russian-test’ saved [53514/53514]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrGJ4DExzJqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import trange\n",
        "from deeppavlov.core.data.simple_vocab import SimpleVocabulary\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL0asATP6gsd",
        "colab_type": "text"
      },
      "source": [
        "**1.1 (1 points)**\n",
        "All the files contain tab-separated triples ```<lemma>-<form>-<tags>```, where ```<form>``` may contain spaces (*будете соответствовать*). Write a function that loads a list of all word forms, that do not contain spaces.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94IFwAeN6gse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_infile(infile):\n",
        "    forms = []\n",
        "    with open(infile, 'r') as f:\n",
        "        for line in f:\n",
        "            lemma, form, tags = line.split('\\t')\n",
        "            if ' ' in form.strip():\n",
        "                continue\n",
        "            forms.append(form.lower())\n",
        "    return forms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqhf39-w6gsg",
        "colab_type": "code",
        "outputId": "469f7f5e-fe3e-45ea-a7bd-cc1803fcc64e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "train_words = read_infile(\"russian-train-high\")\n",
        "dev_words = read_infile(\"russian-dev\")\n",
        "test_words = read_infile(\"russian-test\")\n",
        "print(len(train_words), len(dev_words), len(test_words))\n",
        "print(*train_words[:10])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9213 917 922\n",
            "валлонскому незаконченным истрёпывав личного серьгам необоснованным тюти заросла идеальна гулкой\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy7FPqgZ6gsj",
        "colab_type": "text"
      },
      "source": [
        "**1.2 (2 points)** Write a **Vocabulary** class that allows to transform symbols into their indexes. The class should have the method ```__call__``` that applies this transformation to sequences of symbols and batches of sequences as well. You can also use [SimpleVocabulary](https://github.com/deepmipt/DeepPavlov/blob/c10b079b972493220c82a643d47d718d5358c7f4/deeppavlov/core/data/simple_vocab.py#L31) from DeepPavlov. Fit an instance of this class on the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSyUwp9P6gsj",
        "colab_type": "code",
        "outputId": "bfcc54f3-c234-40ad-eb5d-0f957b60f3fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "vocab = SimpleVocabulary(special_tokens=[\"BEGIN\", \"END\", \"PAD\"], save_path='.')\n",
        "vocab.fit([list(x) for x in train_words])\n",
        "print(len(vocab))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-31 13:33:07.396 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 49: No load path is set for SimpleVocabulary in 'infer' mode. Using save path instead\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVn-CovH_Y7n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "effc5fca-3d96-43c2-e222-47461a1a81c5"
      },
      "source": [
        "vocab(['BEGIN', 'END', 'PAD'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dhKqdNt6gsm",
        "colab_type": "text"
      },
      "source": [
        "**1.3 (2 points)** Write a **Dataset** class, which should be inherited from ```torch.utils.data.Dataset```. It should take a list of words and the ```vocab``` as initialization arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctpsRix86gso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset(TorchDataset):\n",
        "    \"\"\"Custom data.Dataset compatible with data.DataLoader.\"\"\"\n",
        "    def __init__(self, data, vocab):\n",
        "        self.data = data\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Returns one tensor pair (source and target). The source tensor corresponds to the input word,\n",
        "        with \"BEGIN\" and \"END\" symbols attached. The target tensor should contain the answers\n",
        "        for the language model that obtain these word as input.        \n",
        "        \"\"\"\n",
        "        source = [\"BEGIN\", *self.data[index]]\n",
        "        target = [*self.data[index], \"END\"]\n",
        "\n",
        "        source_transformed = self.vocab(source)\n",
        "        target_transformed = self.vocab(target)\n",
        "\n",
        "        return torch.tensor(source_transformed), torch.tensor(target_transformed)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMSc__gA6gsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = Dataset(train_words, vocab)\n",
        "dev_dataset = Dataset(dev_words, vocab)\n",
        "test_dataset = Dataset(test_words, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgeLcu8M_2Y7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "6251ca6e-7277-492d-841c-8adb7c5dfcba"
      },
      "source": [
        "train_dataset[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 0, 11,  4, 12, 12,  3,  7,  9, 14,  3, 13, 16]),\n",
              " tensor([11,  4, 12, 12,  3,  7,  9, 14,  3, 13, 16,  1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqfZAnvK6gsu",
        "colab_type": "text"
      },
      "source": [
        "**1.4 (3 points)** Use a standard ```torch.utils.data.DataLoader``` to obtain an iterable over batches. Print the shape of first 10 input batches with ```batch_size=1```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNkqpms-6gsu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "c4e759fb-411f-4256-a81d-c3dad118f915"
      },
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=1)\n",
        "for i, (source, target) in enumerate(train_dataloader):\n",
        "    if i == 10:\n",
        "        break\n",
        "\n",
        "    print(f'Butch number {i}:')\n",
        "    print(source, target, sep='\\n')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Butch number 0:\n",
            "tensor([[ 0, 11,  4, 12, 12,  3,  7,  9, 14,  3, 13, 16]])\n",
            "tensor([[11,  4, 12, 12,  3,  7,  9, 14,  3, 13, 16,  1]])\n",
            "Butch number 1:\n",
            "tensor([[ 0,  7,  5, 22,  4, 14,  3,  7, 25,  5,  7,  7, 17, 13]])\n",
            "tensor([[ 7,  5, 22,  4, 14,  3,  7, 25,  5,  7,  7, 17, 13,  1]])\n",
            "Butch number 2:\n",
            "tensor([[ 0,  6,  9, 10,  8, 33, 15, 17, 11,  4, 11]])\n",
            "tensor([[ 6,  9, 10,  8, 33, 15, 17, 11,  4, 11,  1]])\n",
            "Butch number 3:\n",
            "tensor([[ 0, 12,  6, 25,  7,  3, 20,  3]])\n",
            "tensor([[12,  6, 25,  7,  3, 20,  3,  1]])\n",
            "Butch number 4:\n",
            "tensor([[ 0,  9,  5,  8, 23, 20,  4, 13]])\n",
            "tensor([[ 9,  5,  8, 23, 20,  4, 13,  1]])\n",
            "Butch number 5:\n",
            "tensor([[ 0,  7,  5,  3, 24,  3,  9,  7,  3, 11,  4,  7,  7, 17, 13]])\n",
            "tensor([[ 7,  5,  3, 24,  3,  9,  7,  3, 11,  4,  7,  7, 17, 13,  1]])\n",
            "Butch number 6:\n",
            "tensor([[ 0, 10, 28, 10,  6]])\n",
            "tensor([[10, 28, 10,  6,  1]])\n",
            "Butch number 7:\n",
            "tensor([[ 0, 22,  4,  8,  3,  9, 12,  4]])\n",
            "tensor([[22,  4,  8,  3,  9, 12,  4,  1]])\n",
            "Butch number 8:\n",
            "tensor([[ 0,  6, 18,  5,  4, 12, 23,  7,  4]])\n",
            "tensor([[ 6, 18,  5,  4, 12, 23,  7,  4,  1]])\n",
            "Butch number 9:\n",
            "tensor([[ 0, 20, 16, 12, 14,  3, 21]])\n",
            "tensor([[20, 16, 12, 14,  3, 21,  1]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPbl8j876gsy",
        "colab_type": "text"
      },
      "source": [
        "**(1.5) 1 point** Explain, why this does not work with larger batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWq7MUanB2Z8",
        "colab_type": "text"
      },
      "source": [
        "Это происходит из-за того, что слова могут иметь разную длина, а тенсзоры поддерживают только равные длины по всем осям"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCxEW1Ly6gsz",
        "colab_type": "text"
      },
      "source": [
        "**(1.6) 5 points** Write a function **collate** that allows you to deal with batches of greater size. See [discussion](https://discuss.pytorch.org/t/dataloader-for-various-length-of-data/6418/8) for an example. Implement your function as a class ```__call__``` method to make it more flexible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7c4FHnP6gsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_tensor(vec, length, dim, pad_symbol):\n",
        "    \"\"\"\n",
        "    Pads a vector ``vec`` up to length ``length`` along axis ``dim`` with pad symbol ``pad_symbol``.\n",
        "    \"\"\"\n",
        "    pad_shape = list(vec.shape)\n",
        "    pad_shape[dim] = length - pad_shape[dim]\n",
        "    pad_vec = torch.zeros(pad_shape, dtype=vec.dtype) + pad_symbol\n",
        "    return torch.cat([vec, pad_vec], dim=dim)\n",
        "\n",
        "class Padder:\n",
        "    def __init__(self, dim=0, pad_symbol=0):\n",
        "        self.dim = dim\n",
        "        self.pad_symbol = pad_symbol\n",
        "    \n",
        "    def __call__(self, batch):\n",
        "        max_len = -1\n",
        "        for single_batch in batch:\n",
        "            for row in single_batch:\n",
        "                max_len = max(max_len, row.shape[self.dim])\n",
        "        \n",
        "        batches = [[] for _ in range(len(batch[0]))]\n",
        "        for single_batch in batch:\n",
        "            for i, row in enumerate(single_batch):\n",
        "                batches[i].append(pad_tensor(row, max_len, self.dim, self.pad_symbol))\n",
        "\n",
        "        for i in range(len(batches)):\n",
        "            batches[i] = torch.cat(batches[i])\n",
        "        return batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdE9FA35D-8z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a845aec7-ef57-4faa-b888-ce553fc52dc2"
      },
      "source": [
        "pad_tensor(torch.tensor([1, 2, 3]), 5, 0, 0)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPR6zNtJ6gs2",
        "colab_type": "text"
      },
      "source": [
        "**(1.7) 1 points** Again, use ```torch.utils.data.DataLoader``` to obtain an iterable over batches. Print the shape of first 10 input batches with the batch size you like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6W7Tt2o6gs2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "outputId": "dcabdf1e-7119-4f2a-d8eb-288c5344d6fe"
      },
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=2, collate_fn=Padder(pad_symbol=vocab('PAD')))\n",
        "for i, (source, target) in enumerate(train_dataloader):\n",
        "    if i == 10:\n",
        "        break\n",
        "\n",
        "    print(f'Butch number {i}:')\n",
        "    print(source, target, sep='\\n')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Butch number 0:\n",
            "tensor([ 0, 11,  4, 12, 12,  3,  7,  9, 14,  3, 13, 16,  2,  2,  0,  7,  5, 22,\n",
            "         4, 14,  3,  7, 25,  5,  7,  7, 17, 13])\n",
            "tensor([11,  4, 12, 12,  3,  7,  9, 14,  3, 13, 16,  1,  2,  2,  7,  5, 22,  4,\n",
            "        14,  3,  7, 25,  5,  7,  7, 17, 13,  1])\n",
            "Butch number 1:\n",
            "tensor([ 0,  6,  9, 10,  8, 33, 15, 17, 11,  4, 11,  0, 12,  6, 25,  7,  3, 20,\n",
            "         3,  2,  2,  2])\n",
            "tensor([ 6,  9, 10,  8, 33, 15, 17, 11,  4, 11,  1, 12,  6, 25,  7,  3, 20,  3,\n",
            "         1,  2,  2,  2])\n",
            "Butch number 2:\n",
            "tensor([ 0,  9,  5,  8, 23, 20,  4, 13,  2,  2,  2,  2,  2,  2,  2,  0,  7,  5,\n",
            "         3, 24,  3,  9,  7,  3, 11,  4,  7,  7, 17, 13])\n",
            "tensor([ 9,  5,  8, 23, 20,  4, 13,  1,  2,  2,  2,  2,  2,  2,  2,  7,  5,  3,\n",
            "        24,  3,  9,  7,  3, 11,  4,  7,  7, 17, 13,  1])\n",
            "Butch number 3:\n",
            "tensor([ 0, 10, 28, 10,  6,  2,  2,  2,  0, 22,  4,  8,  3,  9, 12,  4])\n",
            "tensor([10, 28, 10,  6,  1,  2,  2,  2, 22,  4,  8,  3,  9, 12,  4,  1])\n",
            "Butch number 4:\n",
            "tensor([ 0,  6, 18,  5,  4, 12, 23,  7,  4,  0, 20, 16, 12, 14,  3, 21,  2,  2])\n",
            "tensor([ 6, 18,  5,  4, 12, 23,  7,  4,  1, 20, 16, 12, 14,  3, 21,  1,  2,  2])\n",
            "Butch number 5:\n",
            "tensor([ 0, 12,  5,  9,  3, 15,  6, 12, 23,  7,  3,  0, 22,  4,  8,  3,  9, 27,\n",
            "         5,  5,  2,  2])\n",
            "tensor([12,  5,  9,  3, 15,  6, 12, 23,  7,  3,  1, 22,  4,  8,  3,  9, 27,  5,\n",
            "         5,  1,  2,  2])\n",
            "Butch number 6:\n",
            "tensor([ 0, 24,  8,  3,  7,  5, 15,  3,  5, 22, 18,  5,  2,  2,  2,  0, 15,  8,\n",
            "         3, 24, 12,  5, 13,  4, 10,  6, 25,  7, 17, 26])\n",
            "tensor([24,  8,  3,  7,  5, 15,  3,  5, 22, 18,  5,  1,  2,  2,  2, 15,  8,  3,\n",
            "        24, 12,  5, 13,  4, 10,  6, 25,  7, 17, 26,  1])\n",
            "Butch number 7:\n",
            "tensor([ 0, 10, 16, 27, 33,  7,  3,  5,  2,  0, 10, 14,  7, 16, 12,  6,  9, 23])\n",
            "tensor([10, 16, 27, 33,  7,  3,  5,  1,  2, 10, 14,  7, 16, 12,  6,  9, 23,  1])\n",
            "Butch number 8:\n",
            "tensor([ 0,  9, 15,  4,  9,  6, 10,  5, 12,  6,  2,  0, 14,  3,  9, 13,  3,  7,\n",
            "         4, 11, 10, 16])\n",
            "tensor([ 9, 15,  4,  9,  6, 10,  5, 12,  6,  1,  2, 14,  3,  9, 13,  3,  7,  4,\n",
            "        11, 10, 16,  1])\n",
            "Butch number 9:\n",
            "tensor([ 0,  8,  4,  9, 14, 12,  4,  7, 19, 11, 27,  6,  9, 23,  0,  6,  9, 26,\n",
            "         3, 18,  4, 13,  6,  2,  2,  2,  2,  2])\n",
            "tensor([ 8,  4,  9, 14, 12,  4,  7, 19, 11, 27,  6,  9, 23,  1,  6,  9, 26,  3,\n",
            "        18,  4, 13,  6,  1,  2,  2,  2,  2,  2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUW4o15z6gs5",
        "colab_type": "text"
      },
      "source": [
        "## Task 2. Character-based language modeling. (35 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iGwVrBB6gs5",
        "colab_type": "text"
      },
      "source": [
        "**2.1 (5 points)** Write a network that performs language modeling. It should include three layers:\n",
        "1. **Embedding** layer that transforms input symbols into vectors.\n",
        "2. An **RNN** layer that outputs a sequence of hidden states (you may use https://pytorch.org/docs/stable/nn.html#gru).\n",
        "3. A **Linear** layer with ``softmax`` activation that produces the output distribution for each symbol."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42f7Bd3P6gs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embeddings_dim, hidden_size):\n",
        "        super(RNNLM, self).__init__()\n",
        "        self.embeddings_dim = embeddings_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.emb = nn.Embedding(vocab_size, embeddings_dim)\n",
        "        self.rnn = nn.GRU(embeddings_dim, hidden_size, batch_first=True)\n",
        "        self.lin = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.lin.weight)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, hidden=None):\n",
        "        if hidden is None:\n",
        "            hidden = torch.zeros(1, inputs.shape[0], self.embeddings_dim)\n",
        "\n",
        "        inputs = self.emb(inputs)\n",
        "        inputs, h = self.rnn(inputs)\n",
        "        inputs = stlf.lin(inputs)\n",
        "        \n",
        "        return inputs, h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRXgMJFF6gs8",
        "colab_type": "text"
      },
      "source": [
        "**2.2 (1 points)** Write a function ``validate_on_batch`` that takes as input a model, a batch of inputs and a batch of outputs, and the loss criterion, and outputs the loss tensor for the whole batch. This loss should not be normalized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGagp3_y6gs8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate_on_batch(model, criterion, x, y):\n",
        "    return criterion(model(x), y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9HBeoap6gs-",
        "colab_type": "text"
      },
      "source": [
        "**2.3 (1 points)** Write a function ``train_on_batch`` that accepts all the arguments of ``validate_on_batch`` and also an optimizer, calculates loss and makes a single step of gradient optimization. This function should call ``validate_on_batch`` inside."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc_BwsRr6gs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_on_batch(model, criterion, x, y, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    loss = validate_on_batch(model, criterion, x, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUkc_97q6gtC",
        "colab_type": "text"
      },
      "source": [
        "**2.4 (3 points)** Write a training loop. You should define your ``RNNLM`` model, the criterion, the optimizer and the hyperparameters (number of epochs and batch size). Then train the model for a required number of epochs. On each epoch evaluate the average training loss and the average loss on the validation set. \n",
        "\n",
        "**2.5 (3 points)** Do not forget to average your loss over only non-padding symbols, otherwise it will be too optimistic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzScqh4lx_MW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "USE_CUDA = True\n",
        "TOTAL_EPOCH = 100\n",
        "LAST_EPOCH = 0\n",
        "BATCH_SIZE = 16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kVqS9Sgyaej",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e5b99404-6fde-4788-a827-4b4d21d1ee50"
      },
      "source": [
        "if USE_CUDA and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(\"device: \", device)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device:  cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp2MbAYJzn0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RNNLM(\n",
        "    vocab_size=len(vocab),\n",
        "    embeddings_dim=64,\n",
        "    hidden_size=64\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.03)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=Padder(dim=0, pad_symbol=vocab(\"PAD\"))\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    dev_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=Padder(dim=0, pad_symbol=vocab(\"PAD\"))\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymVEYm7ezoVq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7620313c-ae68-4cfd-beb7-3851dc7f0bde"
      },
      "source": [
        "%%time\n",
        "\n",
        "for i in trange(LAST_EPOCH, TOTAL_EPOCH):\n",
        "    pass"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 340170.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4 ms, sys: 1.03 ms, total: 5.03 ms\n",
            "Wall time: 4.26 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEBiV4PF6gtE",
        "colab_type": "text"
      },
      "source": [
        "**2.6 (5 points)** Write a function **predict_on_batch** that outputs letter probabilities of all words in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPN4mopV6gtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyFZQDK56gtI",
        "colab_type": "text"
      },
      "source": [
        "**2.7 (1 points)** Calculate the letter probabilities for all words in the test dataset. Print them for 20 last words. Do not forget to disable shuffling in the ``DataLoader``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc8qjXL66gtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "== YOUR CODE HERE ==\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWkB4GRE6gtK",
        "colab_type": "text"
      },
      "source": [
        "**2.8 (5 points)** Write a function that generates a single word (sequence of indexes) given the model. Do not forget about the hidden state! Be careful about start and end symbol indexes. Use ``torch.multinomial`` for sampling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT3p4yme6gtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate(model, max_length=20, start_index=1, end_index=2):\n",
        "    \"\"\"\n",
        "    == YOUR CODE HERE ==\n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDnJ7GXl6gtN",
        "colab_type": "text"
      },
      "source": [
        "**2.9 (1 points)** Use ``generate`` to sample 20 pseudowords. Do not forget to transform indexes to letters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jxoIQ8K6gtN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(20):\n",
        "    \"\"\"\n",
        "    == YOUR CODE HERE ==\n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dYwtzuo6gtP",
        "colab_type": "text"
      },
      "source": [
        "**(2.10) 5 points** Write a batched version of the generation function. You should sample the following symbol only for the words that are not finished yet, so apply a boolean mask to trace active words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9825WPZd6gtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(model, batch_size, max_length = 20, start_index=1, end_index=2):\n",
        "    \"\"\"\n",
        "    == YOUR CODE HERE ==\n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A16PTqBk6gtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generated = []\n",
        "for _ in range(2):\n",
        "    generated += generate_batch(model, batch_size=10)\n",
        "\"\"\"\n",
        "== YOUR CODE HERE ==\n",
        "\"\"\"\n",
        "for elem in transformed:\n",
        "    print(\"\".join(elem))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dtpBFyI6gtU",
        "colab_type": "text"
      },
      "source": [
        "**(2.11) 5 points** Experiment with the type of RNN, number of layers, units and/or dropout to improve the perplexity of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl8bgJT76gtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}