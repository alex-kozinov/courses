{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "task1,2_character_lm.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVBpzGzF6icO",
        "colab_type": "code",
        "outputId": "bf93f225-102e-441d-fef1-0138a383ebda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    USE_COLAB = True\n",
        "except:\n",
        "    USE_COLAB = False\n",
        "print(\"Use colab - \", USE_COLAB)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use colab -  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBbWPRe7Bt9q",
        "colab_type": "code",
        "outputId": "e1849dfe-1e57-4bf3-91ff-681952597747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865
        }
      },
      "source": [
        "if USE_COLAB:\n",
        "    !pip install --user deeppavlov"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: deeppavlov in /root/.local/lib/python3.6/site-packages (0.8.0)\n",
            "Requirement already satisfied: overrides==2.7.0 in /root/.local/lib/python3.6/site-packages (from deeppavlov) (2.7.0)\n",
            "Requirement already satisfied: fastapi==0.47.1 in /root/.local/lib/python3.6/site-packages (from deeppavlov) (0.47.1)\n",
            "Requirement already satisfied: Cython==0.29.14 in /root/.local/lib/python3.6/site-packages (from deeppavlov) (0.29.14)\n",
            "Requirement already satisfied: uvicorn==0.11.1 in /root/.local/lib/python3.6/site-packages (from deeppavlov) (0.11.1)\n",
            "Requirement already satisfied: nltk==3.4.5 in /root/.local/lib/python3.6/site-packages (from deeppavlov) (3.4.5)\n",
            "Requirement already satisfied: requests==2.22.0 in /root/.local/lib/python3.6/site-packages (from deeppavlov) (2.22.0)\n",
            "Requirement already satisfied: pandas==0.25.3 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (0.25.3)\n",
            "Requirement already satisfied: pytelegrambotapi==3.6.7 in /root/.local/lib/python3.6/site-packages (from deeppavlov) (3.6.7)\n",
            "Requirement already satisfied: aio-pika==6.4.1 in /root/.local/lib/python3.6/site-packages (from deeppavlov) (6.4.1)\n",
            "Requirement already satisfied: numpy==1.18.0 in /root/.local/lib/python3.6/site-packages (from deeppavlov) (1.18.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (1.4.1)\n",
            "Requirement already satisfied: rusenttokenize==0.0.5 in /root/.local/lib/python3.6/site-packages (from deeppavlov) (0.0.5)\n",
            "Requirement already satisfied: pymorphy2==0.8 in /root/.local/lib/python3.6/site-packages (from deeppavlov) (0.8)\n",
            "Requirement already satisfied: pydantic==1.3 in /root/.local/lib/python3.6/site-packages (from deeppavlov) (1.3)\n",
            "Requirement already satisfied: tqdm==4.41.1 in /root/.local/lib/python3.6/site-packages (from deeppavlov) (4.41.1)\n",
            "Requirement already satisfied: scikit-learn==0.21.2 in /root/.local/lib/python3.6/site-packages (from deeppavlov) (0.21.2)\n",
            "Requirement already satisfied: pyopenssl==19.1.0 in /root/.local/lib/python3.6/site-packages (from deeppavlov) (19.1.0)\n",
            "Requirement already satisfied: fuzzywuzzy==0.17.0 in /root/.local/lib/python3.6/site-packages (from deeppavlov) (0.17.0)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru in /root/.local/lib/python3.6/site-packages (from deeppavlov) (2.4.404381.4453942)\n",
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (2.10.0)\n",
            "Requirement already satisfied: starlette<=0.12.9,>=0.12.9 in /root/.local/lib/python3.6/site-packages (from fastapi==0.47.1->deeppavlov) (0.12.9)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /root/.local/lib/python3.6/site-packages (from uvicorn==0.11.1->deeppavlov) (0.9.0)\n",
            "Requirement already satisfied: click==7.* in /usr/local/lib/python3.6/dist-packages (from uvicorn==0.11.1->deeppavlov) (7.1.1)\n",
            "Requirement already satisfied: httptools==0.0.13; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"pypy\" in /root/.local/lib/python3.6/site-packages (from uvicorn==0.11.1->deeppavlov) (0.0.13)\n",
            "Requirement already satisfied: websockets==8.* in /root/.local/lib/python3.6/site-packages (from uvicorn==0.11.1->deeppavlov) (8.1)\n",
            "Requirement already satisfied: uvloop>=0.14.0; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"pypy\" in /root/.local/lib/python3.6/site-packages (from uvicorn==0.11.1->deeppavlov) (0.14.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.5->deeppavlov) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas==0.25.3->deeppavlov) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas==0.25.3->deeppavlov) (2018.9)\n",
            "Requirement already satisfied: aiormq<4,>=3.2.0 in /root/.local/lib/python3.6/site-packages (from aio-pika==6.4.1->deeppavlov) (3.2.1)\n",
            "Requirement already satisfied: yarl in /root/.local/lib/python3.6/site-packages (from aio-pika==6.4.1->deeppavlov) (1.4.2)\n",
            "Requirement already satisfied: dawg-python>=0.7 in /root/.local/lib/python3.6/site-packages (from pymorphy2==0.8->deeppavlov) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /root/.local/lib/python3.6/site-packages (from pymorphy2==0.8->deeppavlov) (2.4.393442.3710985)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
            "Requirement already satisfied: dataclasses>=0.6; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from pydantic==1.3->deeppavlov) (0.7)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.2->deeppavlov) (0.14.1)\n",
            "Requirement already satisfied: cryptography>=2.8 in /root/.local/lib/python3.6/site-packages (from pyopenssl==19.1.0->deeppavlov) (2.8)\n",
            "Requirement already satisfied: pamqp==2.3.0 in /root/.local/lib/python3.6/site-packages (from aiormq<4,>=3.2.0->aio-pika==6.4.1->deeppavlov) (2.3.0)\n",
            "Requirement already satisfied: multidict>=4.0 in /root/.local/lib/python3.6/site-packages (from yarl->aio-pika==6.4.1->deeppavlov) (4.7.5)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (1.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (2.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXKbKjTB6gsY",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 2. Language modeling.\n",
        "\n",
        "This task is devoted to language modeling. Its goal is to write in PyTorch an RNN-based language model. Since word-based language modeling requires long training and is memory-consuming due to large vocabulary, we start with character-based language modeling. We are going to train the model to generate words as sequence of characters. During training we teach it to predict characters of the words in the training set.\n",
        "\n",
        "\n",
        "\n",
        "## Task 1. Character-based language modeling: data preparation (15 points)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuDXcqhP6gsZ",
        "colab_type": "text"
      },
      "source": [
        "We train the language models on the materials of **Sigmorphon 2018 Shared Task**. First, download the Russian datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjvyqV-f6gsa",
        "colab_type": "code",
        "outputId": "86c88cf8-90c3-49ef-e532-ac0ede2e039d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-train-high\n",
        "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-dev\n",
        "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-test"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-31 15:58:01--  https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-train-high\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 533309 (521K) [text/plain]\n",
            "Saving to: ‘russian-train-high.1’\n",
            "\n",
            "\rrussian-train-high.   0%[                    ]       0  --.-KB/s               \rrussian-train-high. 100%[===================>] 520.81K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-03-31 15:58:01 (13.6 MB/s) - ‘russian-train-high.1’ saved [533309/533309]\n",
            "\n",
            "--2020-03-31 15:58:04--  https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-dev\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53671 (52K) [text/plain]\n",
            "Saving to: ‘russian-dev.1’\n",
            "\n",
            "russian-dev.1       100%[===================>]  52.41K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-03-31 15:58:04 (3.60 MB/s) - ‘russian-dev.1’ saved [53671/53671]\n",
            "\n",
            "--2020-03-31 15:58:06--  https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-test\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53514 (52K) [text/plain]\n",
            "Saving to: ‘russian-test.1’\n",
            "\n",
            "russian-test.1      100%[===================>]  52.26K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-03-31 15:58:06 (3.47 MB/s) - ‘russian-test.1’ saved [53514/53514]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrGJ4DExzJqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import trange\n",
        "from deeppavlov.core.data.simple_vocab import SimpleVocabulary\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL0asATP6gsd",
        "colab_type": "text"
      },
      "source": [
        "**1.1 (1 points)**\n",
        "All the files contain tab-separated triples ```<lemma>-<form>-<tags>```, where ```<form>``` may contain spaces (*будете соответствовать*). Write a function that loads a list of all word forms, that do not contain spaces.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94IFwAeN6gse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_infile(infile):\n",
        "    forms = []\n",
        "    with open(infile, 'r') as f:\n",
        "        for line in f:\n",
        "            lemma, form, tags = line.split('\\t')\n",
        "            if ' ' in form.strip():\n",
        "                continue\n",
        "            forms.append(form.lower())\n",
        "    return forms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqhf39-w6gsg",
        "colab_type": "code",
        "outputId": "0a962fb7-7e70-453e-ee03-b2309764e057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "train_words = read_infile(\"russian-train-high\")\n",
        "dev_words = read_infile(\"russian-dev\")\n",
        "test_words = read_infile(\"russian-test\")\n",
        "print(len(train_words), len(dev_words), len(test_words))\n",
        "print(*train_words[:10])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9213 917 922\n",
            "валлонскому незаконченным истрёпывав личного серьгам необоснованным тюти заросла идеальна гулкой\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy7FPqgZ6gsj",
        "colab_type": "text"
      },
      "source": [
        "**1.2 (2 points)** Write a **Vocabulary** class that allows to transform symbols into their indexes. The class should have the method ```__call__``` that applies this transformation to sequences of symbols and batches of sequences as well. You can also use [SimpleVocabulary](https://github.com/deepmipt/DeepPavlov/blob/c10b079b972493220c82a643d47d718d5358c7f4/deeppavlov/core/data/simple_vocab.py#L31) from DeepPavlov. Fit an instance of this class on the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSyUwp9P6gsj",
        "colab_type": "code",
        "outputId": "6d2da7d9-4be5-4dfd-901c-3ef1febf714e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "vocab = SimpleVocabulary(special_tokens=[\"BEGIN\", \"END\", \"PAD\"], save_path='.')\n",
        "vocab.fit([list(x) for x in train_words])\n",
        "print(len(vocab))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-31 15:58:09.754 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 49: No load path is set for SimpleVocabulary in 'infer' mode. Using save path instead\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVn-CovH_Y7n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2528a725-b2e0-4bbd-e514-55cb20ba440e"
      },
      "source": [
        "vocab(['BEGIN', 'END', 'PAD'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dhKqdNt6gsm",
        "colab_type": "text"
      },
      "source": [
        "**1.3 (2 points)** Write a **Dataset** class, which should be inherited from ```torch.utils.data.Dataset```. It should take a list of words and the ```vocab``` as initialization arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctpsRix86gso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset(TorchDataset):\n",
        "    \"\"\"Custom data.Dataset compatible with data.DataLoader.\"\"\"\n",
        "    def __init__(self, data, vocab):\n",
        "        self.data = data\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Returns one tensor pair (source and target). The source tensor corresponds to the input word,\n",
        "        with \"BEGIN\" and \"END\" symbols attached. The target tensor should contain the answers\n",
        "        for the language model that obtain these word as input.        \n",
        "        \"\"\"\n",
        "        source = [\"BEGIN\", *self.data[index]]\n",
        "        target = [*self.data[index], \"END\"]\n",
        "\n",
        "        source_transformed = self.vocab(source)\n",
        "        target_transformed = self.vocab(target)\n",
        "\n",
        "        return torch.tensor(source_transformed), torch.tensor(target_transformed)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMSc__gA6gsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = Dataset(train_words, vocab)\n",
        "dev_dataset = Dataset(dev_words, vocab)\n",
        "test_dataset = Dataset(test_words, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgeLcu8M_2Y7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "fd7079e1-289a-49ba-cb07-4dbdb552d724"
      },
      "source": [
        "train_dataset[0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 0, 11,  4, 12, 12,  3,  7,  9, 14,  3, 13, 16]),\n",
              " tensor([11,  4, 12, 12,  3,  7,  9, 14,  3, 13, 16,  1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqfZAnvK6gsu",
        "colab_type": "text"
      },
      "source": [
        "**1.4 (3 points)** Use a standard ```torch.utils.data.DataLoader``` to obtain an iterable over batches. Print the shape of first 10 input batches with ```batch_size=1```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNkqpms-6gsu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "d44116f2-df50-45d5-d45e-34bd529649a3"
      },
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=1)\n",
        "for i, (source, target) in enumerate(train_dataloader):\n",
        "    if i == 10:\n",
        "        break\n",
        "\n",
        "    print(f'Butch number {i}:')\n",
        "    print(source, target, sep='\\n')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Butch number 0:\n",
            "tensor([[ 0, 11,  4, 12, 12,  3,  7,  9, 14,  3, 13, 16]])\n",
            "tensor([[11,  4, 12, 12,  3,  7,  9, 14,  3, 13, 16,  1]])\n",
            "Butch number 1:\n",
            "tensor([[ 0,  7,  5, 22,  4, 14,  3,  7, 25,  5,  7,  7, 17, 13]])\n",
            "tensor([[ 7,  5, 22,  4, 14,  3,  7, 25,  5,  7,  7, 17, 13,  1]])\n",
            "Butch number 2:\n",
            "tensor([[ 0,  6,  9, 10,  8, 33, 15, 17, 11,  4, 11]])\n",
            "tensor([[ 6,  9, 10,  8, 33, 15, 17, 11,  4, 11,  1]])\n",
            "Butch number 3:\n",
            "tensor([[ 0, 12,  6, 25,  7,  3, 20,  3]])\n",
            "tensor([[12,  6, 25,  7,  3, 20,  3,  1]])\n",
            "Butch number 4:\n",
            "tensor([[ 0,  9,  5,  8, 23, 20,  4, 13]])\n",
            "tensor([[ 9,  5,  8, 23, 20,  4, 13,  1]])\n",
            "Butch number 5:\n",
            "tensor([[ 0,  7,  5,  3, 24,  3,  9,  7,  3, 11,  4,  7,  7, 17, 13]])\n",
            "tensor([[ 7,  5,  3, 24,  3,  9,  7,  3, 11,  4,  7,  7, 17, 13,  1]])\n",
            "Butch number 6:\n",
            "tensor([[ 0, 10, 28, 10,  6]])\n",
            "tensor([[10, 28, 10,  6,  1]])\n",
            "Butch number 7:\n",
            "tensor([[ 0, 22,  4,  8,  3,  9, 12,  4]])\n",
            "tensor([[22,  4,  8,  3,  9, 12,  4,  1]])\n",
            "Butch number 8:\n",
            "tensor([[ 0,  6, 18,  5,  4, 12, 23,  7,  4]])\n",
            "tensor([[ 6, 18,  5,  4, 12, 23,  7,  4,  1]])\n",
            "Butch number 9:\n",
            "tensor([[ 0, 20, 16, 12, 14,  3, 21]])\n",
            "tensor([[20, 16, 12, 14,  3, 21,  1]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPbl8j876gsy",
        "colab_type": "text"
      },
      "source": [
        "**(1.5) 1 point** Explain, why this does not work with larger batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWq7MUanB2Z8",
        "colab_type": "text"
      },
      "source": [
        "Это происходит из-за того, что слова могут иметь разную длина, а тенсзоры поддерживают только равные длины по всем осям"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCxEW1Ly6gsz",
        "colab_type": "text"
      },
      "source": [
        "**(1.6) 5 points** Write a function **collate** that allows you to deal with batches of greater size. See [discussion](https://discuss.pytorch.org/t/dataloader-for-various-length-of-data/6418/8) for an example. Implement your function as a class ```__call__``` method to make it more flexible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7c4FHnP6gsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_tensor(vec, length, dim, pad_symbol):\n",
        "    \"\"\"\n",
        "    Pads a vector ``vec`` up to length ``length`` along axis ``dim`` with pad symbol ``pad_symbol``.\n",
        "    \"\"\"\n",
        "    pad_shape = list(vec.shape)\n",
        "    pad_shape[dim] = length - pad_shape[dim]\n",
        "    pad_vec = torch.zeros(pad_shape, dtype=vec.dtype) + pad_symbol\n",
        "    return torch.cat([vec, pad_vec], dim=dim)\n",
        "\n",
        "class Padder:\n",
        "    def __init__(self, dim=0, pad_symbol=0):\n",
        "        self.dim = dim\n",
        "        self.pad_symbol = pad_symbol\n",
        "    \n",
        "    def __call__(self, batch):\n",
        "        max_len = -1\n",
        "        for single_batch in batch:\n",
        "            for row in single_batch:\n",
        "                max_len = max(max_len, row.shape[self.dim])\n",
        "        \n",
        "        batches = [[] for _ in range(len(batch[0]))]\n",
        "        for single_batch in batch:\n",
        "            for i, row in enumerate(single_batch):\n",
        "                batches[i].append(pad_tensor(row, max_len, self.dim, self.pad_symbol))\n",
        "\n",
        "        for i in range(len(batches)):\n",
        "            batches[i] = torch.stack(batches[i])\n",
        "        return batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdE9FA35D-8z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fcabcc95-825a-4a5f-e126-a68a328d7c89"
      },
      "source": [
        "pad_tensor(torch.tensor([1, 2, 3]), 5, 0, 0)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPR6zNtJ6gs2",
        "colab_type": "text"
      },
      "source": [
        "**(1.7) 1 points** Again, use ```torch.utils.data.DataLoader``` to obtain an iterable over batches. Print the shape of first 10 input batches with the batch size you like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6W7Tt2o6gs2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "outputId": "94ee062c-3572-4522-a8ea-797739bfc381"
      },
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=2, collate_fn=Padder(pad_symbol=vocab('PAD')))\n",
        "for i, (source, target) in enumerate(train_dataloader):\n",
        "    if i == 10:\n",
        "        break\n",
        "\n",
        "    print(f'Butch number {i}:')\n",
        "    print(source, target, sep='\\n')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Butch number 0:\n",
            "tensor([[ 0, 11,  4, 12, 12,  3,  7,  9, 14,  3, 13, 16,  2,  2],\n",
            "        [ 0,  7,  5, 22,  4, 14,  3,  7, 25,  5,  7,  7, 17, 13]])\n",
            "tensor([[11,  4, 12, 12,  3,  7,  9, 14,  3, 13, 16,  1,  2,  2],\n",
            "        [ 7,  5, 22,  4, 14,  3,  7, 25,  5,  7,  7, 17, 13,  1]])\n",
            "Butch number 1:\n",
            "tensor([[ 0,  6,  9, 10,  8, 33, 15, 17, 11,  4, 11],\n",
            "        [ 0, 12,  6, 25,  7,  3, 20,  3,  2,  2,  2]])\n",
            "tensor([[ 6,  9, 10,  8, 33, 15, 17, 11,  4, 11,  1],\n",
            "        [12,  6, 25,  7,  3, 20,  3,  1,  2,  2,  2]])\n",
            "Butch number 2:\n",
            "tensor([[ 0,  9,  5,  8, 23, 20,  4, 13,  2,  2,  2,  2,  2,  2,  2],\n",
            "        [ 0,  7,  5,  3, 24,  3,  9,  7,  3, 11,  4,  7,  7, 17, 13]])\n",
            "tensor([[ 9,  5,  8, 23, 20,  4, 13,  1,  2,  2,  2,  2,  2,  2,  2],\n",
            "        [ 7,  5,  3, 24,  3,  9,  7,  3, 11,  4,  7,  7, 17, 13,  1]])\n",
            "Butch number 3:\n",
            "tensor([[ 0, 10, 28, 10,  6,  2,  2,  2],\n",
            "        [ 0, 22,  4,  8,  3,  9, 12,  4]])\n",
            "tensor([[10, 28, 10,  6,  1,  2,  2,  2],\n",
            "        [22,  4,  8,  3,  9, 12,  4,  1]])\n",
            "Butch number 4:\n",
            "tensor([[ 0,  6, 18,  5,  4, 12, 23,  7,  4],\n",
            "        [ 0, 20, 16, 12, 14,  3, 21,  2,  2]])\n",
            "tensor([[ 6, 18,  5,  4, 12, 23,  7,  4,  1],\n",
            "        [20, 16, 12, 14,  3, 21,  1,  2,  2]])\n",
            "Butch number 5:\n",
            "tensor([[ 0, 12,  5,  9,  3, 15,  6, 12, 23,  7,  3],\n",
            "        [ 0, 22,  4,  8,  3,  9, 27,  5,  5,  2,  2]])\n",
            "tensor([[12,  5,  9,  3, 15,  6, 12, 23,  7,  3,  1],\n",
            "        [22,  4,  8,  3,  9, 27,  5,  5,  1,  2,  2]])\n",
            "Butch number 6:\n",
            "tensor([[ 0, 24,  8,  3,  7,  5, 15,  3,  5, 22, 18,  5,  2,  2,  2],\n",
            "        [ 0, 15,  8,  3, 24, 12,  5, 13,  4, 10,  6, 25,  7, 17, 26]])\n",
            "tensor([[24,  8,  3,  7,  5, 15,  3,  5, 22, 18,  5,  1,  2,  2,  2],\n",
            "        [15,  8,  3, 24, 12,  5, 13,  4, 10,  6, 25,  7, 17, 26,  1]])\n",
            "Butch number 7:\n",
            "tensor([[ 0, 10, 16, 27, 33,  7,  3,  5,  2],\n",
            "        [ 0, 10, 14,  7, 16, 12,  6,  9, 23]])\n",
            "tensor([[10, 16, 27, 33,  7,  3,  5,  1,  2],\n",
            "        [10, 14,  7, 16, 12,  6,  9, 23,  1]])\n",
            "Butch number 8:\n",
            "tensor([[ 0,  9, 15,  4,  9,  6, 10,  5, 12,  6,  2],\n",
            "        [ 0, 14,  3,  9, 13,  3,  7,  4, 11, 10, 16]])\n",
            "tensor([[ 9, 15,  4,  9,  6, 10,  5, 12,  6,  1,  2],\n",
            "        [14,  3,  9, 13,  3,  7,  4, 11, 10, 16,  1]])\n",
            "Butch number 9:\n",
            "tensor([[ 0,  8,  4,  9, 14, 12,  4,  7, 19, 11, 27,  6,  9, 23],\n",
            "        [ 0,  6,  9, 26,  3, 18,  4, 13,  6,  2,  2,  2,  2,  2]])\n",
            "tensor([[ 8,  4,  9, 14, 12,  4,  7, 19, 11, 27,  6,  9, 23,  1],\n",
            "        [ 6,  9, 26,  3, 18,  4, 13,  6,  1,  2,  2,  2,  2,  2]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUW4o15z6gs5",
        "colab_type": "text"
      },
      "source": [
        "## Task 2. Character-based language modeling. (35 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iGwVrBB6gs5",
        "colab_type": "text"
      },
      "source": [
        "**2.1 (5 points)** Write a network that performs language modeling. It should include three layers:\n",
        "1. **Embedding** layer that transforms input symbols into vectors.\n",
        "2. An **RNN** layer that outputs a sequence of hidden states (you may use https://pytorch.org/docs/stable/nn.html#gru).\n",
        "3. A **Linear** layer with ``softmax`` activation that produces the output distribution for each symbol."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42f7Bd3P6gs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embeddings_dim, hidden_size):\n",
        "        super(RNNLM, self).__init__()\n",
        "        self.embeddings_dim = embeddings_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.emb = nn.Embedding(vocab_size, embeddings_dim)\n",
        "        self.rnn = nn.GRU(embeddings_dim, hidden_size, batch_first=True)\n",
        "        self.lin = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        nn.init.kaiming_normal_(self.lin.weight)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, hidden=None):\n",
        "        if hidden is None:\n",
        "            hidden = torch.zeros(1, inputs.shape[0], self.embeddings_dim)\n",
        "\n",
        "        inputs = self.emb(inputs)\n",
        "        inputs, h = self.rnn(inputs)\n",
        "        inputs = self.lin(inputs)\n",
        "        return inputs, h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRXgMJFF6gs8",
        "colab_type": "text"
      },
      "source": [
        "**2.2 (1 points)** Write a function ``validate_on_batch`` that takes as input a model, a batch of inputs and a batch of outputs, and the loss criterion, and outputs the loss tensor for the whole batch. This loss should not be normalized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGagp3_y6gs8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate_on_batch(model, criterion, x, y):\n",
        "    y_dist = model(x)[0]\n",
        "    y_dist = y_dist.view(-1, y_dist.shape[-1])\n",
        "    y = y.view(-1)\n",
        "    non_pad = torch.sum(y != vacab(\"PAD\"))\n",
        "    print(non_pad)\n",
        "    return criterion(y_dist, y) / non_pad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9HBeoap6gs-",
        "colab_type": "text"
      },
      "source": [
        "**2.3 (1 points)** Write a function ``train_on_batch`` that accepts all the arguments of ``validate_on_batch`` and also an optimizer, calculates loss and makes a single step of gradient optimization. This function should call ``validate_on_batch`` inside."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc_BwsRr6gs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_on_batch(model, criterion, x, y, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    loss = validate_on_batch(model, criterion, x, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUkc_97q6gtC",
        "colab_type": "text"
      },
      "source": [
        "**2.4 (3 points)** Write a training loop. You should define your ``RNNLM`` model, the criterion, the optimizer and the hyperparameters (number of epochs and batch size). Then train the model for a required number of epochs. On each epoch evaluate the average training loss and the average loss on the validation set. \n",
        "\n",
        "**2.5 (3 points)** Do not forget to average your loss over only non-padding symbols, otherwise it will be too optimistic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQHXj-I2UbY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, criterion, dataloader):\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    cnt = 0\n",
        "    for source, target in dataloader:\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzScqh4lx_MW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "USE_CUDA = True\n",
        "TOTAL_EPOCH = 100\n",
        "LAST_EPOCH = 0\n",
        "BATCH_SIZE = 16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kVqS9Sgyaej",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bdd8db8e-7335-44ae-e8c5-599c0c4fd6b8"
      },
      "source": [
        "if USE_CUDA and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(\"device: \", device)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device:  cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp2MbAYJzn0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RNNLM(\n",
        "    vocab_size=len(vocab),\n",
        "    embeddings_dim=64,\n",
        "    hidden_size=64\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.03)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=Padder(dim=0, pad_symbol=vocab(\"PAD\"))\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    dev_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=Padder(dim=0, pad_symbol=vocab(\"PAD\"))\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwruQU71VV_F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "49373eac-59a4-4abf-bd38-cbe416f0f5f8"
      },
      "source": [
        "for x, y in train_dataloader:\n",
        "    print(x.shape)\n",
        "    print(y.shape)\n",
        "    print(validate_on_batch(model, sum_criterion, a.to(device), b.to(device)))\n",
        "    break"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 15])\n",
            "torch.Size([16, 15])\n",
            "tensor(43.7084, device='cuda:0', grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymVEYm7ezoVq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7620313c-ae68-4cfd-beb7-3851dc7f0bde"
      },
      "source": [
        "%%time\n",
        "\n",
        "for i in trange(LAST_EPOCH, TOTAL_EPOCH):\n",
        "    pass"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 340170.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4 ms, sys: 1.03 ms, total: 5.03 ms\n",
            "Wall time: 4.26 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEBiV4PF6gtE",
        "colab_type": "text"
      },
      "source": [
        "**2.6 (5 points)** Write a function **predict_on_batch** that outputs letter probabilities of all words in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPN4mopV6gtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyFZQDK56gtI",
        "colab_type": "text"
      },
      "source": [
        "**2.7 (1 points)** Calculate the letter probabilities for all words in the test dataset. Print them for 20 last words. Do not forget to disable shuffling in the ``DataLoader``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc8qjXL66gtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "== YOUR CODE HERE ==\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWkB4GRE6gtK",
        "colab_type": "text"
      },
      "source": [
        "**2.8 (5 points)** Write a function that generates a single word (sequence of indexes) given the model. Do not forget about the hidden state! Be careful about start and end symbol indexes. Use ``torch.multinomial`` for sampling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT3p4yme6gtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate(model, max_length=20, start_index=1, end_index=2):\n",
        "    \"\"\"\n",
        "    == YOUR CODE HERE ==\n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDnJ7GXl6gtN",
        "colab_type": "text"
      },
      "source": [
        "**2.9 (1 points)** Use ``generate`` to sample 20 pseudowords. Do not forget to transform indexes to letters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jxoIQ8K6gtN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(20):\n",
        "    \"\"\"\n",
        "    == YOUR CODE HERE ==\n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dYwtzuo6gtP",
        "colab_type": "text"
      },
      "source": [
        "**(2.10) 5 points** Write a batched version of the generation function. You should sample the following symbol only for the words that are not finished yet, so apply a boolean mask to trace active words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9825WPZd6gtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(model, batch_size, max_length = 20, start_index=1, end_index=2):\n",
        "    \"\"\"\n",
        "    == YOUR CODE HERE ==\n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A16PTqBk6gtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generated = []\n",
        "for _ in range(2):\n",
        "    generated += generate_batch(model, batch_size=10)\n",
        "\"\"\"\n",
        "== YOUR CODE HERE ==\n",
        "\"\"\"\n",
        "for elem in transformed:\n",
        "    print(\"\".join(elem))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dtpBFyI6gtU",
        "colab_type": "text"
      },
      "source": [
        "**(2.11) 5 points** Experiment with the type of RNN, number of layers, units and/or dropout to improve the perplexity of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl8bgJT76gtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}