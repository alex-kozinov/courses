{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Lab3_DL_parts_4_and_5_optional.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d7b58042156141e4a82ffabd611486a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_de79128426524e4c88348f405b4d98fe",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_10d99a3c2a754c909ecf1b29eaa4c438",
              "IPY_MODEL_0a1a5c5cd59d486c96912fc11305a5e4"
            ]
          }
        },
        "de79128426524e4c88348f405b4d98fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "10d99a3c2a754c909ecf1b29eaa4c438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ae2f81c8f8964146bb7cd4ea1491c256",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46827520,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46827520,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_41c4904b132a4312a19314c3ae7e9c84"
          }
        },
        "0a1a5c5cd59d486c96912fc11305a5e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4b2dd3adee2a4450a7907351ecec469d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 75.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e96d1e0767474e0cb5cf2b20eafa142e"
          }
        },
        "ae2f81c8f8964146bb7cd4ea1491c256": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "41c4904b132a4312a19314c3ae7e9c84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4b2dd3adee2a4450a7907351ecec469d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e96d1e0767474e0cb5cf2b20eafa142e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtPnYqncDGmX",
        "colab_type": "text"
      },
      "source": [
        "# Lab 3: final challenges\n",
        "\n",
        "__Вам предлагается решить задачу классификации сигналов (вы уже встречались с ней во второй лабораторной работе) или задачу классификации изображений. Или обе ;)__\n",
        "\n",
        "__Выполнение этих заданий не является обязательным, но позитивно повлияет на вашу итоговую оценку. Успехов!__\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYmMofXPFjlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "import natsort\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import math\n",
        "import pylab\n",
        "import warnings as w\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import utils\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "\n",
        "import torchsummary\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.rcParams.update({'font.size':14})\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7Vqo9UYEB2d",
        "colab_type": "code",
        "outputId": "1775bc21-ae2d-45d1-f2f0-b9167e4b8403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "def LOG(text):\n",
        "    print(text)\n",
        "    print('-'*70)\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    USE_COLAB = True\n",
        "except:\n",
        "    USE_COLAB = False\n",
        "\n",
        "if USE_COLAB:\n",
        "    LOG(\"\"\"Don't forget to avoid disconnections:\n",
        "function ClickConnect(){\n",
        "    console.log(\"Clicking\"); \n",
        "    document.querySelector(\"colab-connect-button\").click() \n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "    \"\"\")\n",
        "\n",
        "WORK_DIR = './'\n",
        "if USE_COLAB:\n",
        "    from google.colab import files, drive\n",
        "    WORK_DIR = '/content/drive/'\n",
        "    drive.mount(WORK_DIR)\n",
        "    WORK_DIR += 'My Drive/projects/lab-3-dogs/'\n",
        "LOG(f\"Working directory is {WORK_DIR}\")\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "LOG(f'Using device is {device}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Don't forget to avoid disconnections:\n",
            "function ClickConnect(){\n",
            "    console.log(\"Clicking\"); \n",
            "    document.querySelector(\"colab-connect-button\").click() \n",
            "}\n",
            "setInterval(ClickConnect,60000)\n",
            "    \n",
            "----------------------------------------------------------------------\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n",
            "Working directory is /content/drive/My Drive/projects/lab-3-dogs/\n",
            "----------------------------------------------------------------------\n",
            "Using device is cpu\n",
            "----------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJF5mqbfDGmY",
        "colab_type": "text"
      },
      "source": [
        "## Part 4. HAR classification with raw data (2+ points)\n",
        "__Disclaimer__: Это опциональная часть задания. Здесь придется экспериментировать, подбирать оптимальную структуру сети для решения задачи и активно искать подскзаки в сети.\n",
        "\n",
        "\n",
        "Данное задание составлено на основе данного [поста](https://burakhimmetoglu.com/2017/08/22/time-series-classification-with-tensorflow/). С помощью вручную сгенерированных фичей и классических подходов задача распознования движений была решена с точностью 96%. \n",
        "\n",
        "Также будет полезным изучить [вот этот](https://github.com/healthDataScience/deep-learning-HAR), а так же [вот этот репозиторий](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition), где к данной задаче рассматривается несколько подходов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgZN6u2PDGmh",
        "colab_type": "text"
      },
      "source": [
        "Вернемся к задаче классификации движений на основе [данных](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) из репозитория UCI ([прямая ссылка на скачивание](https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip)). \n",
        "\n",
        "В этот раз будем работать с исходными, а не предобработанными данными. Данные представляют собой сигналы с гироскопа и акселерометра, закрепленного на теле человека. Каждому семплу соотвествует 9 связанных временных рядов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9jfJsD5DGmh",
        "colab_type": "text"
      },
      "source": [
        "В начале приведена визуализация данных на основе PCA над вручную сгенерированными признаками. Для отрисовки графиков (цвет и легенда) нам также понадобятся метки классов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHGBUYo7DGmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_with_engineered_features = np.genfromtxt(os.path.join(\"UCI HAR Dataset\", \"train\", \"X_train.txt\"))\n",
        "y_train = np.genfromtxt(os.path.join(\"UCI HAR Dataset\", \"train\", \"y_train.txt\"))\n",
        "\n",
        "y_train_list = list(y_train)\n",
        "X_unique = np.array([X_train_with_engineered_features[y_train_list.index(l)]\n",
        "                             for l in sorted(list(set(y_train)))])\n",
        "\n",
        "legend_labels = [\"WALKING\", \"WALKING.UP\", \"WALKING.DOWN\", \"SITTING\", \"STANDING\", \"LAYING\"]\n",
        "colors_list = ['red', 'blue', 'green', 'orange', 'cyan', 'magenta']\n",
        "mapped_colors = [colors_list[int(i)-1] for i in y_train]\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "\n",
        "X_train_pca = pca.fit_transform(X_train_with_engineered_features)\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "pylab.scatter(X_train_pca[:, 0], X_train_pca[:, 1],\n",
        "             c=mapped_colors)\n",
        "plt.grid()\n",
        "for idx, x in enumerate(pca.transform(X_unique)):\n",
        "    plt.scatter(x[0], \n",
        "                x[1], \n",
        "                c=colors_list[idx], \n",
        "                label=legend_labels[idx])\n",
        "plt.xlabel('First principal component')\n",
        "plt.ylabel('Second principal component')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRrcX3Y1DGml",
        "colab_type": "text"
      },
      "source": [
        "#### Предобработка данных\n",
        "Предобработка сделана за нас автором [данного репозитория](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition). Будьте осторожны с путями."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75q-VgThDGmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Useful Constants\n",
        "\n",
        "# Those are separate normalised input features for the neural network\n",
        "INPUT_SIGNAL_TYPES = [\n",
        "    \"body_acc_x_\",\n",
        "    \"body_acc_y_\",\n",
        "    \"body_acc_z_\",\n",
        "    \"body_gyro_x_\",\n",
        "    \"body_gyro_y_\",\n",
        "    \"body_gyro_z_\",\n",
        "    \"total_acc_x_\",\n",
        "    \"total_acc_y_\",\n",
        "    \"total_acc_z_\"\n",
        "]\n",
        "\n",
        "# Output classes to learn how to classify\n",
        "LABELS = [\n",
        "    \"WALKING\", \n",
        "    \"WALKING_UPSTAIRS\", \n",
        "    \"WALKING_DOWNSTAIRS\", \n",
        "    \"SITTING\", \n",
        "    \"STANDING\", \n",
        "    \"LAYING\"\n",
        "]\n",
        "\n",
        "DATA_PATH = \"./\"\n",
        "\n",
        "DATASET_PATH = DATA_PATH + \"UCI HAR Dataset/\"\n",
        "print(\"\\n\" + \"Dataset is now located at: \" + DATASET_PATH)\n",
        "\n",
        "TRAIN = \"train/\"\n",
        "TEST = \"test/\"\n",
        "\n",
        "\n",
        "# Load \"X\" (the neural network's training and testing inputs)\n",
        "\n",
        "def load_X(X_signals_paths):\n",
        "    X_signals = []\n",
        "    \n",
        "    for signal_type_path in X_signals_paths:\n",
        "        file = open(signal_type_path, 'r')\n",
        "        # Read dataset from disk, dealing with text files' syntax\n",
        "        X_signals.append(\n",
        "            [np.array(serie, dtype=np.float32) for serie in [\n",
        "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "            ]]\n",
        "        )\n",
        "        file.close()\n",
        "    \n",
        "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
        "\n",
        "X_train_signals_paths = [\n",
        "    os.path.join(*[DATASET_PATH, TRAIN, \"Inertial Signals/\", signal+\"train.txt\"]) for signal in INPUT_SIGNAL_TYPES\n",
        "]\n",
        "X_test_signals_paths = [\n",
        "    os.path.join(*[DATASET_PATH, TEST, \"Inertial Signals/\", signal+\"test.txt\"]) for signal in INPUT_SIGNAL_TYPES\n",
        "]\n",
        "\n",
        "X_train = load_X(X_train_signals_paths)\n",
        "X_test = load_X(X_test_signals_paths)\n",
        "\n",
        "\n",
        "# Load \"y\" (the neural network's training and testing outputs)\n",
        "\n",
        "def load_y(y_path):\n",
        "    file = open(y_path, 'r')\n",
        "    # Read dataset from disk, dealing with text file's syntax\n",
        "    y_ = np.array(\n",
        "        [elem for elem in [\n",
        "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "        ]], \n",
        "        dtype=np.int32\n",
        "    )\n",
        "    file.close()\n",
        "    \n",
        "    # Substract 1 to each output class for friendly 0-based indexing \n",
        "    return y_ - 1\n",
        "\n",
        "y_train_path = os.path.join(DATASET_PATH, TRAIN, \"y_train.txt\")\n",
        "y_test_path = os.path.join(DATASET_PATH, TEST, \"y_test.txt\")\n",
        "\n",
        "y_train = load_y(y_train_path)\n",
        "y_test = load_y(y_test_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsOBX5iuDGmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input Data \n",
        "\n",
        "training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
        "test_data_count = len(X_test)  # 2947 testing series\n",
        "n_steps = len(X_train[0])  # 128 timesteps per series\n",
        "n_input = len(X_train[0][0])  # 9 input parameters per timestep\n",
        "\n",
        "\n",
        "# LSTM Neural Network's internal structure\n",
        "\n",
        "n_hidden = 32 # Hidden layer num of features\n",
        "n_classes = 6 # Total classes (should go up, or should go down)\n",
        "\n",
        "\n",
        "# Some debugging info\n",
        "\n",
        "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
        "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
        "print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
        "print(\"The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFs9fEe6DGmt",
        "colab_type": "text"
      },
      "source": [
        "#### Построение сети и эксперименты. (100% +)\n",
        "\n",
        "__Ваша задача - построить сеть, которая решит задачу классификации с точностью (`accuracy`) не менее 86%.__\n",
        "Разбалловка следующая:\n",
        "* $=$86% - 2 points\n",
        "* $>=$89% - 2.5 points\n",
        "* $>=$91% - 3 points\n",
        "\n",
        "\n",
        "__Warning!__ В сети существует несколько решений данной задачи с использованием различных фреймворков. При проверке это будет учитываться, так что свое решение нужно будет объяснить. Пожалуйста, не копируйте бездумно код, такие задания будут оценены 0 баллов. Если задача не решается - можете обратиться к заданию по классификации изображений."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfCUdcQUDGmt",
        "colab_type": "text"
      },
      "source": [
        "После выполнения задания заполните небольшой отчет об экспериментах вида \"Я пробовал(а) ... подходы и получил(а) ... результаты. Наконец, после N+1 чашки кофе/бессонной ночи у меня получилось, и весь секрет был в ...\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5VU6eZrDGmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your experiments here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vjHIwwXDGmx",
        "colab_type": "text"
      },
      "source": [
        "## Part 5. Dogs classification (2+ points)\n",
        "__Disclaimer__: Это опциональная часть задания. Здесь придется экспериментировать, подбирать оптимальную структуру сети для решения задачи и активно искать подскзаки в сети.\n",
        "\n",
        "Предлагаем вам решить задачу классификации пород собак. Вы можете обучить сеть с нуля или же воспользоваться методом fine-tuning'а. Полезная ссылка на [предобученные модели](https://pytorch.org/docs/stable/torchvision/models.html).\n",
        "\n",
        "Данные можно скачать [отсюда](https://www.dropbox.com/s/vgqpz2f1lolxmlv/data.zip?dl=0). Датасет представлен 50 классами пород собак, которые можно найти в папке train в соответствующих директориях. При сдаче данной части задания вместе с ноутбуком необходимо отправить .csv-файл с предсказаниями классов тестовой выборки в формате: <имя изображения>,<метка класса> по одному объекту на строку. Ниже приведите код ваших экспериментов и короткий вывод по их результатам.\n",
        "\n",
        "Будут оцениваться качество классификации (accuracy) на тестовой выборке (2 балла) и проведенные эксперименты (1 балл).\n",
        "Разбалловка следующая:\n",
        "* $>=$93% - 2 points\n",
        "* $>=$84% - 1.5 points\n",
        "* $>=$70% - 0.75 points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG5DPsjnG8c0",
        "colab_type": "text"
      },
      "source": [
        "### Загрузка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3T-uczeLcK0T",
        "colab_type": "text"
      },
      "source": [
        "#### Извлечём изображения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkDlZoouJeao",
        "colab_type": "code",
        "outputId": "5c911ea2-6bfd-4ad2-9b1f-430dd0f52290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "file_name = WORK_DIR + \"data.zip\"\n",
        "!unzip \"$file_name\" > /dev/null\n",
        "!ls data/\n",
        "data_path = 'data/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test  train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6cDVonycRCH",
        "colab_type": "text"
      },
      "source": [
        "#### Изучим размеры картинок"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnEX3ySZbtCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dir = os.path.join(data_path, 'train')\n",
        "test_dir = os.path.join(data_path, 'test')\n",
        "widths = []\n",
        "heights = []\n",
        "\n",
        "for target_folder in os.listdir(train_dir):\n",
        "    target_dir = os.path.join(train_dir, target_folder)\n",
        "    for image_name in os.listdir(target_dir):\n",
        "        image_path = os.path.join(target_dir, image_name)\n",
        "        image = Image.open(image_path)\n",
        "        widths.append(image.width)\n",
        "        heights.append(image.height)\n",
        "widths = np.array(widths)\n",
        "heights = np.array(heights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZvQE9qTcfxK",
        "colab_type": "code",
        "outputId": "c5227c65-83bb-4e34-eda8-c7a44e8dbb7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.figure()\n",
        "plt.scatter(widths, heights)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgbhzJq_dLPN",
        "colab_type": "code",
        "outputId": "abfb3fbf-e263-41be-85f8-6c2ff0d7104f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.figure()\n",
        "plt.hist(np.max(np.vstack([widths, heights]), axis=0) / np.min(np.vstack([widths, heights]), axis=0))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmbBM1oGG7KN",
        "colab_type": "text"
      },
      "source": [
        "#### **Выводы**\n",
        "\n",
        "Можно заметить, что исходные фотографии различных размеров, причём число тех, у которых высота больше ширины, примерно столько же как и тех, у которых ширина больше высоты. Поэтому наша будущая модель будет принимать квадратное изображение. Размер будет $224 \\times 224$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnHkWA2DsXoN",
        "colab_type": "text"
      },
      "source": [
        "#### Загрузим тестовые данные, а так же поделим их на train/validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9nYnGGwkFwV",
        "colab_type": "code",
        "outputId": "c2063025-83b6-4d93-fe1b-cb165e037786",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "full_train_dataset = torchvision.datasets.ImageFolder(\n",
        "    root=train_dir,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                              std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        ")\n",
        "\n",
        "n = len(full_train_dataset)\n",
        "train_size = int(n*0.8)\n",
        "validate_size = n - train_size\n",
        "train_dataset, validate_dataset = torch.utils.data.random_split(full_train_dataset, [train_size, validate_size])\n",
        "validate_dataset.dataset.transform = transforms.Compose([\n",
        "        transforms.Resize([224, 224]),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                              std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "print(f'Length of train dataset is {len(train_dataset)}, should be {train_size}')\n",
        "print(f'Length of validate dataset is {len(validate_dataset)}, should be {validate_size}')\n",
        "\n",
        "full_train_loader = torch.utils.data.DataLoader(\n",
        "    full_train_dataset,\n",
        "    batch_size=128,\n",
        "    num_workers=0,\n",
        "    shuffle=True\n",
        ")\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=128,\n",
        "    num_workers=0,\n",
        "    shuffle=True\n",
        ")\n",
        "validate_loader = torch.utils.data.DataLoader(\n",
        "    validate_dataset,\n",
        "    batch_size=128,\n",
        "    num_workers=0,\n",
        "    shuffle=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train dataset is 5732, should be 5732\n",
            "Length of validate dataset is 1434, should be 1434\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCe41WLiltKt",
        "colab_type": "text"
      },
      "source": [
        "#### Объявим test_dataset и test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbW34Ra6lxO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageSingleFolder(utils.data.Dataset):\n",
        "    def __init__(self, main_dir, transform):\n",
        "        self.main_dir = main_dir\n",
        "        self.transform = transform\n",
        "        all_imgs = os.listdir(main_dir)\n",
        "        self.total_imgs = natsort.natsorted(all_imgs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.total_imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n",
        "        image = Image.open(img_loc).convert(\"RGB\")\n",
        "        tensor_image = self.transform(image)\n",
        "        return tensor_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V36EHcgBu0p3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = ImageSingleFolder(\n",
        "    main_dir=test_dir,\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize([224, 224]),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                              std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    num_workers=0,\n",
        "    shuffle=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTrbe5tTzXnp",
        "colab_type": "text"
      },
      "source": [
        "### Определим нейронную сеть"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHQve_i79mSi",
        "colab_type": "text"
      },
      "source": [
        "#### Загрузка VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I3MVKti399A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vgg16net(num_classes):\n",
        "    net = models.vgg16(pretrained=True)\n",
        "    net.classifier = nn.Sequential(\n",
        "        nn.Linear(25088, 4096),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(p=0.5, inplace=False),\n",
        "        nn.Linear(4096, 4096),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(p=0.5, inplace=False),\n",
        "        nn.Linear(4096, num_classes),\n",
        "    )\n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb1l0Voe9sT7",
        "colab_type": "text"
      },
      "source": [
        "#### Загрузка ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g9-JSof9rnd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_resnet(num_classes):\n",
        "    net = models.resnet18(pretrained=True)\n",
        "    num_ftrs = net.fc.in_features\n",
        "\n",
        "    net.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV7zBPx29kSR",
        "colab_type": "text"
      },
      "source": [
        "#### Класс для обучения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMVHi2k--QJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LearningModel:\n",
        "    def __init__(self, net, net_name):\n",
        "        self.net = net\n",
        "        self.net_name = net_name\n",
        "        self.optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.epoch = 0\n",
        "        self.metrics = {'accuracy' : {}, 'loss' : {}}\n",
        "    \n",
        "    def predict(self, input):\n",
        "        return self.net.forward(input).argmax(axis=-1)\n",
        "    \n",
        "    def get_acc(self, img_batch, label_batch):\n",
        "        predictions = self.predict(img_batch)\n",
        "        return (predictions == label_batch).float().mean()\n",
        "\n",
        "    def get_loss(self, img_batch, label_batch):\n",
        "        predictions = self.net.forward(img_batch)\n",
        "        loss = self.criterion(predictions, label_batch)\n",
        "        return loss\n",
        "\n",
        "    def evaluate(self, data_loaders):\n",
        "        with torch.no_grad():\n",
        "            for name, data_loader in data_loaders.items():\n",
        "                metrics_sum = {'accuracy':0., 'loss':0.}\n",
        "                metrics_func = {'accuracy':self.get_acc, 'loss':self.get_loss}\n",
        "                total_num = 0.\n",
        "                for img_batch, label_batch in data_loader:\n",
        "                    batch_size = img_batch.shape[0]\n",
        "                    img_batch = img_batch.to(device)\n",
        "                    label_batch = label_batch.to(device)\n",
        "                    \n",
        "                    for k in metrics_sum.keys():\n",
        "                        metrics_sum[k] += metrics_func[k](img_batch, label_batch) * batch_size\n",
        "                    \n",
        "                    total_num += batch_size\n",
        "                \n",
        "                for k, v in metrics_sum.items():\n",
        "                    self.metrics[k].setdefault(name, [])\n",
        "                    self.metrics[k][name].append(v / total_num)\n",
        "\n",
        "\n",
        "    def fit(self,\n",
        "            data_loaders,\n",
        "            n_epoch=20,\n",
        "           ):\n",
        "        self.n_epoch = n_epoch\n",
        "\n",
        "        while self.epoch <= n_epoch:\n",
        "            display.clear_output(wait=True)\n",
        "            self.evaluate(data_loaders)\n",
        "            self.visualize()\n",
        "            if self.epoch == n_epoch:\n",
        "                break\n",
        "\n",
        "            for img_batch, label_batch in data_loaders['train']:\n",
        "                img_batch = img_batch.to(device)\n",
        "                label_batch = label_batch.to(device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                \n",
        "                loss = self.get_loss(img_batch, label_batch)\n",
        "            \n",
        "                # Backward\n",
        "                loss.backward()\n",
        "                \n",
        "                # Update weights\n",
        "                self.optimizer.step()\n",
        "            \n",
        "            self.epoch += 1\n",
        "\n",
        "        return self\n",
        "    \n",
        "    def visualize_metric(self, ax, metric_name):\n",
        "        ax.set_title(metric_name)\n",
        "        ax.set_xlabel('Epoch num')\n",
        "        ax.set_ylabel(metric_name)\n",
        "\n",
        "        for k, v in self.metrics[metric_name].items():\n",
        "            ax.plot(v, label=f'On {k} {metric_name}={v[-1]}')\n",
        "        \n",
        "        ax.legend(loc='best')\n",
        "\n",
        "    def visualize(self, independ_plot=False):\n",
        "        print('Epoch {}/{}'.format(self.epoch, self.n_epoch))\n",
        "\n",
        "        fig = plt.figure(figsize=(29.7/2, 21.))\n",
        "\n",
        "        loss_ax = fig.add_subplot(2, 1, 1)\n",
        "        acc_ax = fig.add_subplot(2, 1, 2)\n",
        "\n",
        "        self.visualize_metric(loss_ax, 'loss')\n",
        "        self.visualize_metric(acc_ax, 'accuracy')\n",
        "        \n",
        "        plt.legend(loc='best')\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ddTse3NK3QT",
        "colab_type": "text"
      },
      "source": [
        "### Эксперименты с обучение\n",
        "Для начала выясним при каком числе итерация начинается переобучение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZiSMc5yRYnZ",
        "colab_type": "text"
      },
      "source": [
        "#### Будем обучаться на 80% тренировочной выборке, а валидироваться на 20%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMLtGOqgK8qd",
        "colab_type": "code",
        "outputId": "ea94b66f-aa31-433a-b756-1958593777b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d7b58042156141e4a82ffabd611486a7",
            "de79128426524e4c88348f405b4d98fe",
            "10d99a3c2a754c909ecf1b29eaa4c438",
            "0a1a5c5cd59d486c96912fc11305a5e4",
            "ae2f81c8f8964146bb7cd4ea1491c256",
            "41c4904b132a4312a19314c3ae7e9c84",
            "4b2dd3adee2a4450a7907351ecec469d",
            "e96d1e0767474e0cb5cf2b20eafa142e"
          ]
        }
      },
      "source": [
        "resnet = get_resnet(50).to(device)\n",
        "torchsummary.summary(resnet, (3, 224, 224))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7b58042156141e4a82ffabd611486a7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
            "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
            "             ReLU-14           [-1, 64, 56, 56]               0\n",
            "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
            "             ReLU-17           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
            "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
            "             ReLU-21          [-1, 128, 28, 28]               0\n",
            "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
            "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
            "             ReLU-26          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
            "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
            "             ReLU-30          [-1, 128, 28, 28]               0\n",
            "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
            "             ReLU-33          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
            "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
            "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
            "             ReLU-37          [-1, 256, 14, 14]               0\n",
            "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
            "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
            "             ReLU-42          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
            "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
            "             ReLU-46          [-1, 256, 14, 14]               0\n",
            "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
            "             ReLU-49          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
            "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-53            [-1, 512, 7, 7]               0\n",
            "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
            "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-58            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
            "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-62            [-1, 512, 7, 7]               0\n",
            "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-65            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                   [-1, 50]          25,650\n",
            "================================================================\n",
            "Total params: 11,202,162\n",
            "Trainable params: 11,202,162\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 62.79\n",
            "Params size (MB): 42.73\n",
            "Estimated Total Size (MB): 106.09\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzViJG7eLEEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnet_lm = LearningModel(resnet, 'ResNet18')\n",
        "dataloaders = {\n",
        "    'train' : train_loader,\n",
        "    'validate' : validate_loader,\n",
        "    'full train' : full_train_loader\n",
        "}\n",
        "resnet_lm.fit(dataloaders, n_epoch=7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wGsO3aZNKmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}