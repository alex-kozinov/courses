{"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"language_info":{"name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py","version":"3.7.7","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3}},"notebookId":"6bb475f0-e440-4eaa-807f-09a2eb8cb938","pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}},"colab":{"name":"hw2-student.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":"# Homework 2: CTC Speech Recognition System\nYou can do this notebook in google collab, or in datasphere (if you are brave enougth)\n\n### Grades criteria\n\n```\n[ ] (10 points) Implement a Prefix Decoder\n[ ] (10 points) Train ASR System, WER criterions: 60-50 -- 3 points, 50-40 -- 5 points, 40-35 -- 7 points, <=35 -- 10 points. + Bonus point per 1% WER below 30\n[ ] (5 points) Compare performance of DNN, RNN and BiRNN models in terms of WER, training time and other properties\n[ ] (5 points) Compare alignments obtained from DNN, RNN and BiRNN models\n```\n\nThe results of this task are two artifacts:\n1. this Jupiter Notebook (`.ipynb`) with completed cells, training progress and final score.\n2. file with predictions of your best model for the test data\n\nSave the artifacts to a directory named `{your last name}_{your first name}_hw2` and pack them in `.zip` archive.\n","metadata":{"id":"MK6bCSji-oa1","cellId":"z5qfpgxviokgo9ewdahyxr"}},{"cell_type":"code","source":"#!L\n#pip install torch==1.8.0+cu101\n%pip install torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n%pip install https://github.com/kpu/kenlm/archive/master.zip\n%pip install dulwich\n    \n%enable_full_walk","metadata":{"id":"WWDmSBBn-07C","cellId":"na65u400yyes9ya34xsbvm"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Clone github repo","metadata":{"id":"H046bnEhkIHf","cellId":"4bv5j3c5cgbiqgbxltnrp"}},{"cell_type":"code","source":"#!L\n\nimport dulwich.client\nfrom dulwich.repo import Repo\nfrom dulwich import index\n\nimport os\nimport shutil\n\ndef git_clone(src, target):\n    client, path = dulwich.client.get_transport_and_path(src)\n    if os.path.isdir(target):\n        shutil.rmtree(target)\n    os.makedirs(target)\n    r = Repo.init(target)\n\n    remote_refs = client.fetch(src, r)\n    r[b\"HEAD\"] = remote_refs.refs[b\"HEAD\"]\n\n    index.build_index_from_tree(r.path, r.index_path(), r.object_store, r[b'HEAD'].tree)\n\nsrc = \"https://github.com/yandexdataschool/speech_course\"\ntarget = \"./speech_course\"\n\ngit_clone(src, target)\nos.listdir(target)\n\nweek_05_path = './speech_course/week_05' # Change this path, if it is different in your case","metadata":{"id":"pelDQHU-_DUy","cellId":"x21q44iq67quhpu1a3zem"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\nimport importlib\nimport collections\nimport os\nimport math\nimport numpy as np\nimport time\n\nfrom speech_course.week_05.utils import *\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport torchaudio\nfrom torch import optim\nimport kenlm\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LogNorm","metadata":{"id":"IOaR9CeT-oa7","cellId":"0jg5sfs7qowv79473xsi8eu"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\n# Download LibriSpeech 100hr training and test data\n\nif not os.path.isdir(\"./data\"):\n    os.makedirs(\"./data\")\n\ntrain_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"train-clean-100\", download=True)\ntest_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"test-clean\", download=True)","metadata":{"id":"Cq8Amv2QAKPU","cellId":"uj668v8qaihxclvkvheq6"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tokenizer Class","metadata":{"id":"c79P_B0b-oa8","cellId":"abgm2usdt24l6itvepmw8"}},{"cell_type":"code","source":"#!L\n# Class to transform text to strings of token indecies\nclass Tokenizer:\n    \"\"\"Maps characters to integers and vice versa\"\"\"\n    def __init__(self):\n        char_map_str = \"\"\"\n        ' 0\n        _ 1\n        a 2\n        b 3\n        c 4\n        d 5\n        e 6\n        f 7\n        g 8\n        h 9\n        i 10\n        j 11\n        k 12\n        l 13\n        m 14\n        n 15\n        o 16\n        p 17\n        q 18\n        r 19\n        s 20\n        t 21\n        u 22\n        v 23\n        w 24\n        x 25\n        y 26\n        z 27\n        \"\"\"\n        self.char_map = {}\n        self.index_map = {}\n        for line in char_map_str.strip().split('\\n'):\n            ch, index = line.split()\n            self.char_map[ch] = int(index)\n            self.index_map[int(index)] = ch\n        self.index_map[1] = ' '\n\n    def text_to_indecies(self, text):\n        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n        int_sequence = []\n        for c in text:\n            if c == ' ':\n                ch = self.char_map['_']\n            else:\n                ch = self.char_map[c]\n            int_sequence.append(ch)\n        return int_sequence\n\n    def indecies_to_text(self, labels):\n        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n        string = []\n        for i in labels:\n            string.append(self.index_map[i])\n        return ''.join(string).replace('_', ' ')\ntokenizer = Tokenizer()","metadata":{"id":"_kzOaLiV-oa8","cellId":"jarxy3gwdllxcqufdtz4f"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\n\nINSERT GREEDY DECODER CODE","metadata":{"id":"Jwc1NZkU-obC","cellId":"jzbg8qm8vcrinevp9y0dok"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\n# TESTING THE GREEDY DECODER \n\n#Load numpy matrix, add axis [batch,classes,time]\nmatrix = np.loadtxt(os.path.join(week_05_path, 'test_matrix.txt'))[np.newaxis,:,:]\n\n# Turn into Torch Tensor of shape [batch, time, classes]\nmatrix = torch.Tensor(matrix).transpose(1,2)\n\n# Create list of torch tensor\nlabels_indecies = [torch.Tensor(tokenizer.text_to_indecies('there seems no good reason for believing that it will change'))]\n\n# Run the Decoder\ndecodes, targets = GreedyDecoder(matrix, labels_indecies, [len(labels_indecies[0])])\n\nassert decodes[0] == 'there se ms no good reason for believing that twillc ange'\nassert targets[0] == 'there seems no good reason for believing that it will change'\n\n","metadata":{"id":"m4LozjfI-obD","cellId":"se7g7cudhf1r43b40icy9"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Implement Prefix Decoding With LM (10 points)","metadata":{"cellId":"5usp9pexqf2a1g8vxae8ic"}},{"cell_type":"code","source":"SKELETON IN PROGRESS","metadata":{"cellId":"l2sqrnp6ckupa3ejnjv4","trusted":true},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-1-3ef1120ddf79>, line 1)","traceback":["\u001B[0;36m  File \u001B[0;32m\"<ipython-input-1-3ef1120ddf79>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    SKELETON IN PROGRESS\u001B[0m\n\u001B[0m              ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"]}],"execution_count":1},{"cell_type":"markdown","source":"## Deep Learning part","metadata":{"id":"sr7uip8slCcw","cellId":"zfs3th23sxnnatpxbacw4"}},{"cell_type":"markdown","source":"## Create a Dataloader","metadata":{"id":"sFouWJvO-obE","cellId":"gvc1msqg6viwm3iwt79xy"}},{"cell_type":"code","source":"#!L\n# For train you can use SpecAugment data aug here.\ntrain_audio_transforms = nn.Sequential(\n    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=<YOUR CODE HERE>),\n    ADD YOUR AUGMENTS HERE\n)\n\ntest_audio_transforms = torchaudio.transforms.MelSpectrogram()\n\ntokenizer = Tokenizer()\n\n\n\n\ndef data_processing(data, data_type=\"train\"):\n    spectrograms = []\n    labels = []\n    input_lengths = []\n    label_lengths = []\n    for (waveform, _, utterance, _, _, _) in data:\n        if data_type == 'train':\n            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        elif data_type == 'test':\n            spec = test_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        else:\n            raise Exception('data_type should be train or valid')\n        spectrograms.append(spec)\n        label = torch.Tensor(tokenizer.text_to_indecies(utterance.lower()))\n        labels.append(label)\n        input_lengths.append(spec.shape[0] // 2)\n        label_lengths.append(len(label))\n\n    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n\n    return spectrograms, labels, input_lengths, label_lengths","metadata":{"id":"6FfDdryM-obF","cellId":"4i95bg5u0c4pj26vvkbxyc"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Implement a Neural Network Model\n\nYou should try out a few different model types:\n- Feed-Forward Model (DNN)\n- Recurrent Model (GRU or LSTM)\n- Bidirectional Recurrent Model (bi-GRU or bi-LSTM)\n- Something different for bonus points\n\nBefore any of this models you can use convolutional layers, as shown in the example below\n\nAfter your experiments you should write a report with comparison of different models in terms of different features, for example: parameters, training speed, resulting quality, spectrogram properties, and data augmentations. Remember, that for full mark you need to achive good WER \n\nWER criterions: 60-50 -- 3 points, 50-40 -- 5 points, 40-35 -- 7 points, <= 35 -- 10 points","metadata":{"id":"RqZcwSweECPH","cellId":"jcex1p3ibgiuabzxa0yw"}},{"cell_type":"markdown","source":"### Our model classes are just examples, you can change them as you want","metadata":{"id":"dPwmQyJP5qOe","cellId":"554tm5fog66g78v4s3at8v"}},{"cell_type":"code","source":"#!L\n# Define model\nclass CNNLayerNorm(nn.Module):\n    \"\"\"Layer normalization built for CNNs input\"\"\"\n\n    def __init__(self, n_feats):\n        super(CNNLayerNorm, self).__init__()\n        self.layer_norm = nn.LayerNorm(n_feats)\n\n    def forward(self, x):\n        # x (batch, channel, feature, time)\n        x = x.transpose(2, 3).contiguous()  # (batch, channel, time, feature)\n        x = self.layer_norm(x)\n        return x.transpose(2, 3).contiguous()  # (batch, channel, feature, time)\n\n\nclass ResidualCNN(nn.Module):\n    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n        except with layer norm instead of batch norm\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n        super(ResidualCNN, self).__init__()\n\n        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel // 2)\n        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel // 2)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.layer_norm1 = CNNLayerNorm(n_feats)\n        self.layer_norm2 = CNNLayerNorm(n_feats)\n\n    def forward(self, x):\n        residual = x  # (batch, channel, feature, time)\n        <YOUR CODE HERE>\n        x += residual\n        return x  # (batch, channel, feature, time)\n\n\nclass SpeechRecognitionModel(nn.Module):\n\n    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n        super(SpeechRecognitionModel, self).__init__()\n        n_feats = n_feats // 2\n        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3 // 2)  # cnn for extracting heirachal features\n\n        # n residual cnn layers with filter size of 32\n        self.rescnn_layers = nn.Sequential(*[\n            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats)\n            for _ in range(n_cnn_layers)\n        ])\n        self.fully_connected = <YOUR CODE HERE>\n        self.birnn_layers = <YOUR CODE HERE>\n        self.classifier = <YOUR CODE HERE>\n\n    def forward(self, x):\n        x = self.cnn(x)\n        x = self.rescnn_layers(x)\n        sizes = x.size()\n        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n        x = x.transpose(1, 2)  # (batch, time, feature)\n        x = self.fully_connected(x)\n        x = self.birnn_layers(x)\n        x = self.classifier(x)\n        return x","metadata":{"id":"4icyZP5JD9nW","cellId":"b3ddofit6i1f8yjroo07e"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training and Evaluation Code","metadata":{"id":"T27qdd0iEH9q","cellId":"obxpjwwsdlio3sfm7jgtg"}},{"cell_type":"code","source":"#!L\nfrom tqdm import tqdm_notebook","metadata":{"id":"OwZ3lK3DD-LE","cellId":"x03325d7w5ou3qsh5azgrl"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\ndef train(model, device, train_loader, criterion, optimizer, scheduler, epoch):\n    model.train()\n    data_len = len(train_loader.dataset)\n    for batch_idx, _data in enumerate(train_loader):\n        spectrograms, labels, input_lengths, label_lengths = _data\n        spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(spectrograms)  # (batch, time, n_class)\n        output = F.log_softmax(output, dim=2)\n        output = output.transpose(0, 1)  # (time, batch, n_class)\n\n        loss = criterion(output, labels, input_lengths, label_lengths)\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        if batch_idx % 100 == 0 or batch_idx == data_len:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(spectrograms), data_len,\n                       100. * batch_idx / len(train_loader), loss.item()))\n\n\ndef test(model, device, test_loader, criterion, epoch, decode='Greedy', lm=None):\n    print('Beginning eval...')\n    model.eval()\n    test_loss = 0\n    test_cer, test_wer = [], []\n    with torch.no_grad():\n        start = time.time()\n        for i, _data in enumerate(test_loader):\n            spectrograms, labels, input_lengths, label_lengths = _data\n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n            \n            matrix = model(spectrograms)  # (batch, time, n_class)\n            matrix = F.log_softmax(matrix, dim=2)\n            probs = F.softmax(matrix,dim=2)\n            matrix = matrix.transpose(0, 1)  # (time, batch, n_class)\n                \n            loss = criterion(matrix, labels, input_lengths, label_lengths)\n            test_loss += loss.item() / len(test_loader)\n\n            if decode == 'Greedy':\n                decoded_preds, decoded_targets = GreedyDecoder(matrix.transpose(0, 1), labels, label_lengths)\n            elif decode == 'BeamSearch':\n                ## THIS IS A CLASS YOU SHOULD IMPLEMENT\n                decoded_preds, decoded_targets = BeamSearchDecoder(probs, labels, label_lengths, input_lengths, lm=lm)\n            for j in range(len(decoded_preds)):\n                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n\n    avg_cer = sum(test_cer) / len(test_cer)\n    avg_wer = sum(test_wer) / len(test_wer)\n\n    print(\n        'Epoch: {:d}, Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(epoch, test_loss,\n                                                                                                       avg_cer,\n                                                                                                       avg_wer))","metadata":{"id":"RAA05g-oD-N_","cellId":"hws8dpmno7ymiam3dnafp"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\n#pragma async \n# PRAGMA ASYNC IS NECESSARY FOR TRAINING!\ntorch.manual_seed(7)\nif torch.cuda.is_available():\n    print('GPU found! 🎉')\n    device = 'cuda'\nelse:\n    print('Only CPU found! 💻')\n    device = 'cpu'\n\nverbose=False\n\n# Hyperparameters for your model\nhparams = {\n    \"n_cnn_layers\": 3,\n    \"n_rnn_layers\": <YOUR CODE HERE>,\n    \"rnn_dim\": <YOUR CODE HERE>\n    \"n_class\": 29,\n    \"n_feats\": <YOUR CODE HERE>,\n    \"stride\": 2,\n    \"dropout\": 0.1,\n    \"learning_rate\":  5e-4,\n    \"batch_size\":  10,\n    \"epochs\": 20\n}\n\n# Define Dataloyour training and test data loaders\nkwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n train_loader = <YOUR_CODE>\n test_loader = <YOUR_CODE>\n\n# Define ASR Model \nmodel = SpeechRecognitionModel(\n    hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n    hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n).to(device)\n\nmodel.to(device)\n\nif verbose:\n    print(model)\nprint('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n\n#Define optimizineer, criterion, scheduler\noptimizer = <YOUR CODE>\ncriterion = <YOUR CODE>\nscheduler = <YOUR CODE> - I suggest OneCycleLR for speed.\n\n\n#iter_meter = IterMeter()\nstart = time.time()\nprint(\"Start training...\")\nfor epoch in range(1, hparams['epochs'] + 1):\n    ep_start = time.time()\n    train(model, device, train_loader, criterion, optimizer, scheduler, epoch)\n    #if epoch % 2 == 0:\n    save_checkpoint(model, checkpoint_name=f'model_epoch{epoch}.tar')\n    load_checkpoint(model, checkpoint_name=f'model_epoch{epoch}.tar', path='./', device=device)\n    test(model, device, test_loader, criterion, epoch)\n    print(f\"Time for epoch: {round(time.time() - ep_start, 0)} sec.\")\nsave_checkpoint(model, checkpoint_name=f'model.tar')\nduration = time.time() - start\nprint(f'Training took {np.round(duration / 60.0, 1)} min.')","metadata":{"id":"ZlAaMjh4D-RD","cellId":"7ezpcofubg6gfyyde5c68"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\n# Test the model in Prefix Decode Mode - only do after you have implemented your prefix decoder\n\ntest(model, device, test_loader, criterion, epoch, decode='BeamSearch', lm=None)\n\nlm = kenlm.Model('3-gram.pruned.1e-7.arpa')\ntest(model, device, test_loader, criterion, epoch, decode='BeamSearch', lm=lm)","metadata":{"cellId":"2oj2t9pb6s508m4bomxqr"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Compare different models: DNN, GRU/LSTM, bi-GRU/bi-LSTM (5 points)","metadata":{"id":"VGS9rrTK29fx","cellId":"90vk5p1hyhmqhjlztpswx8"}},{"cell_type":"markdown","source":"## Analyze CTC Alignments (5 points)","metadata":{"id":"OvSNEbMngpZU","cellId":"7l42i8cl7mch5fzh42uv8b"}},{"cell_type":"markdown","source":"## In this section you should compare alignments obtained from different models.\n\nFor example, you can show:\n\n* Examples of alignments and their analysis\n* Differencies in the properties of alignment distributions over the dataset\n* Dynamic of alignments during training (from checkpoints)\n* Connection between alignments and model loss\n","metadata":{"id":"TtcI8Q65xXnJ","cellId":"om7mmrazgrojbft3qos2o"}},{"cell_type":"code","source":"#!L\nADD YOUR CODE FOR CTC FORWARD BACKWARD FROM SEMINAR","metadata":{"id":"hd2cGenN-oa_","cellId":"hz15vjonrhcd0goeoye94"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\n# Test your implementation of CTC\n#Load numpy matrix, add axis [classes,time]\nmatrix = np.loadtxt(os.path.join(week_05_path, 'test_matrix.txt'))\n# Create label_sequence\ntokenizer = Tokenizer()\nlabels_indecies = tokenizer.text_to_indecies('there se ms no good reason for believing that twillc ange')\n\nalign = soft_alignment(labels_indecies, matrix)\n\nref_align = np.loadtxt(os.path.join(week_05_path, 'soft_alignment.txt'))\n\nassert np.all(ref_align == align)","metadata":{"id":"lyO-qCwG-obA","cellId":"j0hk1mmq56mit6ldayht"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Example\nmodel.eval()\n_data = next(iter(test_loader))\nspectrograms, labels, input_lengths, label_lengths = _data\nspectrograms, labels = spectrograms.to(device), labels.to(device)\n\nmatrix = model(spectrograms).transpose(1,2)  # (batch, n_class, time)","metadata":{"id":"QAHLTOQGrgew","cellId":"vrt1ufy7fcefeck5i8hfu"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example of alignment calculation:\nwith torch.no_grad():\n  align = soft_alignment(labels[0].int().cpu().numpy(), F.softmax(matrix[0],dim=0).cpu().numpy())","metadata":{"id":"fL5dwHiGtL1G","cellId":"b80yoozigvm36ra3tisfbf"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(dpi=150)\nplt.imshow(align, aspect='auto', interpolation='nearest')\nplt.colorbar()\n\nplt.figure(dpi=150)\nplt.imshow(np.log(align), aspect='auto', interpolation='nearest')\nplt.colorbar()","metadata":{"id":"DqvHAfyWs7DR","cellId":"t6dtc9k91s90ual4nmzci49"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Conclusions 🧑‍🎓\n\n* What challenges did you encounter while completing this task?\n* What skills have you acquired while doing this task?\n* How difficult did you find this task (on a scale from 0 to 10), and why?\n* What did you like in this homework, and what didn't?","metadata":{"id":"muP0QOp_gvdp","cellId":"fgri9tl5qocpkrzz76s53h"}},{"cell_type":"code","source":"","metadata":{"id":"0LOcRoW3D-Wu","cellId":"iz6t4tmtlns28i9nn6t64h"},"outputs":[],"execution_count":null}]}