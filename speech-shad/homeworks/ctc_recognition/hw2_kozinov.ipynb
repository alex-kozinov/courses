{"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"language_info":{"name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py","version":"3.7.7","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3}},"notebookId":"45ae85d9-298a-41af-a722-94655056b34f","pycharm":{"stem_cell":{"cell_type":"raw","source":[],"metadata":{"collapsed":false}}},"colab":{"name":"hw2-student.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":"# Homework 2: CTC Speech Recognition System\nYou can do this notebook in google collab, or in datasphere (if you are brave enougth)\n\n### Grades criteria\n\n```\n[ ] (10 points) Implement a Prefix Decoder\n[ ] (10 points) Train ASR System, WER criterions: 60-50 -- 3 points, 50-40 -- 5 points, 40-35 -- 7 points, <=35 -- 10 points. + Bonus point per 1% WER below 30\n[ ] (5 points) Compare performance of DNN, RNN and BiRNN models in terms of WER, training time and other properties\n[ ] (5 points) Compare alignments obtained from DNN, RNN and BiRNN models\n```\n\nThe results of this task are two artifacts:\n1. this Jupiter Notebook (`.ipynb`) with completed cells, training progress and final score.\n2. file with predictions of your best model for the test data\n\nSave the artifacts to a directory named `{your last name}_{your first name}_hw2` and pack them in `.zip` archive.\n","metadata":{"id":"MK6bCSji-oa1","cellId":"z5qfpgxviokgo9ewdahyxr"}},{"cell_type":"code","source":"#!L\n#pip install torch==1.8.0+cu101\n%pip install torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n%pip install https://github.com/kpu/kenlm/archive/master.zip\n%pip install dulwich\n%pip install --user -U arpa\n    \n%enable_full_walk","metadata":{"id":"WWDmSBBn-07C","cellId":"na65u400yyes9ya34xsbvm","trusted":true},"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nLooking in links: https://download.pytorch.org/whl/torch_stable.html\nRequirement already satisfied: torchaudio==0.8.0 in /home/jupyter/.local/lib/python3.7/site-packages (0.8.0)\nRequirement already satisfied: torch==1.8.0 in /home/jupyter/.local/lib/python3.7/site-packages (from torchaudio==0.8.0) (1.8.0+rocm4.0.1)\nRequirement already satisfied: numpy in /home/jupyter/.local/lib/python3.7/site-packages (from torch==1.8.0->torchaudio==0.8.0) (1.17.5)\nRequirement already satisfied: typing-extensions in /kernel/lib/python3.7/site-packages (from torch==1.8.0->torchaudio==0.8.0) (3.7.4.3)\nDefaulting to user installation because normal site-packages is not writeable\nCollecting https://github.com/kpu/kenlm/archive/master.zip\n  Using cached https://github.com/kpu/kenlm/archive/master.zip\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: dulwich in /home/jupyter/.local/lib/python3.7/site-packages (0.20.21)\nRequirement already satisfied: urllib3>=1.24.1 in /kernel/lib/python3.7/site-packages (from dulwich) (1.26.4)\nRequirement already satisfied: certifi in /kernel/lib/python3.7/site-packages (from dulwich) (2020.12.5)\nCollecting arpa\n  Downloading arpa-0.1.0b4-py3-none-any.whl (9.6 kB)\nInstalling collected packages: arpa\nSuccessfully installed arpa-0.1.0b4\n"}],"execution_count":13},{"cell_type":"markdown","source":"## Clone github repo","metadata":{"id":"H046bnEhkIHf","cellId":"4bv5j3c5cgbiqgbxltnrp"}},{"cell_type":"code","source":"#!L\n\nimport dulwich.client\nfrom dulwich.repo import Repo\nfrom dulwich import index\n\nimport os\nimport shutil\n\ndef git_clone(src, target):\n    client, path = dulwich.client.get_transport_and_path(src)\n    if os.path.isdir(target):\n        shutil.rmtree(target)\n    os.makedirs(target)\n    r = Repo.init(target)\n\n    remote_refs = client.fetch(src, r)\n    r[b\"HEAD\"] = remote_refs.refs[b\"HEAD\"]\n\n    index.build_index_from_tree(r.path, r.index_path(), r.object_store, r[b'HEAD'].tree)\n\nsrc = \"https://github.com/yandexdataschool/speech_course\"\ntarget = \"./speech_course\"\n\ngit_clone(src, target)\nos.listdir(target)\n\nweek_05_path = './speech_course/week_05' # Change this path, if it is different in your case","metadata":{"id":"pelDQHU-_DUy","cellId":"x21q44iq67quhpu1a3zem","trusted":true},"outputs":[{"output_type":"error","ename":"Execute error","evalue":"Failed to allocate servant L: internal error. Please try again","traceback":[]}],"execution_count":27},{"cell_type":"code","source":"#!L\nimport importlib\nimport collections\nimport os\nimport math\nimport numpy as np\nimport time\n\nfrom speech_course.week_05.utils import *\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport torchaudio\nfrom torch import optim\nimport arpa\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LogNorm","metadata":{"id":"IOaR9CeT-oa7","cellId":"0jg5sfs7qowv79473xsi8eu","trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"#!L\n# Download LibriSpeech 100hr training and test data\n\nif not os.path.isdir(\"./data\"):\n    os.makedirs(\"./data\")\n\ntrain_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"train-clean-100\", download=True)\ntest_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"test-clean\", download=True)","metadata":{"id":"Cq8Amv2QAKPU","cellId":"uj668v8qaihxclvkvheq6","trusted":true},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Tokenizer Class","metadata":{"id":"c79P_B0b-oa8","cellId":"abgm2usdt24l6itvepmw8"}},{"cell_type":"code","source":"#!L\n# Class to transform text to strings of token indecies\nclass Tokenizer:\n    \"\"\"Maps characters to integers and vice versa\"\"\"\n    def __init__(self):\n        char_map_str = \"\"\"\n        ' 0\n        _ 1\n        a 2\n        b 3\n        c 4\n        d 5\n        e 6\n        f 7\n        g 8\n        h 9\n        i 10\n        j 11\n        k 12\n        l 13\n        m 14\n        n 15\n        o 16\n        p 17\n        q 18\n        r 19\n        s 20\n        t 21\n        u 22\n        v 23\n        w 24\n        x 25\n        y 26\n        z 27\n        \"\"\"\n        self.char_map = {}\n        self.index_map = {}\n        for line in char_map_str.strip().split('\\n'):\n            ch, index = line.split()\n            self.char_map[ch] = int(index)\n            self.index_map[int(index)] = ch\n        self.index_map[1] = ' '\n\n    def text_to_indecies(self, text):\n        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n        int_sequence = []\n        for c in text:\n            if c == ' ':\n                ch = self.char_map['_']\n            else:\n                ch = self.char_map[c]\n            int_sequence.append(ch)\n        return int_sequence\n\n    def indecies_to_text(self, labels):\n        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n        string = []\n        for i in labels:\n            string.append(self.index_map[i])\n        return ''.join(string).replace('_', ' ')\ntokenizer = Tokenizer()","metadata":{"id":"_kzOaLiV-oa8","cellId":"jarxy3gwdllxcqufdtz4f","trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"#!L\ndef GreedyDecoder(output: torch.Tensor, labels: list, label_lengths: list, blank_label=28, collapse_repeated=True):\n    \"\"\"ist\n    :param output: torch.Tensor of Probs or Log-Probs of shape [batch, time, classes]\n    :param labels: Listof torch.Tensor of label indecies\n    :param label_lengths:  int of label lengths\n    :param blank_label:\n    :param collapse_repeated:\n    :return: \n    \"\"\"\n\n    # Get max class\n    arg_maxes = torch.argmax(output, dim=2)\n\n    decodes = []\n    targets = []\n\n    #For targets and decodes remove r\n    for i, args in enumerate(arg_maxes):\n        decode = []\n\n        targets.append(tokenizer.indecies_to_text(labels[i][:label_lengths[i]].tolist()))\n\n        # Remove repeeats, then remove blanks\n\n        for j, index in enumerate(args):\n            if index != blank_label:\n                if collapse_repeated and j != 0 and index == args[j - 1]:\n                    continue\n                decode.append(index.item())\n        decodes.append(tokenizer.indecies_to_text(decode))\n    return decodes, targets","metadata":{"id":"Jwc1NZkU-obC","cellId":"jzbg8qm8vcrinevp9y0dok","trusted":true},"outputs":[{"output_type":"error","ename":"Execute error","evalue":"Failed to allocate servant L: internal error. Please try again","traceback":[]}],"execution_count":26},{"cell_type":"code","source":"#!L\n# TESTING THE GREEDY DECODER \n\n#Load numpy matrix, add axis [batch,classes,time]\nmatrix = np.loadtxt(os.path.join(week_05_path, 'test_matrix.txt'))[np.newaxis,:,:]\n\n# Turn into Torch Tensor of shape [batch, time, classes]\nmatrix = torch.Tensor(matrix).transpose(1,2)\n\n# Create list of torch tensor\nlabels_indecies = [torch.Tensor(tokenizer.text_to_indecies('there seems no good reason for believing that it will change'))]\n\n# Run the Decoder\ndecodes, targets = GreedyDecoder(matrix, labels_indecies, [len(labels_indecies[0])])\n\nassert decodes[0] == 'there se ms no good reason for believing that twillc ange'\nassert targets[0] == 'there seems no good reason for believing that it will change'\n\n","metadata":{"id":"m4LozjfI-obD","cellId":"se7g7cudhf1r43b40icy9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Implement Prefix Decoding With LM (10 points)","metadata":{"cellId":"5usp9pexqf2a1g8vxae8ic"}},{"cell_type":"code","source":"#!L\nNEG_INF = -float(\"inf\")\n\n\ndef make_new_beam():\n    fn = lambda: (NEG_INF, NEG_INF)\n    return collections.defaultdict(fn)\n\n\ndef logsumexp(*args):\n    \"\"\"\n    Stable log sum exp.\n    \"\"\"\n    if all(a == NEG_INF for a in args):\n        return NEG_INF\n    a_max = max(args)\n    lsp = np.log(sum(np.exp(a - a_max)\n                       for a in args))\n    return a_max + lsp\n\ndef _BeamSearchDecoder(probs, lm, alpha=0.1, beta=2, beam_size=5, blank=28, space=1, prune=1e-5):\n    \"\"\"\n    Performs inference for the given output probabilities.\n    Arguments:\n        probs: The output probabilities (e.g. post-softmax) for each\n          time step. Should be an array of shape (time x output dim).\n        beam_size (int): Size of the beam to use during inference.\n        blank (int): Index of the CTC blank label.\n    Returns the output label sequence and the corresponding negative\n    log-likelihood estimated by the decoder.\n    \"\"\"\n    T, S = probs.shape\n    probs = np.log(probs)\n    if prune == 0.0:\n        prune = NEG_INF\n    else:\n        prune = np.log(prune)\n    # Elements in the beam are (prefix, (p_blank, p_no_blank))\n    # Initialize the beam with the empty sequence, a probability of\n    # 1 for ending in blank and zero for ending in non-blank\n    # (in log space).\n    beam = [(tuple(), (0.0, NEG_INF))]\n\n    for t in range(T):  # Loop over time\n\n        # A default dictionary to store the next step candidates.\n        next_beam = make_new_beam()\n        prev_beam = None\n\n        # The variables p_b and p_nb are respectively the\n        # probabilities for the prefix given that it ends in a\n        # blank and does not end in a blank at this time step.\n        prev_prefixes = [i[0] for i in beam]\n        for prefix, (p_b, p_nb) in beam:  # Loop over beam\n            for s in range(S):  # Loop over vocab\n                p = probs[t, s]\n                if p > prune: # Prune the vocab\n                    \n                    # If we propose a blank the prefix doesn't change.\n                    # Only the probability of ending in blank gets updated.\n                    if s == blank:\n                        \n                        # Yield next P_blank and P_Not_blank\n                        next_p_blank = p + logsumexp(p_b, p_nb)\n                        next_p_not_blank = NEG_INF\n                        \n                        # Update next P_blank. DO NOT update next P_not_blank\n                        current_prefix_probas = next_beam[prefix]\n                        next_beam[prefix] = (\n                            logsumexp(current_prefix_probas[0], next_p_blank),\n                            logsumexp(current_prefix_probas[1], next_p_not_blank)\n                        )\n                        continue\n\n                        \n                    # Extend the prefix by the new character s and add it to\n                    # the beam. Only the probability of not ending in blank\n                    # gets updated.\n                    n_p_b = NEG_INF\n                    n_p_nb = p + p_b\n                    n_prefix = prefix + (s, )\n                    current_prefix_probas = next_beam[n_prefix]\n                    next_beam[n_prefix] = (\n                        logsumexp(current_prefix_probas[0], n_p_b),\n                        logsumexp(current_prefix_probas[1], n_p_nb)\n                    )\n                    end_t = prefix[-1] if len(prefix) else None\n\n                    # Yield next P_blank and P_Not_blank - this is either empty, obtained form existing prefix\n                    n_p_b = NEG_INF\n                    n_p_nb = NEG_INF \n\n                    #Situation of repeating last NON-BLANK character\n                    if s == end_t:\n                        # We don't include the previous probability of not ending\n                        # in blank (p_nb) if s is repeated at the end. The CTC\n                        # algorithm merges characters not separated by a blank.\n                        n_prefix = prefix\n                        \n                        #we also update the unchanged prefix. This is the merging case.\n                        n_p_nb = p + p_nb\n                        \n                    # If this is a NEW space (repeat space covered in previous situation)\n                    elif s == space and lm is not None:\n                        # Convert new prefix indecies to upper-case string (LM uses upper case!)\n                        # Remove trailing space\n                        l=tokenizer.int_to_text(n_prefix).upper().strip()\n                        \n                        if len(l.replace(' ', '')) > 0:\n                            \n                            # Get LM probability\n                            lm_prob = lm.log_p(l.strip())\n\n                            # Convert to natural log, as ARPA LM uses log10\n                            lm_prob = lm_prob/np.log10(np.e)\n\n                            # Compute next non-blank prob with LM score added\n                            # <YOUR_CODE>\n                            pass\n                        else:\n                            pass\n                            # <YOUR_CODE>\n\n                    # If this is a non-repeat, non-space character\n                    else:\n                        n_p_nb = p + p_nb\n                        \n    \n                    # Make use of discarded prefixes\n                    if n_prefix not in prev_prefixes and t > 0:\n                        prev_prefix_probas = prev_beam[n_prefix]\n                        n_p_b = logsumexp(prev_prefix_probas[0], n_p_b)\n                        n_p_nb = logsumexp(prev_prefix_probas[1], n_p_nb)\n                        \n                    current_prefix_probas = next_beam[n_prefix]\n                    n_p_b = logsumexp(current_prefix_probas[0], n_p_b)\n                    n_p_nb = logsumexp(current_prefix_probas[1], n_p_nb)\n                    next_beam[n_prefix] = (n_p_b, n_p_nb)\n    \n                        \n        # Sort and trim the beam before moving on to the next time-step.\n        if lm is None:\n            beam = sorted(next_beam.items(),\n                  key=lambda x: logsumexp(*x[1]),\n                  reverse=True)\n        else:\n            beam = sorted(next_beam.items(),\n                          # Need to add length bonus here...\n                          key=lambda x: logsumexp(*x[1])+ 0, # <YOUR_CODE>),\n                          reverse=True)\n        beam = beam[:beam_size]\n        prev_beam = next_beam\n\n    best = beam[0]\n    return best[0], -logsumexp(*best[1])\n\ndef BeamSearchDecoder(probs, labels, label_lengths, input_lengths, lm, beam_size=35, blank=28, space=1, prune=1e-3, alpha=0.1, beta=2):\n    probs = probs.cpu().detach().numpy()\n    decodes, targets = [], []\n    for i, prob in enumerate(probs):\n\n        targets.append(tokenizer.int_to_text(labels[i][:label_lengths[i]].tolist()))\n        int_seq, _ = _BeamSearchDecoder(prob[:input_lengths[i]], lm=lm, beam_size=beam_size, blank=blank, prune=prune, alpha=alpha, beta=beta)\n        decodes.append(tokenizer.int_to_text(int_seq))\n        \n    return decodes, targets","metadata":{"cellId":"l2sqrnp6ckupa3ejnjv4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\n# TESTING PREFIX Decoder\n\n#Create LM\nimport arpa\nalm = arpa.loadf('3-gram.pruned.1e-7.arpa')[0]\n\n\n#Load numpy matrix, add axis [batch,classes,time]\nmatrix = np.loadtxt('test_matrix.txt')[np.newaxis,:,:]\n\n# Turn into Torch Tensor of shape [batch, time, classes]\nmatrix = torch.Tensor(matrix).transpose(1,2)\n\n\nlabels_indecies = [torch.Tensor(tokenizer.text_to_int('there seems no good reason for believing that it will change'))]\n\n#Run the Decoder\ndecodes, targets = BeamSearchDecoder(matrix, labels_indecies, [len(labels_indecies[0])], [matrix.size()[1]], lm=None, beam_size=100, prune=1e-3, alpha=0.1, beta=2)\n\n\nassert decodes[0] == 'there se ms no good reason for believing that twil c ange'\nassert targets[0] == 'there seems no good reason for believing that it will change'\n\ndecodes, targets = BeamSearchDecoder(matrix, labels_indecies, [len(labels_indecies[0])], [matrix.size()[1]], lm=alm, beam_size=100, prune=1e-3, alpha=0.1, beta=2)\n\n\nassert decodes[0] == 'there seems no good reason for believing that twillc ange'\nassert targets[0] == 'there seems no good reason for believing that it will change'\n\n","metadata":{"cellId":"w167u74ao4bsoz3htaa738","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Deep Learning part","metadata":{"id":"sr7uip8slCcw","cellId":"zfs3th23sxnnatpxbacw4"}},{"cell_type":"markdown","source":"## Create a Dataloader","metadata":{"id":"sFouWJvO-obE","cellId":"gvc1msqg6viwm3iwt79xy"}},{"cell_type":"code","source":"#!L\n# For train you can use SpecAugment data aug here.\ntrain_audio_transforms = nn.Sequential(\n    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=<YOUR CODE HERE>),\n    ADD YOUR AUGMENTS HERE\n)\n\ntest_audio_transforms = torchaudio.transforms.MelSpectrogram()\n\ntokenizer = Tokenizer()\n\n\n\n\ndef data_processing(data, data_type=\"train\"):\n    spectrograms = []\n    labels = []\n    input_lengths = []\n    label_lengths = []\n    for (waveform, _, utterance, _, _, _) in data:\n        if data_type == 'train':\n            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        elif data_type == 'test':\n            spec = test_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        else:\n            raise Exception('data_type should be train or valid')\n        spectrograms.append(spec)\n        label = torch.Tensor(tokenizer.text_to_indecies(utterance.lower()))\n        labels.append(label)\n        input_lengths.append(spec.shape[0] // 2)\n        label_lengths.append(len(label))\n\n    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n\n    return spectrograms, labels, input_lengths, label_lengths","metadata":{"id":"6FfDdryM-obF","cellId":"4i95bg5u0c4pj26vvkbxyc"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Implement a Neural Network Model\n\nYou should try out a few different model types:\n- Feed-Forward Model (DNN)\n- Recurrent Model (GRU or LSTM)\n- Bidirectional Recurrent Model (bi-GRU or bi-LSTM)\n- Something different for bonus points\n\nBefore any of this models you can use convolutional layers, as shown in the example below\n\nAfter your experiments you should write a report with comparison of different models in terms of different features, for example: parameters, training speed, resulting quality, spectrogram properties, and data augmentations. Remember, that for full mark you need to achive good WER \n\nWER criterions: 60-50 -- 3 points, 50-40 -- 5 points, 40-35 -- 7 points, <= 35 -- 10 points","metadata":{"id":"RqZcwSweECPH","cellId":"jcex1p3ibgiuabzxa0yw"}},{"cell_type":"markdown","source":"### Our model classes are just examples, you can change them as you want","metadata":{"id":"dPwmQyJP5qOe","cellId":"554tm5fog66g78v4s3at8v"}},{"cell_type":"code","source":"#!L\n# Define model\nclass CNNLayerNorm(nn.Module):\n    \"\"\"Layer normalization built for CNNs input\"\"\"\n\n    def __init__(self, n_feats):\n        super(CNNLayerNorm, self).__init__()\n        self.layer_norm = nn.LayerNorm(n_feats)\n\n    def forward(self, x):\n        # x (batch, channel, feature, time)\n        x = x.transpose(2, 3).contiguous()  # (batch, channel, time, feature)\n        x = self.layer_norm(x)\n        return x.transpose(2, 3).contiguous()  # (batch, channel, feature, time)\n\n\nclass ResidualCNN(nn.Module):\n    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n        except with layer norm instead of batch norm\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n        super(ResidualCNN, self).__init__()\n\n        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel // 2)\n        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel // 2)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.layer_norm1 = CNNLayerNorm(n_feats)\n        self.layer_norm2 = CNNLayerNorm(n_feats)\n\n    def forward(self, x):\n        residual = x  # (batch, channel, feature, time)\n        <YOUR CODE HERE>\n        x += residual\n        return x  # (batch, channel, feature, time)\n\n\nclass SpeechRecognitionModel(nn.Module):\n\n    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n        super(SpeechRecognitionModel, self).__init__()\n        n_feats = n_feats // 2\n        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3 // 2)  # cnn for extracting heirachal features\n\n        # n residual cnn layers with filter size of 32\n        self.rescnn_layers = nn.Sequential(*[\n            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats)\n            for _ in range(n_cnn_layers)\n        ])\n        self.fully_connected = <YOUR CODE HERE>\n        self.birnn_layers = <YOUR CODE HERE>\n        self.classifier = <YOUR CODE HERE>\n\n    def forward(self, x):\n        x = self.cnn(x)\n        x = self.rescnn_layers(x)\n        sizes = x.size()\n        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n        x = x.transpose(1, 2)  # (batch, time, feature)\n        x = self.fully_connected(x)\n        x = self.birnn_layers(x)\n        x = self.classifier(x)\n        return x","metadata":{"id":"4icyZP5JD9nW","cellId":"b3ddofit6i1f8yjroo07e"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training and Evaluation Code","metadata":{"id":"T27qdd0iEH9q","cellId":"obxpjwwsdlio3sfm7jgtg"}},{"cell_type":"code","source":"#!L\nfrom tqdm import tqdm_notebook","metadata":{"id":"OwZ3lK3DD-LE","cellId":"x03325d7w5ou3qsh5azgrl"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\ndef train(model, device, train_loader, criterion, optimizer, scheduler, epoch):\n    model.train()\n    data_len = len(train_loader.dataset)\n    for batch_idx, _data in enumerate(train_loader):\n        spectrograms, labels, input_lengths, label_lengths = _data\n        spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(spectrograms)  # (batch, time, n_class)\n        output = F.log_softmax(output, dim=2)\n        output = output.transpose(0, 1)  # (time, batch, n_class)\n\n        loss = criterion(output, labels, input_lengths, label_lengths)\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n        if batch_idx % 100 == 0 or batch_idx == data_len:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(spectrograms), data_len,\n                       100. * batch_idx / len(train_loader), loss.item()))\n\n\ndef test(model, device, test_loader, criterion, epoch, decode='Greedy', lm=None):\n    print('Beginning eval...')\n    model.eval()\n    test_loss = 0\n    test_cer, test_wer = [], []\n    with torch.no_grad():\n        start = time.time()\n        for i, _data in enumerate(test_loader):\n            spectrograms, labels, input_lengths, label_lengths = _data\n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n            \n            matrix = model(spectrograms)  # (batch, time, n_class)\n            matrix = F.log_softmax(matrix, dim=2)\n            probs = F.softmax(matrix,dim=2)\n            matrix = matrix.transpose(0, 1)  # (time, batch, n_class)\n                \n            loss = criterion(matrix, labels, input_lengths, label_lengths)\n            test_loss += loss.item() / len(test_loader)\n\n            if decode == 'Greedy':\n                decoded_preds, decoded_targets = GreedyDecoder(matrix.transpose(0, 1), labels, label_lengths)\n            elif decode == 'BeamSearch':\n                ## THIS IS A CLASS YOU SHOULD IMPLEMENT\n                decoded_preds, decoded_targets = BeamSearchDecoder(probs, labels, label_lengths, input_lengths, lm=lm)\n            for j in range(len(decoded_preds)):\n                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n\n    avg_cer = sum(test_cer) / len(test_cer)\n    avg_wer = sum(test_wer) / len(test_wer)\n\n    print(\n        'Epoch: {:d}, Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(epoch, test_loss,\n                                                                                                       avg_cer,\n                                                                                                       avg_wer))","metadata":{"id":"RAA05g-oD-N_","cellId":"hws8dpmno7ymiam3dnafp"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\n#pragma async \n# PRAGMA ASYNC IS NECESSARY FOR TRAINING!\ntorch.manual_seed(7)\nif torch.cuda.is_available():\n    print('GPU found! 🎉')\n    device = 'cuda'\nelse:\n    print('Only CPU found! 💻')\n    device = 'cpu'\n\nverbose=False\n\n# Hyperparameters for your model\nhparams = {\n    \"n_cnn_layers\": 3,\n    \"n_rnn_layers\": <YOUR CODE HERE>,\n    \"rnn_dim\": <YOUR CODE HERE>\n    \"n_class\": 29,\n    \"n_feats\": <YOUR CODE HERE>,\n    \"stride\": 2,\n    \"dropout\": 0.1,\n    \"learning_rate\":  5e-4,\n    \"batch_size\":  10,\n    \"epochs\": 20\n}\n\n# Define Dataloyour training and test data loaders\nkwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n train_loader = <YOUR_CODE>\n test_loader = <YOUR_CODE>\n\n# Define ASR Model \nmodel = SpeechRecognitionModel(\n    hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n    hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n).to(device)\n\nmodel.to(device)\n\nif verbose:\n    print(model)\nprint('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n\n#Define optimizineer, criterion, scheduler\noptimizer = <YOUR CODE>\ncriterion = <YOUR CODE>\nscheduler = <YOUR CODE> - I suggest OneCycleLR for speed.\n\n\n#iter_meter = IterMeter()\nstart = time.time()\nprint(\"Start training...\")\nfor epoch in range(1, hparams['epochs'] + 1):\n    ep_start = time.time()\n    train(model, device, train_loader, criterion, optimizer, scheduler, epoch)\n    #if epoch % 2 == 0:\n    save_checkpoint(model, checkpoint_name=f'model_epoch{epoch}.tar')\n    load_checkpoint(model, checkpoint_name=f'model_epoch{epoch}.tar', path='./', device=device)\n    test(model, device, test_loader, criterion, epoch)\n    print(f\"Time for epoch: {round(time.time() - ep_start, 0)} sec.\")\nsave_checkpoint(model, checkpoint_name=f'model.tar')\nduration = time.time() - start\nprint(f'Training took {np.round(duration / 60.0, 1)} min.')","metadata":{"id":"ZlAaMjh4D-RD","cellId":"7ezpcofubg6gfyyde5c68"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\n# Test the model in Prefix Decode Mode - only do after you have implemented your prefix decoder\n\ntest(model, device, test_loader, criterion, epoch, decode='BeamSearch', lm=None)\n\nalm = arpa.loadf('3-gram.pruned.1e-7.arpa')[0]\ntest(model, device, test_loader, criterion, epoch, decode='BeamSearch', lm=alm)","metadata":{"cellId":"2oj2t9pb6s508m4bomxqr"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Compare different models: DNN, GRU/LSTM, bi-GRU/bi-LSTM (5 points)","metadata":{"id":"VGS9rrTK29fx","cellId":"90vk5p1hyhmqhjlztpswx8"}},{"cell_type":"markdown","source":"## Analyze CTC Alignments (5 points)","metadata":{"id":"OvSNEbMngpZU","cellId":"7l42i8cl7mch5fzh42uv8b"}},{"cell_type":"markdown","source":"## In this section you should compare alignments obtained from different models.\n\nFor example, you can show:\n\n* Examples of alignments and their analysis\n* Differencies in the properties of alignment distributions over the dataset\n* Dynamic of alignments during training (from checkpoints)\n* Connection between alignments and model loss\n","metadata":{"id":"TtcI8Q65xXnJ","cellId":"om7mmrazgrojbft3qos2o"}},{"cell_type":"code","source":"#!L\nADD YOUR CODE FOR CTC FORWARD BACKWARD FROM SEMINAR","metadata":{"id":"hd2cGenN-oa_","cellId":"hz15vjonrhcd0goeoye94"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!L\n# Test your implementation of CTC\n#Load numpy matrix, add axis [classes,time]\nmatrix = np.loadtxt(os.path.join(week_05_path, 'test_matrix.txt'))\n# Create label_sequence\ntokenizer = Tokenizer()\nlabels_indecies = tokenizer.text_to_indecies('there se ms no good reason for believing that twillc ange')\n\nalign = soft_alignment(labels_indecies, matrix)\n\nref_align = np.loadtxt(os.path.join(week_05_path, 'soft_alignment.txt'))\n\nassert np.all(ref_align == align)","metadata":{"id":"lyO-qCwG-obA","cellId":"j0hk1mmq56mit6ldayht"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Example\nmodel.eval()\n_data = next(iter(test_loader))\nspectrograms, labels, input_lengths, label_lengths = _data\nspectrograms, labels = spectrograms.to(device), labels.to(device)\n\nmatrix = model(spectrograms).transpose(1,2)  # (batch, n_class, time)","metadata":{"id":"QAHLTOQGrgew","cellId":"vrt1ufy7fcefeck5i8hfu"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example of alignment calculation:\nwith torch.no_grad():\n  align = soft_alignment(labels[0].int().cpu().numpy(), F.softmax(matrix[0],dim=0).cpu().numpy())","metadata":{"id":"fL5dwHiGtL1G","cellId":"b80yoozigvm36ra3tisfbf"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(dpi=150)\nplt.imshow(align, aspect='auto', interpolation='nearest')\nplt.colorbar()\n\nplt.figure(dpi=150)\nplt.imshow(np.log(align), aspect='auto', interpolation='nearest')\nplt.colorbar()","metadata":{"id":"DqvHAfyWs7DR","cellId":"t6dtc9k91s90ual4nmzci49"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Conclusions 🧑‍🎓\n\n* What challenges did you encounter while completing this task?\n* What skills have you acquired while doing this task?\n* How difficult did you find this task (on a scale from 0 to 10), and why?\n* What did you like in this homework, and what didn't?","metadata":{"id":"muP0QOp_gvdp","cellId":"fgri9tl5qocpkrzz76s53h"}},{"cell_type":"code","source":"","metadata":{"id":"0LOcRoW3D-Wu","cellId":"iz6t4tmtlns28i9nn6t64h"},"outputs":[],"execution_count":null}]}