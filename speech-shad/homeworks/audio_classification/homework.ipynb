{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "homework.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yerWB31n0NI3"
      },
      "source": [
        "# Homework # 1: Audio Classification\n",
        "\n",
        "In this work you will master all the basic skills with audio applied to the problem of classification.\n",
        "\n",
        "You will:\n",
        "* 🔥 master `torchaudio` as a library for working with audio in conjunction with `torch`\n",
        "* 🔊 try out the different feature representations of the audio signal in practice\n",
        "* 👨‍🔬 develop an audio classification model based on recurrent networks\n",
        "* 🗣 test the trained model on real data (on your own voice)\n",
        "\n",
        "We will work with the **AudioMNIST** dataset (similar to the MNIST dataset, but from the audio world). It was presented in [paper](https://arxiv.org/pdf/1807.03418.pdf) 📑 , dedicated to the study of audio representations. But this article is indirectly related to our homework.\n",
        "\n",
        "![](https://miro.medium.com/max/1400/0*vkUOacXAsNIQCpDu.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46H3S4Vf0NI6"
      },
      "source": [
        "### Installing libraries\n",
        "\n",
        "We will work with `torch` version `1.7.1` (with the corresponding version of `touchaudio`).\n",
        "\n",
        "While the installation is in progress, you can run your eyes through the [documentation](https://pytorch.org/audio/stable/transforms.html) to `torchaudio` – from this library, we will only need a few transforms, as well as audio loading functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umxsbl1h0NI7",
        "outputId": "4b753806-5433-4e49-a57c-eccdc24d48ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!L\n",
        "! pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2  -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "! pip install numpy==1.17.5 matplotlib==3.3.3 tqdm==4.54.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (735.4MB)\n",
            "\u001b[K     |████████████████████████████████| 735.4MB 24kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.8.2%2Bcu101-cp37-cp37m-linux_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 263kB/s \n",
            "\u001b[?25hCollecting torchaudio==0.7.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/16/ecdb9eb09ec6b8133d6c9536ea9e49cd13c9b5873c8488b8b765a39028da/torchaudio-0.7.2-cp37-cp37m-manylinux1_x86_64.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cu101) (7.1.2)\n",
            "\u001b[31mERROR: torchtext 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.7.1+cu101 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "Successfully installed torch-1.7.1+cu101 torchaudio-0.7.2 torchvision-0.8.2+cu101\n",
            "Collecting numpy==1.17.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/51/20098150b6108061cb7542af3de7bfcfe0182bca21613697153e49dc4adc/numpy-1.17.5-cp37-cp37m-manylinux1_x86_64.whl (20.0MB)\n",
            "\u001b[K     |████████████████████████████████| 20.0MB 155kB/s \n",
            "\u001b[?25hCollecting matplotlib==3.3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/f2/10c822cb0ca5ebec58bd1892187bc3e3db64a867ac26531c6204663fc218/matplotlib-3.3.3-cp37-cp37m-manylinux1_x86_64.whl (11.6MB)\n",
            "\u001b[K     |████████████████████████████████| 11.6MB 12.8MB/s \n",
            "\u001b[?25hCollecting tqdm==4.54.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/4e/afa45872365fe2abd13c8022d39348c01808b8cfeea129937920d7bb2244/tqdm-4.54.0-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.3) (7.1.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.3) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.3) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.3) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.3) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.3.3) (1.15.0)\n",
            "\u001b[31mERROR: torchtext 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.7.1+cu101 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement numpy~=1.19.2, but you'll have numpy 1.17.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement numpy>=1.18.5, but you'll have numpy 1.17.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, matplotlib, tqdm\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed matplotlib-3.3.3 numpy-1.17.5 tqdm-4.54.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpqDQVME0gVs",
        "outputId": "6637d3b0-a0ec-4709-e800-46736f719f38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! apt-get install sox"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libmagic-mgc libmagic1 libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa\n",
            "  libsox-fmt-base libsox3\n",
            "Suggested packages:\n",
            "  file libsox-fmt-all\n",
            "The following NEW packages will be installed:\n",
            "  libmagic-mgc libmagic1 libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa\n",
            "  libsox-fmt-base libsox3 sox\n",
            "0 upgraded, 8 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 760 kB of archives.\n",
            "After this operation, 6,717 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopencore-amrnb0 amd64 0.1.3-2.1 [92.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopencore-amrwb0 amd64 0.1.3-2.1 [45.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox3 amd64 14.4.2-3ubuntu0.18.04.1 [226 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox-fmt-alsa amd64 14.4.2-3ubuntu0.18.04.1 [10.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox-fmt-base amd64 14.4.2-3ubuntu0.18.04.1 [32.1 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 sox amd64 14.4.2-3ubuntu0.18.04.1 [101 kB]\n",
            "Fetched 760 kB in 1s (545 kB/s)\n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "(Reading database ... 160772 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libopencore-amrnb0_0.1.3-2.1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.3-2.1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../1-libopencore-amrwb0_0.1.3-2.1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.3-2.1) ...\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "Preparing to unpack .../2-libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../3-libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../4-libsox3_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../5-libsox-fmt-alsa_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../6-libsox-fmt-base_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package sox.\n",
            "Preparing to unpack .../7-sox_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking sox (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.3-2.1) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.3-2.1) ...\n",
            "Setting up libsox3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Setting up sox (14.4.2-3ubuntu0.18.04.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0T26YOI0NI9"
      },
      "source": [
        "### Obtaining AudioMNIST dataset 0️⃣1️⃣2️⃣3️⃣4️⃣5️⃣6️⃣7️⃣8️⃣9️⃣\n",
        "\n",
        "To get the dataset, clone the repository (`~1Gb`) running the code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65k3KLZc0NI9",
        "outputId": "85f7de77-472d-4c79-9440-0a89db2512b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#!L:bash\n",
        "# If in Yandex Datasphere run as below:\n",
        "!git clone https://github.com/soerenab/AudioMNIST\n",
        "# If run locally:\n",
        "# ! git clone https://github.com/soerenab/AudioMNIST"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'AudioMNIST'...\n",
            "remote: Enumerating objects: 30119, done.\u001b[K\n",
            "remote: Total 30119 (delta 0), reused 0 (delta 0), pack-reused 30119\u001b[K\n",
            "Receiving objects: 100% (30119/30119), 944.54 MiB | 14.77 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n",
            "Checking out files: 100% (30018/30018), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9ppFDfRNVfo",
        "outputId": "54493ab7-d4b5-4d52-c2db-b155b617e210",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/yandexdataschool/speech_course.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'speech_course'...\n",
            "remote: Enumerating objects: 306, done.\u001b[K\n",
            "remote: Total 306 (delta 0), reused 0 (delta 0), pack-reused 306\u001b[K\n",
            "Receiving objects: 100% (306/306), 144.87 MiB | 30.32 MiB/s, done.\n",
            "Resolving deltas: 100% (111/111), done.\n",
            "Checking out files: 100% (99/99), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO4D3QJDNeaB"
      },
      "source": [
        "import os\n",
        "os.chdir(\"speech_course/week_02\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0TjQh0K0NI-"
      },
      "source": [
        "#!L\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "from typing import List, Tuple\n",
        "from glob import glob\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from IPython.display import display, Audio\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US4SZsi00NI-"
      },
      "source": [
        "If you run this Jupyter Notebook on a computer with a video card (**GPU**), you need to specify which card you want to work with, and you also need to make sure that `torch` sees it. \n",
        "\n",
        "If you *really* want, you can do your homework without using a video card, but then the training will take a very long time.\n",
        "\n",
        "**Tip**: if the video card is an exhausted resource (i.e. you have a usage quota, or you pay for usage), then it is better to **debug** training without the GPU: to make sure that the model does not crash with an error, and the loss decreases in the first iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXIYnyte0NI-"
      },
      "source": [
        "#!L\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU found! 🎉')\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print('Only CPU found! 💻')\n",
        "    device = 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_L1qWBf0NI_"
      },
      "source": [
        "### Prepare the set for testing 🎛\n",
        "\n",
        "The set for validation will be taken from the `AudioMNIST` dataset. But as you know, there is a risk of overfitting not only on training set, but also on validation set. Therefore, for the purity of the experiment, we will evaluate the work of our algorithm on real data, namely, our voice!\n",
        "\n",
        "You can record your own (or someone else's) voice speaking out the numbers, so you will have a real mini-dataset for testing. You will need it exclusively. We will test your model on our **own** testing set. 👀\n",
        "\n",
        "To record a voice, you can use web-sites like [this](https://voice-recorder-online.com), and also you can do this via the command line (you need the `sox` utility):\n",
        "\n",
        "```\n",
        "rec recording.wav trim 0 10   # record a 10-second audio fragment\n",
        "of play recording.wav         # listen to it\n",
        "```\n",
        "\n",
        "The recording may differ not only in format (be `mp3`, `ogg`), but in sampling rate (`22050Hz`, `44100Hz`), being a stereo recording, and so on. \n",
        "\n",
        "To bring the record to the unified format we need:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELGyq3Gl0NI_"
      },
      "source": [
        "#!S\n",
        "# -> 16kHz, 16bit, mono\n",
        "! sox recording.wav -r 16000 -b 16 -c 1 recording_16kHz.wav"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "eWmFoqsv0NI_"
      },
      "source": [
        "#!L\n",
        "x, sr = torchaudio.load('recording_16kHz.wav')\n",
        "x = x[0].numpy()\n",
        "\n",
        "DIGIT_NAMES = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
        "print(\", \".join(DIGIT_NAMES))\n",
        "display(Audio(x, rate=sr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcbiJ14C0NJA"
      },
      "source": [
        "You need to break this record into fragments, each of which would contain audio with the pronunciation of **only one** digit.\n",
        "\n",
        "Let's use the energy ⚡️ VAD!\n",
        "\n",
        "**Calculate** the energy of the signal in a windowed way (choose the window size however you like) and look at its plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAZp9c_t0NJA"
      },
      "source": [
        "#!L\n",
        "# empirically, you can choose the optimal window size\n",
        "win_size = 1024\n",
        "\n",
        "energies_db = []\n",
        "for i in range(len(x) // win_size + 1):\n",
        "    x_win = x[i * win_size: (i + 1) * win_size]\n",
        "    # calculate energy of signal fragment\n",
        "    ################################################################################\n",
        "    # ...\n",
        "    ################################################################################\n",
        "energies_db = np.array(energies_db)\n",
        "\n",
        "def plot_signal_energies(x, sr, energies_db, intervals=None):\n",
        "    fig = plt.figure(figsize=(len(x) / sr, 2.5))\n",
        "    plt.plot(x, color='#20639b', label='audio signal')\n",
        "    plt.xticks(np.arange(0, len(x), sr), np.int32(np.arange(0, len(x), sr) / sr))\n",
        "    plt.grid(alpha=0.5, linestyle='--')\n",
        "    plt.xlabel('time [s]')\n",
        "    plt.ylabel('amplitude')\n",
        "    plt.ylim([-0.51, 0.51])\n",
        "    plt.xlim(-50, len(x) + 50)\n",
        "    if intervals:\n",
        "        for a, b in intervals:            \n",
        "            plt.gca().add_patch(patches.Rectangle(\n",
        "                (a, -0.51), (b - a), 1.2, \n",
        "                linewidth=1, edgecolor='none', \n",
        "                facecolor='#3caea3', alpha=0.3))\n",
        "\n",
        "    plt.twinx()\n",
        "    plt.plot(\n",
        "        np.linspace(0, len(x), len(energies_db)), energies_db, \n",
        "        color='#3caea3', label='energy (window size = 512)')\n",
        "    plt.ylabel('Energy [dB]')\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "plot_signal_energies(x, sr, energies_db)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFyBp-Zk0NJB"
      },
      "source": [
        "I hope you will have no trouble coming up with **a heuristic** for finding the boundaries of spoken words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp5W8J5T0NJB"
      },
      "source": [
        "#!L\n",
        "# list of digit intervals [(a_0, b_0), ...]\n",
        "# where a_i -- first audio sample of spoken word, b_i -- last sample\n",
        "intervals = []\n",
        "\n",
        "################################################################################\n",
        "# ...\n",
        "################################################################################\n",
        "\n",
        "plot_signal_energies(x, sr, energies_db, intervals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imExbjtV0NJB"
      },
      "source": [
        "We will split the original record into fragments and save them to the directory with the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaCybd7e0NJB"
      },
      "source": [
        "#!L\n",
        "test_directory = 'AudioMNIST/data/test'\n",
        "if not os.path.exists(test_directory):\n",
        "    os.mkdir(test_directory)\n",
        "\n",
        "n = 0\n",
        "for a, b in intervals:\n",
        "    x_digit = x[a: b]\n",
        "    path = f'{test_directory}/{n % 10}_test_{n // 10}.wav'\n",
        "    print(f'\"{DIGIT_NAMES[n % 10]}\" -> {path}')\n",
        "    n += 1\n",
        "    \n",
        "    torchaudio.save(path, torch.FloatTensor(x_digit), sr)\n",
        "    display(Audio(path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD7bK_DM0NJC"
      },
      "source": [
        "### Class for the dataset 🗄\n",
        "\n",
        "The class below (the a subclass of `torch.utils.data.Dataset`) holds the logic of loading `AudioMNIST` dataset. All the file handling routins are already written, so you are required to do the following:\n",
        "\n",
        "* 👩‍💻 completed the code (at least, in the `__getitem__` method) for obtaining the audio features (representations) you need from the signal\n",
        "* 🤔 explain your choice with a comment in `__getitem__`\n",
        "\n",
        "You could use the waveform itself as an audio representation (features), **but** as you know from the lecture (*DSP Basics*), this is not the best representation for many tasks.\n",
        "\n",
        "You are able to return to this stage many times while doing you homework, so iterate from simple representations to sophisticated ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyfxWgmZ0NJC"
      },
      "source": [
        "#!L\n",
        "class AudioMNISTDataset(data.Dataset):\n",
        "    def __init__(self, directory: str, mode='train'):\n",
        "        self.mode = mode\n",
        "        self.filepaths = []\n",
        "        self.labels = []\n",
        "        for filepath in sorted(glob(os.path.join(directory, '*', '?_*_*.wav'))):\n",
        "            digit, speaker, example_id = os.path.basename(filepath).replace('.wav', '').split('_')\n",
        "            add = False\n",
        "            if speaker == 'test' and mode == 'test':\n",
        "                add = True\n",
        "            if speaker != 'test' and int(speaker) % 6 == 0 and mode == 'valid':\n",
        "                add = True\n",
        "            if speaker != 'test' and int(speaker) % 6 != 0 and mode == 'train':\n",
        "                add = True\n",
        "            if add:\n",
        "                self.filepaths.append(filepath)\n",
        "                self.labels.append(int(digit))\n",
        "                \n",
        "        print(f\"Dataset [{mode}]: {len(self.filepaths)} audios\")\n",
        "        \n",
        "        ################################################################################\n",
        "        # ...\n",
        "        ################################################################################\n",
        "    \n",
        "    def __getitem__(self, idx) -> Tuple[torch.FloatTensor, torch.LongTensor]:\n",
        "        x, sr = torchaudio.load(self.filepaths[idx])\n",
        "        y = torch.LongTensor([self.labels[idx]])[0]\n",
        "\n",
        "        #####################################################################\n",
        "        # ...\n",
        "        #####################################################################\n",
        "        \n",
        "        # ARGUMENTATION:\n",
        "        # I've chosen this representation, because...\n",
        "        \n",
        "        # x: audio features\n",
        "        # y: target digit class\n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.filepaths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHZbSpgG0NJC"
      },
      "source": [
        "#!L\n",
        "train_dataset = AudioMNISTDataset('AudioMNIST/data', mode='train')\n",
        "valid_dataset = AudioMNISTDataset('AudioMNIST/data', mode='valid')\n",
        "test_dataset = AudioMNISTDataset('AudioMNIST/data', mode='test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avGSwmFg0NJC"
      },
      "source": [
        "It's interesting to see what we got...\n",
        "\n",
        "You may see some interesting points in your audio representation, and **may want to try a different representation**. 🕵️‍ Or return to this later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "DiPW1UXd0NJD"
      },
      "source": [
        "#!L\n",
        "def show_some_examples(dataset, K, figsize=(10, 6)):\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    for digit in range(10):\n",
        "        indices = np.where(np.array(dataset.labels) == digit)[0]\n",
        "        for i in range(K):\n",
        "            x, y = dataset[indices[i]]\n",
        "            ax = fig.add_subplot(3, 10, digit + i * 10 + 1)\n",
        "            # roate\n",
        "            if i == 0:\n",
        "                ax.set_title(DIGIT_NAMES[digit])\n",
        "            ax.imshow(x)\n",
        "            ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "print(\"visualize dataset for training\")\n",
        "show_some_examples(train_dataset, 2)\n",
        "\n",
        "print(\"visualize dataset for testing\")\n",
        "show_some_examples(test_dataset, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRO7t5ga0NJD"
      },
      "source": [
        "Can determine **which number is pronounced** looking at it? If so, then the computer can!\n",
        "\n",
        "Note that the audios have different durations, at least because the words have different numbers of phonemes. Thus, our feature tensors do not have a fixed size either.\n",
        "\n",
        "```\n",
        "- Can we bring them to the same size?\n",
        "- Yes, we can. And it will even work (see the dataset article).\n",
        "- So why don't we do that?\n",
        "- This approach has a limited scope. And here's why:\n",
        "```\n",
        "\n",
        "* First, in real ASR systems, we do not classify words, but *sounds* (graphemes, phonemes) – they are more difficult to localize and difficult to classify independently. Although, for example, for the task of classifying the sounds of birdsong, this approach would be normal.\n",
        "* Secondly, convolutional neural networks (actually for which we would like images of the same size) poorly orient in space, since they react to patterns, and may not pay attention to their spatial location. In our problem, the words are quite unique, so they are uniquely identified by a set of sounds (phonemes), so this approach would work. But we're taking a course in **speech recognition and synthesis, not sound classification**. 🙂"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pk_s__l0NJD"
      },
      "source": [
        "### Batch generation class 📦\n",
        "\n",
        "All DL frameworks work with tensors (not lists of tensors). You need to somehow concatenate audio into a batch, but initially they are of different lengths. You can try zero padding (don't forget to store original length)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsirbVMl0NJD"
      },
      "source": [
        "#!L\n",
        "class Collate:\n",
        "    def __call__(self, batch: List[Tuple[torch.Tensor, torch.Tensor]]):\n",
        "        ################################################################################\n",
        "        # ...\n",
        "        ################################################################################\n",
        "\n",
        "        \n",
        "        # xs: batch of features\n",
        "        # ls: batch of lengths\n",
        "        # ys: batch of targets\n",
        "        return xs, ls, ys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWsrHjNs0NJD"
      },
      "source": [
        "#!L\n",
        "collate_fn = Collate()\n",
        "train_loader = data.DataLoader(\n",
        "    train_dataset, batch_size=32, shuffle=True, \n",
        "    num_workers=4, drop_last=True, collate_fn=collate_fn)\n",
        "valid_loader = data.DataLoader(\n",
        "    valid_dataset, batch_size=32, shuffle=False, \n",
        "    num_workers=4, collate_fn=collate_fn)\n",
        "test_loader = data.DataLoader(\n",
        "    test_dataset, batch_size=len(test_dataset), shuffle=False, \n",
        "    num_workers=1, collate_fn=collate_fn)\n",
        "\n",
        "xs, ls, ys = next(iter(train_loader))\n",
        "print(xs.size(), ls.size(), ys.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVBmQuUY0NJE"
      },
      "source": [
        "### The neural network classifier 🤖\n",
        "\n",
        "Write a model for classifying audios. It is desirable to use some recurrent layer in the model. Start with a simple one, and make it more complicated if necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_7JSFHK0NJE"
      },
      "source": [
        "#!L\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        ################################################################################\n",
        "        # ...\n",
        "        ################################################################################\n",
        "\n",
        "    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
        "        ################################################################################\n",
        "        # ...\n",
        "        ################################################################################\n",
        "        return x\n",
        "    \n",
        "def create_model_and_optimizer(device):\n",
        "    ################################################################################\n",
        "    # ...\n",
        "    ################################################################################\n",
        "    return model, optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4gl6YSu0NJE"
      },
      "source": [
        "### Training, validation, testing 🏋 ️\n",
        "\n",
        "The minimally sufficient code for training and testing is already written. But feel free to modify the code (but try not to break the compatibility).\n",
        "\n",
        "**Tips**:\n",
        "* before you start long-term training, see how your model behaves on the first iterations/epochs\n",
        "* it is desirable that the training epoch takes less one minute; if longer, maybe your model is too complex\n",
        "* models converge quickly, so it hardly makes sense to do training more than 100 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "uBXJZd1u0NJF"
      },
      "source": [
        "#!L\n",
        "def train(model, optimizer, loader, metrics):\n",
        "    model.train()\n",
        "    \n",
        "    losses = []\n",
        "    for x, lengths, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        lengths = lengths.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x, lengths)\n",
        "        loss = F.cross_entropy(y_pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        losses.append(loss.item())\n",
        "    \n",
        "    metrics['train_loss'].append(np.mean(losses) )\n",
        "\n",
        "\n",
        "def evaluate(model, loader, metrics, mode):\n",
        "    model.eval()\n",
        "    \n",
        "    losses = []\n",
        "    cm = np.zeros((10, 10), dtype=np.int32)\n",
        "    with torch.no_grad():\n",
        "        valid_losses = []\n",
        "        for x, lengths, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            lengths = lengths.to(device)\n",
        "    \n",
        "            y_pred = model(x, lengths)\n",
        "            loss = F.cross_entropy(y_pred, y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            y_pred = y_pred.argmax(axis=-1)\n",
        "            for i in range(len(x)):\n",
        "                cm[y[i].item(), y_pred[i].item()] += 1\n",
        "    \n",
        "    metrics[f'{mode}_loss'].append(np.mean(losses))\n",
        "    metrics[f'{mode}_accuracy'].append(np.trace(cm) / np.sum(cm))\n",
        "    metrics[f'{mode}_confusion'].append(cm)\n",
        "\n",
        "\n",
        "def train_from_scratch(model, optimizer, train_loader, valid_loader, test_loader):\n",
        "    metrics = {\n",
        "        'train_loss': [],\n",
        "        'valid_loss': [],\n",
        "        'valid_accuracy': [],\n",
        "        'valid_confusion': [],\n",
        "        'test_loss': [],\n",
        "        'test_accuracy': [],\n",
        "        'test_confusion': []\n",
        "    }\n",
        "\n",
        "    best_valid_accuracy = 0.0\n",
        "    for epoch in tqdm(range(20)):\n",
        "        train(model, optimizer, train_loader, metrics)\n",
        "                \n",
        "        evaluate(model, valid_loader, metrics, 'valid')\n",
        "        evaluate(model, test_loader, metrics, 'test')\n",
        "        \n",
        "        if metrics['valid_accuracy'][-1] > best_valid_accuracy:\n",
        "            best_valid_accuracy = metrics['valid_accuracy'][-1]\n",
        "            torch.save({\n",
        "                'state_dict': model.state_dict(), \n",
        "                'metrics': metrics\n",
        "            }, checkpoint_path)\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "\n",
        "checkpoint_path = 'model.pth'\n",
        "model, optimizer = create_model_and_optimizer(device)\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f'Loading model weights from {checkpoint_path}')\n",
        "    ckpt = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(ckpt['state_dict'])\n",
        "    metrics = ckpt['metrics']\n",
        "    evaluate(model, valid_loader, metrics, 'valid')\n",
        "    evaluate(model, test_loader, metrics, 'test')\n",
        "else:\n",
        "    print('Training model from scratch..')\n",
        "    metrics = train_from_scratch(model, optimizer, train_loader, valid_loader, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31DSDI0H0NJF"
      },
      "source": [
        "### Visualization of the results 📉 📈\n",
        "\n",
        "We will select the model based on the best accuracy value during validation. Let's evaluate the accuracy of the model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "YcxrD_Ej0NJF"
      },
      "source": [
        "#!L\n",
        "def plot_accuracies(valid_accuracy, test_accuracy, best_epoch):\n",
        "    best_valid = valid_accuracy[best_epoch]\n",
        "    best_test = test_accuracy[best_epoch]\n",
        "    \n",
        "    plt.figure(figsize=(7, 3))\n",
        "    plt.title('Accuracy')\n",
        "    plt.plot(valid_accuracy, label='valid', color='#20639b')\n",
        "    plt.plot(test_accuracy, label='test', color='#3caea3')\n",
        "    plt.grid(linestyle='--', alpha=0.5)\n",
        "    plt.vlines(best_epoch, 0.6, 1, color='#ed553b', linestyle='--', label='best')\n",
        "    plt.hlines(best_valid, 0, best_epoch, linestyle='--', color='#555555')\n",
        "    plt.hlines(best_test, 0, best_epoch, linestyle='--', color='#555555')\n",
        "    plt.plot(best_epoch, best_valid, marker='o', color='#20639b')\n",
        "    plt.plot(best_epoch, best_test, marker='o', color='#3caea3')\n",
        "    plt.text(0, best_valid - 0.02, f\"{best_valid:.4f}\")\n",
        "    plt.text(0, best_test - 0.02, f\"{best_test:.4f}\")\n",
        "    plt.ylim(0.8, 1.01)\n",
        "    plt.xlim(0, len(valid_accuracy) - 1)\n",
        "    plt.xticks(range(len(valid_accuracy)))\n",
        "    plt.yticks(np.linspace(0.8, 1.0, 11))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, names, title='Confusion matrix', normalize=False):\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.get_cmap('Blues'))\n",
        "    plt.title(title)\n",
        "\n",
        "    if names is not None:\n",
        "        tick_marks = np.arange(len(names))\n",
        "        plt.xticks(tick_marks, names, rotation=45)\n",
        "        plt.yticks(tick_marks, names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            kwargs = {\n",
        "                'color': \"white\" if cm[i, j] > thresh else \"black\",\n",
        "                'horizontalalignment': 'center'\n",
        "            }\n",
        "            if normalize:\n",
        "                plt.text(j, i, \"{:0.2f}\".format(cm[i, j]), **kwargs)\n",
        "            else:\n",
        "                plt.text(j, i, \"{:,}\".format(cm[i, j]), **kwargs)\n",
        "\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel(f'Predicted label\\naccuracy={accuracy:0.4f}')\n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "# choosing model by best accuracy on validation set\n",
        "best = np.argmax(metrics['valid_accuracy'])\n",
        "best_cm = metrics['test_confusion'][best]\n",
        "\n",
        "plot_accuracies(metrics['valid_accuracy'], metrics['test_accuracy'], best)\n",
        "plot_confusion_matrix(best_cm, range(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhApmQCd0NJG"
      },
      "source": [
        "### Augmentation\n",
        "\n",
        "Augmentations are applied to training data to improve the generalization ability of the deep neural model. Think about what augmentations you can apply to the audio, and then check how they work by experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkXXB1kH0NJG"
      },
      "source": [
        "#!S\n",
        "# run experiments here, reuse previously defined functions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMa7uK270NJH"
      },
      "source": [
        "### Conclusions 🧑‍🎓\n",
        "\n",
        "* What challenges did you encounter while completing this task?\n",
        "* What skills have you acquired while doing this task?\n",
        "* How difficult did you find this task (on a scale from 0 to 10), and why?\n",
        "* What did you like in this homework, and what didn't?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX7e8Xif0NJH"
      },
      "source": [
        "### Grades criteria\n",
        "\n",
        "```\n",
        "[ ] (1 point)  implement energy VAD for audio splitting\n",
        "[ ] (1 point)  explain the reason for choosing the audio representation features\n",
        "[ ] (3 points) train classification model (> 95% accuracy on validation)\n",
        "[ ] (2 points) achieve good test-score on our side (> 85% accuracy on hidden sample)\n",
        "[ ] (2 points) provide augmentation experiments\n",
        "[ ] (1 point)  write conclusions\n",
        "```\n",
        "\n",
        "The results of this task are two artifacts:\n",
        "1. this Jupiter Notebook (`.ipynb`) with completed cells\n",
        "2. checkpoint of the `model.path` model on the \"best\" iteration\n",
        "\n",
        "Save the artifacts to a directory named `{your last name}_{your first name}_hw1` and pack them in `.zip` archive."
      ]
    }
  ]
}